{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###### Install Configuration\n",
    "\n",
    "# !python3 -m pip install -U numpy\n",
    "# !python3 -m pip install -U pip\n",
    "# !python3 -m pip install -U torch\n",
    "# !python3 -m pip install -U setuptools wheel\n",
    "# !python3 -m pip install -U \"mxnet_cu101<2.0.0\"\n",
    "# # !python3 -m pip install -U \"mxnet<2.0.0\"\n",
    "# !python3 -m pip install autogluon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### necessary packages\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordering predicting vectors \n",
    "def find_stable_point():\n",
    "    \n",
    "    res = np.zeros(625)-99\n",
    "    b=[]\n",
    "    xm=[]\n",
    "    ym=[]\n",
    "    filename= './sr_C1_20/800rpm_100kPa_30C_Swirl=20_B00006_PIV_MP(2x32x32_50ov)=unknown/model-3-3-prediction-3-to-6/x/1-raw.dat'\n",
    "    data=pd.read_csv(filename,index_col = 0).values\n",
    "    for k in range(25):\n",
    "        for i in range(1,24):\n",
    "            for j in range(1,24):\n",
    "                #for any sequence in i and j\n",
    "                if data[i][j]!=0:\n",
    "                    if data[i-1][j]==0 or data[i+1][j]==0 or data[i][j-1]==0 or data[i][j+1]==0:\n",
    "                        b.append(i*25+j)\n",
    "                        xm.append(i)\n",
    "                        ym.append(j)\n",
    "        for i in range(len(xm)):\n",
    "            data[xm[i]][ym[i]]=0             \n",
    "    b = np.flipud(b)\n",
    "    b=np.append(b,range(625))\n",
    "    l1=list(b)\n",
    "    for el in range(len(l1)-1, -1, -1):\n",
    "        if l1.count(l1[el]) > 1:\n",
    "            l1.pop(el)\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stable_point(md = 'x', mode = 'high', phase = 1):\n",
    "    res = np.zeros(625)\n",
    "    res=res-99\n",
    "    if md=='x':\n",
    "        path1 ='x/result-'+mode+'-'+str(phase)+'/'\n",
    "    if md == 'y':\n",
    "        path1 ='y/result-'+mode+'-'+str(phase)+'/'\n",
    "    b=[]\n",
    "    xm=[]\n",
    "    ym=[]\n",
    "    filename= './sr_C1_20/800rpm_100kPa_30C_Swirl=20_B00006_PIV_MP(2x32x32_50ov)=unknown/model-3-3-prediction-3-to-6/x/1-raw.dat'\n",
    "    data=pd.read_csv(filename,index_col = 0).values\n",
    "    # print(data)\n",
    "    for k in range(25):\n",
    "        for i in range(1,24):\n",
    "            for j in range(1,24):\n",
    "                #for any sequence in i and j\n",
    "                if data[i][j]!=0:\n",
    "                    if data[i-1][j]==0 or data[i+1][j]==0 or data[i][j-1]==0 or data[i][j+1]==0:\n",
    "                        b.append(i*25+j)\n",
    "\n",
    "                        xm.append(i)\n",
    "                        ym.append(j)\n",
    "        for i in range(len(xm)):\n",
    "            data[xm[i]][ym[i]]=0\n",
    "                    \n",
    "    b = np.flipud(b)\n",
    "    b=np.append(b,range(625))\n",
    "    l1=list(b)\n",
    "    for el in range(len(l1)-1, -1, -1):\n",
    "        if l1.count(l1[el]) > 1:\n",
    "            l1.pop(el)\n",
    "    # print(l1)\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(find_stable_point('x','high',6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def second_phase_prediction(direction,vec_num,mtype,phase,data1,nxt_phase,scope):\n",
    "    req_column = len(data1.columns)-1\n",
    "    path1='./a/'+direction+'/result-'+mtype+'-'+str(phase)+'/'+str(phase-scope+1)+'-'+str(phase)+'-'+str(nxt_phase)+'/tgModels'+str(vec_num)\n",
    "    isExists=os.path.exists('./a/'+direction+'/result-'+mtype+'-'+str(phase)+'/'+str(phase-scope+1)+'-'+str(phase)+'-'+str(nxt_phase))\n",
    "    if not isExists:\n",
    "\n",
    "        os.makedirs('./a/'+direction+'/result-'+mtype+'-'+str(phase)+'/'+str(phase-scope+1)+'-'+str(phase)+'-'+str(nxt_phase))\n",
    "\n",
    "    save_path = './a/'+direction+'/result-'+mtype+'-'+str(phase)+'/'+str(phase-scope+1)+'-'+str(phase)+'-'+str(nxt_phase)+'/'+str(vec_num)\n",
    "    isExists2 = os.path.exists(path1)\n",
    "    if isExists2:\n",
    "        return\n",
    "    \n",
    "        \n",
    "    data_property=data1[req_column].describe()\n",
    "    if(data1[req_column].max()==0):\n",
    "        return\n",
    "    # train_data.index.duplicated(keep=False)\n",
    "    data_property.to_csv(save_path+'_data.csv')\n",
    "    data1.index = range(116*scope*90)\n",
    "    # \n",
    "    predictor_target = TabularPredictor(label=req_column, path=path1).fit(data1, time_limit=180, hyperparameters = { 'CAT': {}, 'XGB': {}, 'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge']})\n",
    "    performance = predictor_target.evaluate(data1)\n",
    "    result=predictor_target.leaderboard(data1, silent=True)\n",
    "    result.to_csv(save_path+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(mtype = 'high', sequence = 1):\n",
    "    if mtype == 'high':\n",
    "        path1='sr_C1_20'\n",
    "        swirl = '20'\n",
    "    elif mtype == 'medium':\n",
    "        path1= 'sr_C4_12'\n",
    "        swirl = '12'\n",
    "    elif mtype == 'low':\n",
    "        path1 = 'sr_C6_01'\n",
    "        swirl = '01'\n",
    "    path2=\"800rpm_100kPa_30C_Swirl=\"+swirl+\"_B\"+str(sequence).zfill(5)+\"_PIV_MP(2x32x32_50ov)=unknown/\"\n",
    "    path = path1 + '/' + path2\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phasewise_prediction(md = 'x',model = 'high',phase = 1,next_phase = 2,scope =1):\n",
    "\n",
    "    stable_priority = find_stable_point(md, model, next_phase)\n",
    "    pre_data = read_data(md,model,phase,scope)\n",
    "\n",
    "    \n",
    "    pre_data.index = range(scope*90*116)\n",
    "    \n",
    "    # pre_data.to_csv('pre_data.csv')    \n",
    "    nxt_data = read_data(md,model,next_phase,scope)\n",
    "    nxt_data.index = range(scope*90*116)\n",
    "\n",
    "    for i in range(625):\n",
    "\n",
    "        pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
    "\n",
    "        second_phase_prediction(md,i,model,phase,pre_data,next_phase,scope)\n",
    "        \n",
    "    #priority list_of stable_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crank_angle_trainer(md,begin_CAD,end_CAD,next_CAD):\n",
    "    phase = 151+int(end_CAD/2)\n",
    "    scope = int((end_CAD-begin_CAD)/2)+1\n",
    "    next_phase = 151+int(next_CAD/2)\n",
    "    \n",
    "    print(md,phase,next_phase,scope)\n",
    "    phasewise_prediction('x',md,phase,next_phase,scope)\n",
    "    phasewise_prediction('y',md,phase,next_phase,scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(mtype = 'high', sequence = 1):\n",
    "    if mtype == 'high':\n",
    "        path1='sr_C1_20'\n",
    "        swirl = '20'\n",
    "    elif mtype == 'medium':\n",
    "        path1= 'sr_C4_12'\n",
    "        swirl = '12'\n",
    "    elif mtype == 'low':\n",
    "        path1 = 'sr_C6_01'\n",
    "        swirl = '01'\n",
    "    path2=\"800rpm_100kPa_30C_Swirl=\"+swirl+\"_B\"+str(int(sequence)).zfill(5)+\"_PIV_MP(2x32x32_50ov)=unknown/\"\n",
    "    path = path1 + '/' + path2\n",
    "    return path\n",
    "\n",
    "\n",
    "def read_data(model = 'x',mtype = 'high',phase = 1,scope = 1):\n",
    "    series = pd.DataFrame(None,columns=range(625))\n",
    "    train_cycles = []\n",
    "    print(phase)\n",
    "    for i in range(10):\n",
    "        for j in range(1,10):\n",
    "            train_cycles.append(10*i+j)\n",
    "    for j in range(116):\n",
    "        for i in train_cycles:            \n",
    "            new_series = read_batch(get_path(mtype, phase + j) + 'B' + str(i).zfill(5), model)\n",
    "            series = pd.concat([series, new_series], ignore_index=True)\n",
    "\n",
    "    \n",
    "    series.index = range(116*90*scope)\n",
    "    return series\n",
    "\n",
    "\n",
    "def read_batch(str1 = 'B00001',model = 'x'):\n",
    "    suf_1 = '.dat'\n",
    "    \n",
    "\n",
    "    data1 = pd.read_csv(str1+suf_1,skiprows = 3,header = None,delimiter = ' ')\n",
    "    \n",
    "    value = data1.values.reshape(25,25,-1)\n",
    "    if model == 'x':\n",
    "        vx=pd.DataFrame(value[:,:,2].reshape(-1)).transpose()\n",
    "    if model == 'y':\n",
    "        vx=pd.DataFrame(value[:,:,3].reshape(-1)).transpose()\n",
    "#     y=value[:,:,1]\n",
    "#     vx=value[:,:,2]\n",
    "#     vy = value[:,:,3]\n",
    "    return vx\n",
    "\n",
    "\n",
    "def single_phase_predictor(mtype = 'low',phase = 1):\n",
    "    data1 = read_data('x',mtype, phase)\n",
    "    data2 = read_data('y',mtype, phase)\n",
    "    # print(data1)\n",
    "    for target_column in range(625):\n",
    "        in_phase_prediction('x',target_column,mtype,phase,data1)\n",
    "        in_phase_prediction('y',target_column,mtype,phase,data2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_phase_predictor('high',23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prediction(mtype = 'high'):\n",
    "    data = pd.read_csv(\"lab.csv\")\n",
    "    begin_CAD = data['begin_CAD']\n",
    "    end_CAD = data['end_CAD']\n",
    "    next_CAD = data['next_CAD']\n",
    "    predict_CAD = data['predict_CAD']\n",
    "    for i in range(data.shape[0]):\n",
    "        crank_angle_trainer(mtype,begin_CAD[i],end_CAD[i],next_CAD[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high 1 6 1\n",
      "1\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels0\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   71.51 GB / 2000.36 GB (3.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 625\n",
      "Label Column: 625\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.5053016814, -14.0150741749, -0.9542, 2.68203)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62431.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.2 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 310 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 310 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t310 features in original data used to generate 310 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 25.89 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.21s of the 179.21s of remaining time.\n",
      "\t-2.2795\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 175.94s of the 175.94s of remaining time.\n",
      "\t-2.299\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 173.96s of the 173.96s of remaining time.\n",
      "\t-2.2724\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 158.39s of the 158.39s of remaining time.\n",
      "\t-2.317\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 151.86s of the 151.86s of remaining time.\n",
      "\t-2.312\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.21s of the 137.67s of remaining time.\n",
      "\t-2.2672\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 42.84s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels0\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -1.6614110076228372\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -1.6614110076228372,\n",
      "    \"mean_squared_error\": -2.760286536250333,\n",
      "    \"mean_absolute_error\": -1.2938924609399793,\n",
      "    \"r2\": 0.6162331780640394,\n",
      "    \"pearsonr\": 0.8150898591647839,\n",
      "    \"median_absolute_error\": -1.059725542967334\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels1\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   71.47 GB / 2000.36 GB (3.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 626\n",
      "Label Column: 626\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (8.5895392385, -22.1172406148, -4.06294, 2.94164)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62301.57 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.28 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 311 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 311 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t311 features in original data used to generate 311 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 25.97 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n",
      "\t-1.2867\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.3s of the 176.3s of remaining time.\n",
      "\t-1.2831\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.96s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 174.28s of the 174.28s of remaining time.\n",
      "\t-1.2684\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 163.21s of the 163.21s of remaining time.\n",
      "\t-1.2823\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 155.48s of the 155.48s of remaining time.\n",
      "\t-1.2734\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the 141.63s of remaining time.\n",
      "\t-1.2587\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 38.85s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels1\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.777717833976649\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.777717833976649,\n",
      "    \"mean_squared_error\": -0.6048450292853302,\n",
      "    \"mean_absolute_error\": -0.5983084102316207,\n",
      "    \"r2\": 0.9300950541802878,\n",
      "    \"pearsonr\": 0.9657636676577096,\n",
      "    \"median_absolute_error\": -0.49055349468549725\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels2\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   71.44 GB / 2000.36 GB (3.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 627\n",
      "Label Column: 627\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.5991351181, -10.7145322745, -0.09315, 2.81791)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62269.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.37 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 312 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 312 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t312 features in original data used to generate 312 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n",
      "\t-1.3605\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.53s of the 176.53s of remaining time.\n",
      "\t-1.3493\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 174.15s of the 174.15s of remaining time.\n",
      "\t-1.3461\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 160.54s of the 160.54s of remaining time.\n",
      "\t-1.3646\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 153.91s of the 153.91s of remaining time.\n",
      "\t-1.3694\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the 143.31s of remaining time.\n",
      "\t-1.3391\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 37.32s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels2\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -1.0657087854074663\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -1.0657087854074663,\n",
      "    \"mean_squared_error\": -1.1357352152946565,\n",
      "    \"mean_absolute_error\": -0.8383232188433826,\n",
      "    \"r2\": 0.8569581551419325,\n",
      "    \"pearsonr\": 0.928019282195689,\n",
      "    \"median_absolute_error\": -0.6999922915708252\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels263\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   71.41 GB / 2000.36 GB (3.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 888\n",
      "Label Column: 888\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.9415250363, -11.3214126103, -1.12561, 2.55632)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62300.2 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.17 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 573 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 573 | ['83', '84', '85', '86', '87', ...]\n",
      "\t1.0s = Fit runtime\n",
      "\t573 features in original data used to generate 573 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.86 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 178.86s of the 178.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.240276\n",
      "[2000]\tvalid_set's rmse: 0.238581\n",
      "[3000]\tvalid_set's rmse: 0.238395\n",
      "[4000]\tvalid_set's rmse: 0.238342\n",
      "[5000]\tvalid_set's rmse: 0.238324\n",
      "[6000]\tvalid_set's rmse: 0.238322\n",
      "[7000]\tvalid_set's rmse: 0.238322\n",
      "[8000]\tvalid_set's rmse: 0.238321\n",
      "[9000]\tvalid_set's rmse: 0.238321\n",
      "[10000]\tvalid_set's rmse: 0.238321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2383\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.93s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 121.92s of the 121.92s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.20734\n",
      "[2000]\tvalid_set's rmse: 0.206584\n",
      "[3000]\tvalid_set's rmse: 0.206554\n",
      "[4000]\tvalid_set's rmse: 0.206539\n",
      "[5000]\tvalid_set's rmse: 0.206539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2065\t = Validation score   (-root_mean_squared_error)\n",
      "\t42.93s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 78.26s of the 78.26s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 2745.\n",
      "\t-0.1739\t = Validation score   (-root_mean_squared_error)\n",
      "\t78.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 178.86s of the -0.65s of remaining time.\n",
      "\t-0.1713\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.44s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels263\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.06168474866571034\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.06168474866571034,\n",
      "    \"mean_squared_error\": -0.0038050082179518618,\n",
      "    \"mean_absolute_error\": -0.03494087002328769,\n",
      "    \"r2\": 0.9994176731977672,\n",
      "    \"pearsonr\": 0.9997088127919815,\n",
      "    \"median_absolute_error\": -0.021399358010022556\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels264\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   71.32 GB / 2000.36 GB (3.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 889\n",
      "Label Column: 889\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.7987780084, -15.2153912372, -2.72649, 3.15755)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62032.2 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.25 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 574 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 574 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t574 features in original data used to generate 574 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.21s of the 179.21s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.490597\n",
      "[2000]\tvalid_set's rmse: 0.486713\n",
      "[3000]\tvalid_set's rmse: 0.486307\n",
      "[4000]\tvalid_set's rmse: 0.486272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4863\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.93s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.74s of the 163.74s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.460627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4603\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.74s of the 156.74s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6818.\n",
      "\t-0.4142\t = Validation score   (-root_mean_squared_error)\n",
      "\t156.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.21s of the -0.86s of remaining time.\n",
      "\t-0.4132\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.54s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels264\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13161363797740103\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13161363797740103,\n",
      "    \"mean_squared_error\": -0.017322149701646264,\n",
      "    \"mean_absolute_error\": -0.04241570391258611,\n",
      "    \"r2\": 0.9982624327062928,\n",
      "    \"pearsonr\": 0.9991328968066444,\n",
      "    \"median_absolute_error\": -0.011174017922827018\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels265\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   71.25 GB / 2000.36 GB (3.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 890\n",
      "Label Column: 890\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (28.034844132, -16.6843196042, -2.6628, 3.47964)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61977.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.33 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 575 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 575 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t575 features in original data used to generate 575 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.02 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.56682\n",
      "[2000]\tvalid_set's rmse: 0.562827\n",
      "[3000]\tvalid_set's rmse: 0.562046\n",
      "[4000]\tvalid_set's rmse: 0.561813\n",
      "[5000]\tvalid_set's rmse: 0.561758\n",
      "[6000]\tvalid_set's rmse: 0.561748\n",
      "[7000]\tvalid_set's rmse: 0.561743\n",
      "[8000]\tvalid_set's rmse: 0.561743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5617\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.4s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 150.04s of the 150.04s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.50216\n",
      "[2000]\tvalid_set's rmse: 0.500325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5003\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 137.07s of the 137.06s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6026.\n",
      "\t-0.4568\t = Validation score   (-root_mean_squared_error)\n",
      "\t137.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.69s of remaining time.\n",
      "\t-0.4553\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.35s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels265\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1448762635478455\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1448762635478455,\n",
      "    \"mean_squared_error\": -0.02098913173958482,\n",
      "    \"mean_absolute_error\": -0.045113339371806706,\n",
      "    \"r2\": 0.998266321448543,\n",
      "    \"pearsonr\": 0.9991346528806248,\n",
      "    \"median_absolute_error\": -0.010394431478613075\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels266\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   71.16 GB / 2000.36 GB (3.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 891\n",
      "Label Column: 891\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.0886111557, -10.6354684905, -0.09573, 2.44829)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61973.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.42 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 576 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 576 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t576 features in original data used to generate 576 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.11 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.426745\n",
      "[2000]\tvalid_set's rmse: 0.421754\n",
      "[3000]\tvalid_set's rmse: 0.42124\n",
      "[4000]\tvalid_set's rmse: 0.421047\n",
      "[5000]\tvalid_set's rmse: 0.421065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.421\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.39s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.06s of the 161.05s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.368022\n",
      "[2000]\tvalid_set's rmse: 0.366741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3667\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 147.67s of the 147.66s of remaining time.\n",
      "\t-0.3384\t = Validation score   (-root_mean_squared_error)\n",
      "\t123.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 23.59s of the 23.59s of remaining time.\n",
      "\t-0.3873\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.75s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.63s of remaining time.\n",
      "\t-0.3379\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.4s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels266\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11027535865386011\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11027535865386011,\n",
      "    \"mean_squared_error\": -0.012160654726237521,\n",
      "    \"mean_absolute_error\": -0.043522827346965685,\n",
      "    \"r2\": 0.9979710450939712,\n",
      "    \"pearsonr\": 0.9989887742503605,\n",
      "    \"median_absolute_error\": -0.018898905947177136\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels267\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   71.09 GB / 2000.36 GB (3.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 892\n",
      "Label Column: 892\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.2399383775, -18.6316706427, -2.10628, 3.53394)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61929.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.5 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 577 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 577 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t577 features in original data used to generate 577 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.19 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.582272\n",
      "[2000]\tvalid_set's rmse: 0.576341\n",
      "[3000]\tvalid_set's rmse: 0.575672\n",
      "[4000]\tvalid_set's rmse: 0.575572\n",
      "[5000]\tvalid_set's rmse: 0.575532\n",
      "[6000]\tvalid_set's rmse: 0.575513\n",
      "[7000]\tvalid_set's rmse: 0.575508\n",
      "[8000]\tvalid_set's rmse: 0.575508\n",
      "[9000]\tvalid_set's rmse: 0.575508\n",
      "[10000]\tvalid_set's rmse: 0.575508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5755\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.27s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 146.61s of the 146.61s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.503372\n",
      "[2000]\tvalid_set's rmse: 0.502381\n",
      "[3000]\tvalid_set's rmse: 0.502271\n",
      "[4000]\tvalid_set's rmse: 0.502231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5022\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 121.79s of the 121.79s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5395.\n",
      "\t-0.4669\t = Validation score   (-root_mean_squared_error)\n",
      "\t121.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.35s of remaining time.\n",
      "\t-0.4644\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels267\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1483481629063746\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1483481629063746,\n",
      "    \"mean_squared_error\": -0.02200717743769604,\n",
      "    \"mean_absolute_error\": -0.04887012261016388,\n",
      "    \"r2\": 0.9982376703913926,\n",
      "    \"pearsonr\": 0.9991202991424204,\n",
      "    \"median_absolute_error\": -0.014268178433202827\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels268\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   71.00 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 893\n",
      "Label Column: 893\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.2361228083, -9.0017988455, -0.66194, 2.39154)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61902.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.58 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 578 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 578 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t578 features in original data used to generate 578 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.27 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.380823\n",
      "[2000]\tvalid_set's rmse: 0.37766\n",
      "[3000]\tvalid_set's rmse: 0.377221\n",
      "[4000]\tvalid_set's rmse: 0.377084\n",
      "[5000]\tvalid_set's rmse: 0.37707\n",
      "[6000]\tvalid_set's rmse: 0.377061\n",
      "[7000]\tvalid_set's rmse: 0.377061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3771\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 153.59s of the 153.59s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.334974\n",
      "[2000]\tvalid_set's rmse: 0.334615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3346\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 140.92s of the 140.91s of remaining time.\n",
      "\t-0.3024\t = Validation score   (-root_mean_squared_error)\n",
      "\t85.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 54.78s of the 54.78s of remaining time.\n",
      "\t-0.3523\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 14.8s of the 14.8s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 486. Best iteration is:\n",
      "\t[486]\tvalid_set's rmse: 0.363547\n",
      "\t-0.3635\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.77s of remaining time.\n",
      "\t-0.3022\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.43s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels268\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10833447398315912\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10833447398315912,\n",
      "    \"mean_squared_error\": -0.01173635825320776,\n",
      "    \"mean_absolute_error\": -0.060021079692671786,\n",
      "    \"r2\": 0.9979478029694842,\n",
      "    \"pearsonr\": 0.9989783976170836,\n",
      "    \"median_absolute_error\": -0.037563263213040265\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels269\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.91 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 894\n",
      "Label Column: 894\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.7996558116, -15.7475243336, -1.477, 3.64509)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61877.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.67 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 579 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 579 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t579 features in original data used to generate 579 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.36 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.914883\n",
      "[2000]\tvalid_set's rmse: 0.905981\n",
      "[3000]\tvalid_set's rmse: 0.905326\n",
      "[4000]\tvalid_set's rmse: 0.90523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9052\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.63s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.2s of the 163.2s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.842784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8416\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 154.78s of the 154.77s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6669.\n",
      "\t-0.7797\t = Validation score   (-root_mean_squared_error)\n",
      "\t154.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.62s of remaining time.\n",
      "\t-0.7797\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.24s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels269\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24734330865512252\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24734330865512252,\n",
      "    \"mean_squared_error\": -0.06117871233646283,\n",
      "    \"mean_absolute_error\": -0.0720364346487992,\n",
      "    \"r2\": 0.9953950409152105,\n",
      "    \"pearsonr\": 0.9976986599679167,\n",
      "    \"median_absolute_error\": -0.012673457006306454\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels270\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.84 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 895\n",
      "Label Column: 895\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.2585797918, -10.7021382114, -0.35144, 2.42835)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61825.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.75 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 580 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 580 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t580 features in original data used to generate 580 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.44 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.58835\n",
      "[2000]\tvalid_set's rmse: 0.583367\n",
      "[3000]\tvalid_set's rmse: 0.582856\n",
      "[4000]\tvalid_set's rmse: 0.582666\n",
      "[5000]\tvalid_set's rmse: 0.582621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5826\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.72s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.03s of the 160.03s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.53304\n",
      "[2000]\tvalid_set's rmse: 0.532105\n",
      "[3000]\tvalid_set's rmse: 0.531959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5319\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 143.93s of the 143.93s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6238.\n",
      "\t-0.5099\t = Validation score   (-root_mean_squared_error)\n",
      "\t144.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.83s of remaining time.\n",
      "\t-0.5084\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.45s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels270\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16144023388846335\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16144023388846335,\n",
      "    \"mean_squared_error\": -0.02606294911796173,\n",
      "    \"mean_absolute_error\": -0.049742623569042174,\n",
      "    \"r2\": 0.995579791568006,\n",
      "    \"pearsonr\": 0.9977912539633057,\n",
      "    \"median_absolute_error\": -0.009524021408636507\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels271\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.76 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 896\n",
      "Label Column: 896\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.1324696981, -16.4081114708, -1.08344, 3.64794)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61845.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.83 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 581 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 581 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t581 features in original data used to generate 581 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.53 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.470184\n",
      "[2000]\tvalid_set's rmse: 0.46747\n",
      "[3000]\tvalid_set's rmse: 0.466899\n",
      "[4000]\tvalid_set's rmse: 0.466832\n",
      "[5000]\tvalid_set's rmse: 0.466825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4668\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.22s of the 161.22s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.364529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3642\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 154.83s of the 154.83s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6699.\n",
      "\t-0.3353\t = Validation score   (-root_mean_squared_error)\n",
      "\t154.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.63s of remaining time.\n",
      "\t-0.3311\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.32s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels271\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10642507996464573\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10642507996464573,\n",
      "    \"mean_squared_error\": -0.01132629764548131,\n",
      "    \"mean_absolute_error\": -0.03614712408719862,\n",
      "    \"r2\": 0.9991487961608136,\n",
      "    \"pearsonr\": 0.9995748568327334,\n",
      "    \"median_absolute_error\": -0.013901135182006918\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels272\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.69 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 897\n",
      "Label Column: 897\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.3704754031, -9.4068994378, 0.12065, 2.50893)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61841.48 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.92 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 582 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 582 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t582 features in original data used to generate 582 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.61 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.22841\n",
      "[2000]\tvalid_set's rmse: 0.226342\n",
      "[3000]\tvalid_set's rmse: 0.22618\n",
      "[4000]\tvalid_set's rmse: 0.226161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2262\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.09s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.43s of the 162.43s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.151862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1517\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.44s of the 156.43s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6676.\n",
      "\t-0.1428\t = Validation score   (-root_mean_squared_error)\n",
      "\t156.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -1.07s of remaining time.\n",
      "\t-0.1377\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.81s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels272\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.04659885241343949\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.04659885241343949,\n",
      "    \"mean_squared_error\": -0.002171453046249517,\n",
      "    \"mean_absolute_error\": -0.021932022079356198,\n",
      "    \"r2\": 0.999655003403396,\n",
      "    \"pearsonr\": 0.9998277227053797,\n",
      "    \"median_absolute_error\": -0.01181102730506578\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels273\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.62 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 898\n",
      "Label Column: 898\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.8548797236, -22.6744589469, -0.95399, 3.9013)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61817.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.0 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 583 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 583 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t583 features in original data used to generate 583 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.69 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.939515\n",
      "[2000]\tvalid_set's rmse: 0.931677\n",
      "[3000]\tvalid_set's rmse: 0.930306\n",
      "[4000]\tvalid_set's rmse: 0.930133\n",
      "[5000]\tvalid_set's rmse: 0.930102\n",
      "[6000]\tvalid_set's rmse: 0.930102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9301\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.11s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 158.27s of the 158.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.829286\n",
      "[2000]\tvalid_set's rmse: 0.826736\n",
      "[3000]\tvalid_set's rmse: 0.826461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8265\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 140.26s of the 140.25s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6034.\n",
      "\t-0.7847\t = Validation score   (-root_mean_squared_error)\n",
      "\t140.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.6s of remaining time.\n",
      "\t-0.7822\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels273\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24834467102200433\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24834467102200433,\n",
      "    \"mean_squared_error\": -0.061675075625027616,\n",
      "    \"mean_absolute_error\": -0.06793339567356574,\n",
      "    \"r2\": 0.995947403844915,\n",
      "    \"pearsonr\": 0.9979772335002934,\n",
      "    \"median_absolute_error\": -0.014322381298730313\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels274\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.54 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 899\n",
      "Label Column: 899\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.7185937408, -7.7360143901, 0.9142, 2.61981)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61845.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.08 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 584 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 584 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t584 features in original data used to generate 584 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.78 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.495593\n",
      "[2000]\tvalid_set's rmse: 0.49095\n",
      "[3000]\tvalid_set's rmse: 0.49064\n",
      "[4000]\tvalid_set's rmse: 0.490575\n",
      "[5000]\tvalid_set's rmse: 0.490566\n",
      "[6000]\tvalid_set's rmse: 0.490554\n",
      "[7000]\tvalid_set's rmse: 0.490553\n",
      "[8000]\tvalid_set's rmse: 0.490553\n",
      "[9000]\tvalid_set's rmse: 0.490552\n",
      "[10000]\tvalid_set's rmse: 0.490552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4906\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.74s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 147.07s of the 147.07s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.450068\n",
      "[2000]\tvalid_set's rmse: 0.448824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4487\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 132.5s of the 132.49s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5725.\n",
      "\t-0.4256\t = Validation score   (-root_mean_squared_error)\n",
      "\t132.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.8s of remaining time.\n",
      "\t-0.4245\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.49s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels274\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13631673521462112\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13631673521462112,\n",
      "    \"mean_squared_error\": -0.01858225229957312,\n",
      "    \"mean_absolute_error\": -0.0492194884511788,\n",
      "    \"r2\": 0.9972922971400592,\n",
      "    \"pearsonr\": 0.9986482392293548,\n",
      "    \"median_absolute_error\": -0.015976910319543514\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels275\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.45 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 900\n",
      "Label Column: 900\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.8898168419, -23.3831935531, -1.13323, 3.7833)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61782.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.17 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 585 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 585 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t585 features in original data used to generate 585 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.86 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.478392\n",
      "[2000]\tvalid_set's rmse: 0.476063\n",
      "[3000]\tvalid_set's rmse: 0.475761\n",
      "[4000]\tvalid_set's rmse: 0.475712\n",
      "[5000]\tvalid_set's rmse: 0.475728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4757\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.19s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.64s of the 162.64s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.374263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.374\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.03s of the 156.02s of remaining time.\n",
      "\t-0.3355\t = Validation score   (-root_mean_squared_error)\n",
      "\t122.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 32.96s of the 32.96s of remaining time.\n",
      "\t-0.4187\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 13.87s of the 13.87s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 440. Best iteration is:\n",
      "\t[440]\tvalid_set's rmse: 0.430346\n",
      "\t-0.4303\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.39s of remaining time.\n",
      "\t-0.3319\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.01s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels275\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10985852238817599\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10985852238817599,\n",
      "    \"mean_squared_error\": -0.01206889494131335,\n",
      "    \"mean_absolute_error\": -0.04504015259880827,\n",
      "    \"r2\": 0.9991567266259166,\n",
      "    \"pearsonr\": 0.9995798281176371,\n",
      "    \"median_absolute_error\": -0.023476869276758183\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels276\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.37 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 901\n",
      "Label Column: 901\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.2724703533, -7.3240663859, 0.61792, 2.65306)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61811.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.25 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 586 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 586 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t586 features in original data used to generate 586 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.32259\n",
      "[2000]\tvalid_set's rmse: 0.319442\n",
      "[3000]\tvalid_set's rmse: 0.319091\n",
      "[4000]\tvalid_set's rmse: 0.318963\n",
      "[5000]\tvalid_set's rmse: 0.318963\n",
      "[6000]\tvalid_set's rmse: 0.318947\n",
      "[7000]\tvalid_set's rmse: 0.318951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3189\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.13s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 153.12s of the 153.11s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.289952\n",
      "[2000]\tvalid_set's rmse: 0.289436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2894\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 139.58s of the 139.58s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6001.\n",
      "\t-0.264\t = Validation score   (-root_mean_squared_error)\n",
      "\t139.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.85s of remaining time.\n",
      "\t-0.263\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels276\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.0838342452780048\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.0838342452780048,\n",
      "    \"mean_squared_error\": -0.007028180681332679,\n",
      "    \"mean_absolute_error\": -0.02783108170599137,\n",
      "    \"r2\": 0.9990013998528722,\n",
      "    \"pearsonr\": 0.9995008938351169,\n",
      "    \"median_absolute_error\": -0.007131366622558588\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels277\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.29 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 902\n",
      "Label Column: 902\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.572474719, -26.0838963755, -1.88158, 3.47585)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61868.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.34 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 587 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 587 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t587 features in original data used to generate 587 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.03 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.881865\n",
      "[2000]\tvalid_set's rmse: 0.873995\n",
      "[3000]\tvalid_set's rmse: 0.873297\n",
      "[4000]\tvalid_set's rmse: 0.873264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8732\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.58s of the 162.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.775142\n",
      "[2000]\tvalid_set's rmse: 0.772557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7725\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 149.12s of the 149.12s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6654.\n",
      "\t-0.7281\t = Validation score   (-root_mean_squared_error)\n",
      "\t149.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.65s of remaining time.\n",
      "\t-0.7269\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.49s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels277\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2307951993740861\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2307951993740861,\n",
      "    \"mean_squared_error\": -0.05326642405412424,\n",
      "    \"mean_absolute_error\": -0.07004458765321961,\n",
      "    \"r2\": 0.9955906640314975,\n",
      "    \"pearsonr\": 0.9978002982070124,\n",
      "    \"median_absolute_error\": -0.013286976114318838\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels278\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.22 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 903\n",
      "Label Column: 903\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.8272732398, -7.1825509929, 0.31765, 2.63984)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62042.6 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.42 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 588 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 588 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t588 features in original data used to generate 588 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.11 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.284479\n",
      "[2000]\tvalid_set's rmse: 0.281489\n",
      "[3000]\tvalid_set's rmse: 0.28118\n",
      "[4000]\tvalid_set's rmse: 0.281072\n",
      "[5000]\tvalid_set's rmse: 0.281059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2811\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.36s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.33s of the 160.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.233086\n",
      "[2000]\tvalid_set's rmse: 0.232606\n",
      "[3000]\tvalid_set's rmse: 0.23258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2326\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.99s of the 144.98s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6452.\n",
      "\t-0.2213\t = Validation score   (-root_mean_squared_error)\n",
      "\t145.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.87s of remaining time.\n",
      "\t-0.2174\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.71s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels278\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.06894928692798577\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.06894928692798577,\n",
      "    \"mean_squared_error\": -0.004754004167877728,\n",
      "    \"mean_absolute_error\": -0.01984348865108761,\n",
      "    \"r2\": 0.999317747329575,\n",
      "    \"pearsonr\": 0.9996588512656862,\n",
      "    \"median_absolute_error\": -0.0033088473771294286\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels279\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.14 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 904\n",
      "Label Column: 904\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.8451090018, -16.7831701683, -1.78645, 3.10694)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62058.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.5 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 589 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 589 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t589 features in original data used to generate 589 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.19 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.536327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5336\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.82s of the 171.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.483836\n",
      "[2000]\tvalid_set's rmse: 0.483007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4828\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.87s of the 159.87s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7034.\n",
      "\t-0.4385\t = Validation score   (-root_mean_squared_error)\n",
      "\t160.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -0.73s of remaining time.\n",
      "\t-0.4382\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.68s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels279\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1390800521110997\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1390800521110997,\n",
      "    \"mean_squared_error\": -0.019343260895226227,\n",
      "    \"mean_absolute_error\": -0.04138624658318813,\n",
      "    \"r2\": 0.9979959699177736,\n",
      "    \"pearsonr\": 0.998997816454148,\n",
      "    \"median_absolute_error\": -0.007644694578663638\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels280\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.08 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 905\n",
      "Label Column: 905\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.5903355177, -7.4935662498, 0.04487, 2.52764)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62050.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.59 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 590 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 590 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t590 features in original data used to generate 590 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.28 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.289893\n",
      "[2000]\tvalid_set's rmse: 0.286442\n",
      "[3000]\tvalid_set's rmse: 0.286205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2861\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.53s of the 168.53s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.235692\n",
      "[2000]\tvalid_set's rmse: 0.235055\n",
      "[3000]\tvalid_set's rmse: 0.235021\n",
      "[4000]\tvalid_set's rmse: 0.235024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.235\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 148.37s of the 148.36s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6573.\n",
      "\t-0.2288\t = Validation score   (-root_mean_squared_error)\n",
      "\t148.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.85s of remaining time.\n",
      "\t-0.2219\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.67s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels280\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.07037817770273477\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.07037817770273477,\n",
      "    \"mean_squared_error\": -0.004953087896757703,\n",
      "    \"mean_absolute_error\": -0.02079340398969761,\n",
      "    \"r2\": 0.9992246713149253,\n",
      "    \"pearsonr\": 0.9996123427068069,\n",
      "    \"median_absolute_error\": -0.0036266566309084425\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels281\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   70.00 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 906\n",
      "Label Column: 906\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.1678788914, -15.894692661, -1.39614, 2.87229)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62037.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.67 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 591 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 591 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t591 features in original data used to generate 591 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.36 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.469698\n",
      "[2000]\tvalid_set's rmse: 0.466225\n",
      "[3000]\tvalid_set's rmse: 0.46586\n",
      "[4000]\tvalid_set's rmse: 0.465798\n",
      "[5000]\tvalid_set's rmse: 0.465748\n",
      "[6000]\tvalid_set's rmse: 0.465733\n",
      "[7000]\tvalid_set's rmse: 0.465732\n",
      "[8000]\tvalid_set's rmse: 0.465733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4657\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.74s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 152.47s of the 152.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.389487\n",
      "[2000]\tvalid_set's rmse: 0.388881\n",
      "[3000]\tvalid_set's rmse: 0.388813\n",
      "[4000]\tvalid_set's rmse: 0.388765\n",
      "[5000]\tvalid_set's rmse: 0.388756\n",
      "[6000]\tvalid_set's rmse: 0.388751\n",
      "[7000]\tvalid_set's rmse: 0.388753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3888\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.58s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 111.84s of the 111.84s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5051.\n",
      "\t-0.3616\t = Validation score   (-root_mean_squared_error)\n",
      "\t111.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.82s of remaining time.\n",
      "\t-0.3594\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.65s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels281\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11491347296569979\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11491347296569979,\n",
      "    \"mean_squared_error\": -0.013205106269038636,\n",
      "    \"mean_absolute_error\": -0.03860385168835788,\n",
      "    \"r2\": 0.9983992355910705,\n",
      "    \"pearsonr\": 0.9992021367570219,\n",
      "    \"median_absolute_error\": -0.011566785558935477\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels282\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.91 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 907\n",
      "Label Column: 907\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.3201590325, -7.7131985408, -0.28852, 2.43576)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62045.1 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.75 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 592 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 592 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t592 features in original data used to generate 592 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.44 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.29996\n",
      "[2000]\tvalid_set's rmse: 0.296909\n",
      "[3000]\tvalid_set's rmse: 0.296404\n",
      "[4000]\tvalid_set's rmse: 0.296352\n",
      "[5000]\tvalid_set's rmse: 0.296333\n",
      "[6000]\tvalid_set's rmse: 0.29633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2963\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 158.4s of the 158.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.259641\n",
      "[2000]\tvalid_set's rmse: 0.259344\n",
      "[3000]\tvalid_set's rmse: 0.259309\n",
      "[4000]\tvalid_set's rmse: 0.259301\n",
      "[5000]\tvalid_set's rmse: 0.259299\n",
      "[6000]\tvalid_set's rmse: 0.259298\n",
      "[7000]\tvalid_set's rmse: 0.259298\n",
      "[8000]\tvalid_set's rmse: 0.259298\n",
      "[9000]\tvalid_set's rmse: 0.259298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2593\t = Validation score   (-root_mean_squared_error)\n",
      "\t47.21s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 109.99s of the 109.98s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 4945.\n",
      "\t-0.2387\t = Validation score   (-root_mean_squared_error)\n",
      "\t110.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.86s of remaining time.\n",
      "\t-0.2366\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.75s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels282\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.07677764915236233\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.07677764915236233,\n",
      "    \"mean_squared_error\": -0.005894807409363222,\n",
      "    \"mean_absolute_error\": -0.030745383121100155,\n",
      "    \"r2\": 0.9990063306318895,\n",
      "    \"pearsonr\": 0.9995032991969611,\n",
      "    \"median_absolute_error\": -0.012122816207490533\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels283\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.82 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 908\n",
      "Label Column: 908\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (9.1354480787, -16.4137251529, -0.77507, 2.90028)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62041.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.84 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 593 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 593 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t593 features in original data used to generate 593 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.53 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.473108\n",
      "[2000]\tvalid_set's rmse: 0.468057\n",
      "[3000]\tvalid_set's rmse: 0.467554\n",
      "[4000]\tvalid_set's rmse: 0.46742\n",
      "[5000]\tvalid_set's rmse: 0.467385\n",
      "[6000]\tvalid_set's rmse: 0.467368\n",
      "[7000]\tvalid_set's rmse: 0.467367\n",
      "[8000]\tvalid_set's rmse: 0.467367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4674\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.4s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 152.86s of the 152.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.410032\n",
      "[2000]\tvalid_set's rmse: 0.408129\n",
      "[3000]\tvalid_set's rmse: 0.407967\n",
      "[4000]\tvalid_set's rmse: 0.407952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4079\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 127.62s of the 127.62s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5717.\n",
      "\t-0.391\t = Validation score   (-root_mean_squared_error)\n",
      "\t127.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.85s of remaining time.\n",
      "\t-0.3851\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.72s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels283\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.12229964505402283\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.12229964505402283,\n",
      "    \"mean_squared_error\": -0.014957203180339974,\n",
      "    \"mean_absolute_error\": -0.03637931445400271,\n",
      "    \"r2\": 0.9982216718649322,\n",
      "    \"pearsonr\": 0.9991114560411622,\n",
      "    \"median_absolute_error\": -0.00738822961329344\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels284\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.73 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 909\n",
      "Label Column: 909\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (9.4774358118, -9.3117663681, -0.6737, 2.48334)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62058.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.92 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 594 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 594 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t594 features in original data used to generate 594 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.61 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.313585\n",
      "[2000]\tvalid_set's rmse: 0.309835\n",
      "[3000]\tvalid_set's rmse: 0.309151\n",
      "[4000]\tvalid_set's rmse: 0.309111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3091\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.96s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.9s of the 164.9s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.277895\n",
      "[2000]\tvalid_set's rmse: 0.277388\n",
      "[3000]\tvalid_set's rmse: 0.277264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2773\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.51s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.97s of the 144.97s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6387.\n",
      "\t-0.2414\t = Validation score   (-root_mean_squared_error)\n",
      "\t145.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.63s of remaining time.\n",
      "\t-0.2414\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.42s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels284\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.07676637166606343\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.07676637166606343,\n",
      "    \"mean_squared_error\": -0.005893075818772172,\n",
      "    \"mean_absolute_error\": -0.02454934012605574,\n",
      "    \"r2\": 0.9990443229371485,\n",
      "    \"pearsonr\": 0.9995221244101242,\n",
      "    \"median_absolute_error\": -0.005372411867273719\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels285\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.66 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 910\n",
      "Label Column: 910\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.3587601829, -17.5353009039, -0.12957, 3.16986)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62048.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.0 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 595 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 595 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t595 features in original data used to generate 595 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.69 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.34s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.516249\n",
      "[2000]\tvalid_set's rmse: 0.510833\n",
      "[3000]\tvalid_set's rmse: 0.510458\n",
      "[4000]\tvalid_set's rmse: 0.51034\n",
      "[5000]\tvalid_set's rmse: 0.510319\n",
      "[6000]\tvalid_set's rmse: 0.510315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5103\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.63s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.06s of the 159.06s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.430471\n",
      "[2000]\tvalid_set's rmse: 0.430082\n",
      "[3000]\tvalid_set's rmse: 0.429992\n",
      "[4000]\tvalid_set's rmse: 0.429964\n",
      "[5000]\tvalid_set's rmse: 0.429959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.43\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.3s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 129.88s of the 129.88s of remaining time.\n",
      "\t-0.3995\t = Validation score   (-root_mean_squared_error)\n",
      "\t110.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 19.11s of the 19.11s of remaining time.\n",
      "\t-0.4672\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.34s of the -0.31s of remaining time.\n",
      "\t-0.3979\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels285\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13178265266808686\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13178265266808686,\n",
      "    \"mean_squared_error\": -0.017366667544237574,\n",
      "    \"mean_absolute_error\": -0.05795437996486133,\n",
      "    \"r2\": 0.9982714690325972,\n",
      "    \"pearsonr\": 0.9991388564824245,\n",
      "    \"median_absolute_error\": -0.027673352084698455\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels286\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.57 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 911\n",
      "Label Column: 911\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.5148249722, -11.3338127122, -0.97466, 2.66749)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62029.8 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.09 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 596 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 596 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t596 features in original data used to generate 596 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.78 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.409261\n",
      "[2000]\tvalid_set's rmse: 0.404268\n",
      "[3000]\tvalid_set's rmse: 0.403915\n",
      "[4000]\tvalid_set's rmse: 0.403748\n",
      "[5000]\tvalid_set's rmse: 0.403702\n",
      "[6000]\tvalid_set's rmse: 0.403702\n",
      "[7000]\tvalid_set's rmse: 0.4037\n",
      "[8000]\tvalid_set's rmse: 0.403697\n",
      "[9000]\tvalid_set's rmse: 0.403697\n",
      "[10000]\tvalid_set's rmse: 0.403697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4037\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.32s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 146.82s of the 146.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.356213\n",
      "[2000]\tvalid_set's rmse: 0.355206\n",
      "[3000]\tvalid_set's rmse: 0.355086\n",
      "[4000]\tvalid_set's rmse: 0.355074\n",
      "[5000]\tvalid_set's rmse: 0.35507\n",
      "[6000]\tvalid_set's rmse: 0.35507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3551\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 113.93s of the 113.93s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5084.\n",
      "\t-0.3284\t = Validation score   (-root_mean_squared_error)\n",
      "\t114.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.31s of remaining time.\n",
      "\t-0.3277\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.26s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels286\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10543544057844363\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10543544057844363,\n",
      "    \"mean_squared_error\": -0.011116632129970478,\n",
      "    \"mean_absolute_error\": -0.03893403388010991,\n",
      "    \"r2\": 0.9984375440087857,\n",
      "    \"pearsonr\": 0.9992200696325146,\n",
      "    \"median_absolute_error\": -0.013331409880926626\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels287\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.47 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 912\n",
      "Label Column: 912\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.7257015058, -20.5908843286, 0.25641, 3.50229)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61967.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.17 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 597 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 597 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t597 features in original data used to generate 597 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.86 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.442299\n",
      "[2000]\tvalid_set's rmse: 0.43858\n",
      "[3000]\tvalid_set's rmse: 0.438351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4382\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.02s of the 166.01s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.403905\n",
      "[2000]\tvalid_set's rmse: 0.403431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4034\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 153.83s of the 153.83s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6707.\n",
      "\t-0.3518\t = Validation score   (-root_mean_squared_error)\n",
      "\t153.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.89s of remaining time.\n",
      "\t-0.3512\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.78s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels287\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11154551068726666\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11154551068726666,\n",
      "    \"mean_squared_error\": -0.012442400954483106,\n",
      "    \"mean_absolute_error\": -0.034067993196388924,\n",
      "    \"r2\": 0.9989855224593615,\n",
      "    \"pearsonr\": 0.9994933621182831,\n",
      "    \"median_absolute_error\": -0.006836282353549217\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels288\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.40 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 913\n",
      "Label Column: 913\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.2185593597, -10.1273270007, -0.40842, 2.72703)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62024.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.25 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 598 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 598 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t598 features in original data used to generate 598 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.95 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.329243\n",
      "[2000]\tvalid_set's rmse: 0.326228\n",
      "[3000]\tvalid_set's rmse: 0.325879\n",
      "[4000]\tvalid_set's rmse: 0.325791\n",
      "[5000]\tvalid_set's rmse: 0.325753\n",
      "[6000]\tvalid_set's rmse: 0.32574\n",
      "[7000]\tvalid_set's rmse: 0.325738\n",
      "[8000]\tvalid_set's rmse: 0.325737\n",
      "[9000]\tvalid_set's rmse: 0.325737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3257\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.9s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 147.38s of the 147.38s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.300453\n",
      "[2000]\tvalid_set's rmse: 0.299787\n",
      "[3000]\tvalid_set's rmse: 0.299683\n",
      "[4000]\tvalid_set's rmse: 0.299657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2997\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.77s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 121.82s of the 121.82s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5407.\n",
      "\t-0.2746\t = Validation score   (-root_mean_squared_error)\n",
      "\t121.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.64s of remaining time.\n",
      "\t-0.2716\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.49s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels288\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.08670781821514534\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.08670781821514534,\n",
      "    \"mean_squared_error\": -0.007518245739630661,\n",
      "    \"mean_absolute_error\": -0.029327215684926227,\n",
      "    \"r2\": 0.9989889403558047,\n",
      "    \"pearsonr\": 0.9994950404828128,\n",
      "    \"median_absolute_error\": -0.008201089777282733\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels289\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.31 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 914\n",
      "Label Column: 914\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.5822807064, -23.5418501214, 0.51728, 3.69017)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62034.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.34 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 599 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 599 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t599 features in original data used to generate 599 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.03 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.611405\n",
      "[2000]\tvalid_set's rmse: 0.605401\n",
      "[3000]\tvalid_set's rmse: 0.60454\n",
      "[4000]\tvalid_set's rmse: 0.604313\n",
      "[5000]\tvalid_set's rmse: 0.604296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6043\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.51s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.22s of the 160.22s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.512857\n",
      "[2000]\tvalid_set's rmse: 0.511848\n",
      "[3000]\tvalid_set's rmse: 0.5118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5118\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 140.07s of the 140.07s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6140.\n",
      "\t-0.4915\t = Validation score   (-root_mean_squared_error)\n",
      "\t140.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.88s of remaining time.\n",
      "\t-0.484\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels289\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15349159523271175\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15349159523271175,\n",
      "    \"mean_squared_error\": -0.023559669807082555,\n",
      "    \"mean_absolute_error\": -0.04417690384063797,\n",
      "    \"r2\": 0.9982697169351085,\n",
      "    \"pearsonr\": 0.9991353282901968,\n",
      "    \"median_absolute_error\": -0.007475907073913657\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels290\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.23 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 915\n",
      "Label Column: 915\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.3711940221, -11.2325768164, -0.49221, 2.90585)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62091.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.42 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 600 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 600 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t600 features in original data used to generate 600 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.11 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.410912\n",
      "[2000]\tvalid_set's rmse: 0.407263\n",
      "[3000]\tvalid_set's rmse: 0.407075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.407\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.37s of the 167.37s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.368531\n",
      "[2000]\tvalid_set's rmse: 0.367812\n",
      "[3000]\tvalid_set's rmse: 0.367762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3677\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.86s of the 150.86s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6558.\n",
      "\t-0.3229\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.66s of remaining time.\n",
      "\t-0.3228\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels290\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10243117830787887\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10243117830787887,\n",
      "    \"mean_squared_error\": -0.010492146289540517,\n",
      "    \"mean_absolute_error\": -0.030286801429656326,\n",
      "    \"r2\": 0.9987573192895188,\n",
      "    \"pearsonr\": 0.9993788738614653,\n",
      "    \"median_absolute_error\": -0.0055283537755002055\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels291\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.16 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 916\n",
      "Label Column: 916\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.1198691877, -18.1834154735, 0.92141, 3.4754)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62029.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.5 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 601 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 601 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t601 features in original data used to generate 601 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.2 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.479211\n",
      "[2000]\tvalid_set's rmse: 0.476597\n",
      "[3000]\tvalid_set's rmse: 0.476189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4761\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 165.62s of the 165.62s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.430585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4304\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 157.22s of the 157.22s of remaining time.\n",
      "\t-0.3841\t = Validation score   (-root_mean_squared_error)\n",
      "\t101.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 55.34s of the 55.34s of remaining time.\n",
      "\t-0.4645\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.33s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 13.49s of the 13.49s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 426. Best iteration is:\n",
      "\t[426]\tvalid_set's rmse: 0.476168\n",
      "\t-0.4762\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.65s of remaining time.\n",
      "\t-0.3819\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.51s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels291\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1303257051119012\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1303257051119012,\n",
      "    \"mean_squared_error\": -0.016984789412914262,\n",
      "    \"mean_absolute_error\": -0.06481565070464762,\n",
      "    \"r2\": 0.9985936570593845,\n",
      "    \"pearsonr\": 0.9992986147359588,\n",
      "    \"median_absolute_error\": -0.03571761019744868\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels292\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.08 GB / 2000.36 GB (3.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 917\n",
      "Label Column: 917\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.3389781539, -17.0471702341, -1.06989, 3.28586)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61990.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.59 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 602 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 602 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t602 features in original data used to generate 602 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.28 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.505205\n",
      "[2000]\tvalid_set's rmse: 0.500384\n",
      "[3000]\tvalid_set's rmse: 0.499542\n",
      "[4000]\tvalid_set's rmse: 0.499461\n",
      "[5000]\tvalid_set's rmse: 0.499443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4994\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.63s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.07s of the 159.07s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.445818\n",
      "[2000]\tvalid_set's rmse: 0.445455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4453\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 146.84s of the 146.84s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6309.\n",
      "\t-0.3977\t = Validation score   (-root_mean_squared_error)\n",
      "\t146.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.64s of remaining time.\n",
      "\t-0.3976\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.5s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels292\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1263080741811093\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1263080741811093,\n",
      "    \"mean_squared_error\": -0.015953729603340554,\n",
      "    \"mean_absolute_error\": -0.03816319000935521,\n",
      "    \"r2\": 0.9985222354324476,\n",
      "    \"pearsonr\": 0.9992616531065261,\n",
      "    \"median_absolute_error\": -0.007992441411132534\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels293\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   69.00 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 918\n",
      "Label Column: 918\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.8692121661, -21.1347351408, 1.12412, 3.56925)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61991.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.67 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 603 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 603 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t603 features in original data used to generate 603 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.36 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.622445\n",
      "[2000]\tvalid_set's rmse: 0.616866\n",
      "[3000]\tvalid_set's rmse: 0.615992\n",
      "[4000]\tvalid_set's rmse: 0.615786\n",
      "[5000]\tvalid_set's rmse: 0.615745\n",
      "[6000]\tvalid_set's rmse: 0.615742\n",
      "[7000]\tvalid_set's rmse: 0.615738\n",
      "[8000]\tvalid_set's rmse: 0.615736\n",
      "[9000]\tvalid_set's rmse: 0.615736\n",
      "[10000]\tvalid_set's rmse: 0.615736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6157\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.15s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 145.79s of the 145.79s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.523074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5225\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 137.3s of the 137.3s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5948.\n",
      "\t-0.4825\t = Validation score   (-root_mean_squared_error)\n",
      "\t137.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.86s of remaining time.\n",
      "\t-0.4812\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.73s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels293\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1535951154970949\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1535951154970949,\n",
      "    \"mean_squared_error\": -0.023591459504565922,\n",
      "    \"mean_absolute_error\": -0.05011394520219565,\n",
      "    \"r2\": 0.9981479975112288,\n",
      "    \"pearsonr\": 0.9990758037923984,\n",
      "    \"median_absolute_error\": -0.014549704887768411\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels294\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.92 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 919\n",
      "Label Column: 919\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.9608829802, -17.0251660014, -1.32395, 3.22579)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61993.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.76 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 604 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 604 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t604 features in original data used to generate 604 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.45 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.535902\n",
      "[2000]\tvalid_set's rmse: 0.532355\n",
      "[3000]\tvalid_set's rmse: 0.532161\n",
      "[4000]\tvalid_set's rmse: 0.532039\n",
      "[5000]\tvalid_set's rmse: 0.532056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.532\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.0s of the 161.0s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.465071\n",
      "[2000]\tvalid_set's rmse: 0.463349\n",
      "[3000]\tvalid_set's rmse: 0.463229\n",
      "[4000]\tvalid_set's rmse: 0.463217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4632\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 139.62s of the 139.62s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6052.\n",
      "\t-0.4255\t = Validation score   (-root_mean_squared_error)\n",
      "\t139.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.66s of remaining time.\n",
      "\t-0.4248\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.47s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels294\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13491225859315847\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13491225859315847,\n",
      "    \"mean_squared_error\": -0.018201317518707252,\n",
      "    \"mean_absolute_error\": -0.03942749014623516,\n",
      "    \"r2\": 0.9982506689034649,\n",
      "    \"pearsonr\": 0.9991259974914393,\n",
      "    \"median_absolute_error\": -0.008327719356079122\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels295\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.84 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 920\n",
      "Label Column: 920\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (24.4197826506, -17.1035896711, 0.35006, 3.40927)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62015.7 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.84 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 605 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 605 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t605 features in original data used to generate 605 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.53 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.704558\n",
      "[2000]\tvalid_set's rmse: 0.701278\n",
      "[3000]\tvalid_set's rmse: 0.700879\n",
      "[4000]\tvalid_set's rmse: 0.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7008\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.48s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.27s of the 162.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.634884\n",
      "[2000]\tvalid_set's rmse: 0.634087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.634\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 149.31s of the 149.31s of remaining time.\n",
      "\t-0.6129\t = Validation score   (-root_mean_squared_error)\n",
      "\t116.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 32.68s of the 32.68s of remaining time.\n",
      "\t-0.6627\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.86s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -0.37s of remaining time.\n",
      "\t-0.6103\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.24s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels295\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19670245286345445\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19670245286345445,\n",
      "    \"mean_squared_error\": -0.038691854962499404,\n",
      "    \"mean_absolute_error\": -0.0608063579980641,\n",
      "    \"r2\": 0.9966708023255544,\n",
      "    \"pearsonr\": 0.9983413717899423,\n",
      "    \"median_absolute_error\": -0.026738763501330753\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels296\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.76 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 921\n",
      "Label Column: 921\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.8339903363, -14.9974528337, 0.0212, 2.91466)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61979.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.92 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 606 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 606 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t606 features in original data used to generate 606 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.61 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.774883\n",
      "[2000]\tvalid_set's rmse: 0.770174\n",
      "[3000]\tvalid_set's rmse: 0.769656\n",
      "[4000]\tvalid_set's rmse: 0.769489\n",
      "[5000]\tvalid_set's rmse: 0.769414\n",
      "[6000]\tvalid_set's rmse: 0.769394\n",
      "[7000]\tvalid_set's rmse: 0.76939\n",
      "[8000]\tvalid_set's rmse: 0.769386\n",
      "[9000]\tvalid_set's rmse: 0.769385\n",
      "[10000]\tvalid_set's rmse: 0.769385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7694\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.8s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 144.91s of the 144.91s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.698266\n",
      "[2000]\tvalid_set's rmse: 0.69679\n",
      "[3000]\tvalid_set's rmse: 0.696506\n",
      "[4000]\tvalid_set's rmse: 0.696482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6965\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.27s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 119.83s of the 119.83s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5234.\n",
      "\t-0.6562\t = Validation score   (-root_mean_squared_error)\n",
      "\t119.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.64s of remaining time.\n",
      "\t-0.6551\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.6s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels296\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2100194186306166\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2100194186306166,\n",
      "    \"mean_squared_error\": -0.04410815620194227,\n",
      "    \"mean_absolute_error\": -0.07326410079855143,\n",
      "    \"r2\": 0.9948074001780922,\n",
      "    \"pearsonr\": 0.9974146931110186,\n",
      "    \"median_absolute_error\": -0.02380393396659697\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels297\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.66 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 922\n",
      "Label Column: 922\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.9563634512, -11.6253352794, 0.53879, 2.8768)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61989.1 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.01 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 607 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 607 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t607 features in original data used to generate 607 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.7 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.459088\n",
      "[2000]\tvalid_set's rmse: 0.456769\n",
      "[3000]\tvalid_set's rmse: 0.456519\n",
      "[4000]\tvalid_set's rmse: 0.456439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4564\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.09s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.72s of the 163.72s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.365647\n",
      "[2000]\tvalid_set's rmse: 0.365235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3652\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 149.62s of the 149.62s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6422.\n",
      "\t-0.3268\t = Validation score   (-root_mean_squared_error)\n",
      "\t149.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.9s of remaining time.\n",
      "\t-0.3254\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.8s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels297\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10324543723684926\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10324543723684926,\n",
      "    \"mean_squared_error\": -0.010659620310228147,\n",
      "    \"mean_absolute_error\": -0.02742997792806994,\n",
      "    \"r2\": 0.9987118611828755,\n",
      "    \"pearsonr\": 0.9993567014988559,\n",
      "    \"median_absolute_error\": -0.00558760946343384\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels298\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.59 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 923\n",
      "Label Column: 923\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.3216562578, -11.1317696764, 0.63528, 3.05366)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61997.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.09 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 608 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 608 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t608 features in original data used to generate 608 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.78 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.493067\n",
      "[2000]\tvalid_set's rmse: 0.488992\n",
      "[3000]\tvalid_set's rmse: 0.488603\n",
      "[4000]\tvalid_set's rmse: 0.488454\n",
      "[5000]\tvalid_set's rmse: 0.488416\n",
      "[6000]\tvalid_set's rmse: 0.488398\n",
      "[7000]\tvalid_set's rmse: 0.488398\n",
      "[8000]\tvalid_set's rmse: 0.488398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4884\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 151.58s of the 151.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.47416\n",
      "[2000]\tvalid_set's rmse: 0.473236\n",
      "[3000]\tvalid_set's rmse: 0.473143\n",
      "[4000]\tvalid_set's rmse: 0.473121\n",
      "[5000]\tvalid_set's rmse: 0.473115\n",
      "[6000]\tvalid_set's rmse: 0.473114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4731\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.77s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 114.81s of the 114.81s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5008.\n",
      "\t-0.4243\t = Validation score   (-root_mean_squared_error)\n",
      "\t114.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.85s of remaining time.\n",
      "\t-0.4243\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.71s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels298\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13656688896685823\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13656688896685823,\n",
      "    \"mean_squared_error\": -0.018650515162086215,\n",
      "    \"mean_absolute_error\": -0.05021011116196528,\n",
      "    \"r2\": 0.9979997195078286,\n",
      "    \"pearsonr\": 0.9990019152789134,\n",
      "    \"median_absolute_error\": -0.01729304276637572\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels299\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.50 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 924\n",
      "Label Column: 924\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.8726530827, -15.8830232556, -0.20443, 3.44065)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62075.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.17 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 609 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 609 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t609 features in original data used to generate 609 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.86 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.633456\n",
      "[2000]\tvalid_set's rmse: 0.630209\n",
      "[3000]\tvalid_set's rmse: 0.629821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6298\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.48s of the 166.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.550897\n",
      "[2000]\tvalid_set's rmse: 0.549631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5496\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 151.13s of the 151.13s of remaining time.\n",
      "\t-0.5295\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 0.4s of the 0.39s of remaining time.\n",
      "\t-3.1845\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.2s of remaining time.\n",
      "\t-0.5257\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.1s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels299\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1675293692543157\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1675293692543157,\n",
      "    \"mean_squared_error\": -0.028066089562748912,\n",
      "    \"mean_absolute_error\": -0.0479807617460981,\n",
      "    \"r2\": 0.9976289431606127,\n",
      "    \"pearsonr\": 0.9988154953671646,\n",
      "    \"median_absolute_error\": -0.013926031497747848\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels300\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.43 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 925\n",
      "Label Column: 925\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.039529445, -16.5453604414, -0.21705, 3.66081)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61992.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.26 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 610 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 610 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t610 features in original data used to generate 610 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.95 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.790913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7888\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.11s of the 171.11s of remaining time.\n",
      "\t-0.6753\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 165.42s of the 165.41s of remaining time.\n",
      "\t-0.6544\t = Validation score   (-root_mean_squared_error)\n",
      "\t126.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 38.47s of the 38.47s of remaining time.\n",
      "\t-0.7103\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.92s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 19.14s of the 19.14s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 635. Best iteration is:\n",
      "\t[627]\tvalid_set's rmse: 0.714303\n",
      "\t-0.7143\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.45s of remaining time.\n",
      "\t-0.6468\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.29s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels300\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22137977465293665\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22137977465293665,\n",
      "    \"mean_squared_error\": -0.04900900462538484,\n",
      "    \"mean_absolute_error\": -0.10487698034636811,\n",
      "    \"r2\": 0.9963426832772078,\n",
      "    \"pearsonr\": 0.9981874768030647,\n",
      "    \"median_absolute_error\": -0.061841131855285725\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels301\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.35 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 926\n",
      "Label Column: 926\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (26.9905929864, -20.9532470222, -1.18761, 4.00178)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61981.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.34 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 611 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 611 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t611 features in original data used to generate 611 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.03 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.998852\n",
      "[2000]\tvalid_set's rmse: 0.993664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9932\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.66s of the 168.65s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.92131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.92\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 158.51s of the 158.51s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6751.\n",
      "\t-0.8903\t = Validation score   (-root_mean_squared_error)\n",
      "\t158.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -1.18s of remaining time.\n",
      "\t-0.8837\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.02s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels301\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2820769750190104\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2820769750190104,\n",
      "    \"mean_squared_error\": -0.0795674198358754,\n",
      "    \"mean_absolute_error\": -0.09049509975729683,\n",
      "    \"r2\": 0.9950309903981498,\n",
      "    \"pearsonr\": 0.9975160773512044,\n",
      "    \"median_absolute_error\": -0.024513918846874816\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels302\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.28 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 927\n",
      "Label Column: 927\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (26.2237963857, -19.2698006363, -1.22072, 3.72815)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62002.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.42 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 612 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 612 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t612 features in original data used to generate 612 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.11 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.701085\n",
      "[2000]\tvalid_set's rmse: 0.696192\n",
      "[3000]\tvalid_set's rmse: 0.695365\n",
      "[4000]\tvalid_set's rmse: 0.695139\n",
      "[5000]\tvalid_set's rmse: 0.695046\n",
      "[6000]\tvalid_set's rmse: 0.695043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.695\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 155.63s of the 155.62s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.630505\n",
      "[2000]\tvalid_set's rmse: 0.628823\n",
      "[3000]\tvalid_set's rmse: 0.628455\n",
      "[4000]\tvalid_set's rmse: 0.628298\n",
      "[5000]\tvalid_set's rmse: 0.62827\n",
      "[6000]\tvalid_set's rmse: 0.628279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6283\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.4s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 120.24s of the 120.23s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5199.\n",
      "\t-0.558\t = Validation score   (-root_mean_squared_error)\n",
      "\t120.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.86s of remaining time.\n",
      "\t-0.5575\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.79s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels302\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17893483288895268\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17893483288895268,\n",
      "    \"mean_squared_error\": -0.03201767442099761,\n",
      "    \"mean_absolute_error\": -0.06325077539241292,\n",
      "    \"r2\": 0.9976961970255344,\n",
      "    \"pearsonr\": 0.9988485873958068,\n",
      "    \"median_absolute_error\": -0.020216610993505857\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels303\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.20 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 928\n",
      "Label Column: 928\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.4204135035, -15.3414188825, 0.15992, 3.01175)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61993.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 613 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 613 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t613 features in original data used to generate 613 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.2 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.630128\n",
      "[2000]\tvalid_set's rmse: 0.625335\n",
      "[3000]\tvalid_set's rmse: 0.624945\n",
      "[4000]\tvalid_set's rmse: 0.624617\n",
      "[5000]\tvalid_set's rmse: 0.624652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6246\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.26s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.45s of the 160.45s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.594452\n",
      "[2000]\tvalid_set's rmse: 0.593749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5937\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 147.0s of the 147.0s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6266.\n",
      "\t-0.545\t = Validation score   (-root_mean_squared_error)\n",
      "\t147.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.84s of remaining time.\n",
      "\t-0.544\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.71s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels303\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17304403773979576\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17304403773979576,\n",
      "    \"mean_squared_error\": -0.029944238997291837,\n",
      "    \"mean_absolute_error\": -0.055037549933128406,\n",
      "    \"r2\": 0.9966984669257204,\n",
      "    \"pearsonr\": 0.9983500271397803,\n",
      "    \"median_absolute_error\": -0.01212283257144317\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels304\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.12 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 929\n",
      "Label Column: 929\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.1702452195, -9.0468206239, 2.28013, 2.67003)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61994.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.59 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 614 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 614 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t614 features in original data used to generate 614 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.28 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.701642\n",
      "[2000]\tvalid_set's rmse: 0.696907\n",
      "[3000]\tvalid_set's rmse: 0.695975\n",
      "[4000]\tvalid_set's rmse: 0.695902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6959\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.41s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.08s of the 161.08s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.67178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6713\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 152.38s of the 152.38s of remaining time.\n",
      "\t-0.6329\t = Validation score   (-root_mean_squared_error)\n",
      "\t86.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 65.81s of the 65.8s of remaining time.\n",
      "\t-0.7084\t = Validation score   (-root_mean_squared_error)\n",
      "\t47.87s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 17.46s of the 17.46s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 574. Best iteration is:\n",
      "\t[574]\tvalid_set's rmse: 0.724802\n",
      "\t-0.7248\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.44s of remaining time.\n",
      "\t-0.6319\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.3s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels304\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22925890028176424\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22925890028176424,\n",
      "    \"mean_squared_error\": -0.0525596433584039,\n",
      "    \"mean_absolute_error\": -0.13076344409928892,\n",
      "    \"r2\": 0.9926267092367271,\n",
      "    \"pearsonr\": 0.9963594620328105,\n",
      "    \"median_absolute_error\": -0.08093181013804918\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels305\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   68.04 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 930\n",
      "Label Column: 930\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.8687412988, -8.0647565873, 3.50596, 2.6379)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61972.2 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.67 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 615 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 615 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t615 features in original data used to generate 615 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.36 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.460067\n",
      "[2000]\tvalid_set's rmse: 0.457964\n",
      "[3000]\tvalid_set's rmse: 0.457495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4575\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.48s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.34s of the 164.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.447143\n",
      "[2000]\tvalid_set's rmse: 0.446101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.446\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 148.1s of the 148.1s of remaining time.\n",
      "\t-0.4042\t = Validation score   (-root_mean_squared_error)\n",
      "\t118.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 29.05s of the 29.04s of remaining time.\n",
      "\t-0.4705\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.24s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.72s of remaining time.\n",
      "\t-0.4035\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.59s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels305\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13426444269884735\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13426444269884735,\n",
      "    \"mean_squared_error\": -0.018026940573232147,\n",
      "    \"mean_absolute_error\": -0.061371725814962544,\n",
      "    \"r2\": 0.9974091197317827,\n",
      "    \"pearsonr\": 0.9987082860045207,\n",
      "    \"median_absolute_error\": -0.029091000521251777\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels306\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.96 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 931\n",
      "Label Column: 931\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.1082443894, -7.9273367183, 3.58261, 2.65272)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61948.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.76 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 616 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 616 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t616 features in original data used to generate 616 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.45 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.472133\n",
      "[2000]\tvalid_set's rmse: 0.468805\n",
      "[3000]\tvalid_set's rmse: 0.468158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4681\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.21s of the 164.21s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.451803\n",
      "[2000]\tvalid_set's rmse: 0.450784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4507\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.9s of the 150.9s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6387.\n",
      "\t-0.4149\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.7s of remaining time.\n",
      "\t-0.4137\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.58s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels306\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13229052530607838\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13229052530607838,\n",
      "    \"mean_squared_error\": -0.017500783085758132,\n",
      "    \"mean_absolute_error\": -0.046658533723475976,\n",
      "    \"r2\": 0.9975127617199397,\n",
      "    \"pearsonr\": 0.998757209053037,\n",
      "    \"median_absolute_error\": -0.013151663780320025\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels307\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.89 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 932\n",
      "Label Column: 932\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.1347151698, -10.6279851914, 2.47948, 2.85877)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61966.64 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.84 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 617 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 617 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t617 features in original data used to generate 617 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.53 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.465705\n",
      "[2000]\tvalid_set's rmse: 0.462202\n",
      "[3000]\tvalid_set's rmse: 0.461832\n",
      "[4000]\tvalid_set's rmse: 0.461736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4617\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.68s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.82s of the 161.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.435515\n",
      "[2000]\tvalid_set's rmse: 0.434733\n",
      "[3000]\tvalid_set's rmse: 0.434644\n",
      "[4000]\tvalid_set's rmse: 0.434604\n",
      "[5000]\tvalid_set's rmse: 0.434602\n",
      "[6000]\tvalid_set's rmse: 0.434601\n",
      "[7000]\tvalid_set's rmse: 0.434601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4346\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.94s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 118.83s of the 118.83s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5092.\n",
      "\t-0.4087\t = Validation score   (-root_mean_squared_error)\n",
      "\t118.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.69s of remaining time.\n",
      "\t-0.4049\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.54s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels307\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.12957053558339557\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.12957053558339557,\n",
      "    \"mean_squared_error\": -0.016788523691368063,\n",
      "    \"mean_absolute_error\": -0.045649964831813154,\n",
      "    \"r2\": 0.9979455433722435,\n",
      "    \"pearsonr\": 0.9989747346977015,\n",
      "    \"median_absolute_error\": -0.013333694005651653\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels391\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.80 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 1016\n",
      "Label Column: 1016\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.6925914924, -11.5297227278, -0.22585, 2.22198)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61958.57 MB\n",
      "\tTrain Data (Original)  Memory Usage: 84.86 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 398): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '933', '934', '935', '936', '937', '938', '939', '940', '941', '942', '943', '944', '945', '946', '947', '948', '949', '950', '951', '952', '953', '954', '955', '956', '957', '958', '959', '960', '961', '962', '963', '964', '965', '966', '967', '968', '969', '970', '971', '972', '973', '974', '975', '976', '977', '978', '979', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '1010', '1011', '1012', '1013', '1014', '1015']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 618 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 618 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t618 features in original data used to generate 618 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.62 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.26s of remaining time.\n",
      "\t-1.561\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.13s of the 176.13s of remaining time.\n",
      "\t-1.554\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 173.02s of the 173.02s of remaining time.\n",
      "\t-1.5628\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 153.84s of the 153.83s of remaining time.\n",
      "\t-1.584\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 141.47s of the 141.47s of remaining time.\n",
      "\t-1.5676\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the 119.0s of remaining time.\n",
      "\t-1.5429\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 61.88s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels391\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.785903916929078\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.785903916929078,\n",
      "    \"mean_squared_error\": -0.6176449666444663,\n",
      "    \"mean_absolute_error\": -0.5618181094318451,\n",
      "    \"r2\": 0.8748881872841257,\n",
      "    \"pearsonr\": 0.9417762678368359,\n",
      "    \"median_absolute_error\": -0.4231713066983125\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoGluon will save models to \"./a/x/result-high-1/1-1-6/tgModels541\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.74 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 1166\n",
      "Label Column: 1166\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.8310714137, -10.457938211, -1.05402, 1.99279)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61970.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 97.38 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 547): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '933', '934', '935', '936', '937', '938', '939', '940', '941', '942', '943', '944', '945', '946', '947', '948', '949', '950', '951', '952', '953', '954', '955', '956', '957', '958', '959', '960', '961', '962', '963', '964', '965', '966', '967', '968', '969', '970', '971', '972', '973', '974', '975', '976', '977', '978', '979', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '1010', '1011', '1012', '1013', '1014', '1015', '1017', '1018', '1019', '1020', '1021', '1022', '1023', '1024', '1025', '1026', '1027', '1028', '1029', '1030', '1031', '1032', '1033', '1034', '1035', '1036', '1037', '1038', '1039', '1040', '1041', '1042', '1043', '1044', '1045', '1046', '1047', '1048', '1049', '1050', '1051', '1052', '1053', '1054', '1055', '1056', '1057', '1058', '1059', '1060', '1061', '1062', '1063', '1064', '1065', '1066', '1067', '1068', '1069', '1070', '1071', '1072', '1073', '1074', '1075', '1076', '1077', '1078', '1079', '1080', '1081', '1082', '1083', '1084', '1085', '1086', '1087', '1088', '1089', '1090', '1091', '1092', '1093', '1094', '1095', '1096', '1097', '1098', '1099', '1100', '1101', '1102', '1103', '1104', '1105', '1106', '1107', '1108', '1109', '1110', '1111', '1112', '1113', '1114', '1115', '1116', '1117', '1118', '1119', '1120', '1121', '1122', '1123', '1124', '1125', '1126', '1127', '1128', '1129', '1130', '1131', '1132', '1133', '1134', '1135', '1136', '1137', '1138', '1139', '1140', '1141', '1142', '1143', '1144', '1145', '1146', '1147', '1148', '1149', '1150', '1151', '1152', '1153', '1154', '1155', '1156', '1157', '1158', '1159', '1160', '1161', '1162', '1163', '1164', '1165']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 619 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 619 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t619 features in original data used to generate 619 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.7 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.2s of the 179.2s of remaining time.\n",
      "\t-1.338\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 177.02s of the 177.01s of remaining time.\n",
      "\t-1.3332\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 174.26s of the 174.26s of remaining time.\n",
      "\t-1.336\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 159.17s of the 159.16s of remaining time.\n",
      "\t-1.3357\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.74s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 147.38s of the 147.37s of remaining time.\n",
      "\t-1.3444\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.2s of the 131.53s of remaining time.\n",
      "\t-1.3231\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 49.34s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/x/result-high-1/1-1-6/tgModels541\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.9109812214128129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.9109812214128129,\n",
      "    \"mean_squared_error\": -0.8298867857667757,\n",
      "    \"mean_absolute_error\": -0.7071419733569106,\n",
      "    \"r2\": 0.7910038221236065,\n",
      "    \"pearsonr\": 0.8994633268240169,\n",
      "    \"median_absolute_error\": -0.5763991461597169\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels0\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.68 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 625\n",
      "Label Column: 625\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.7256061726, -15.8936020511, -0.99883, 3.0962)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62024.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.2 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 310 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 310 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t310 features in original data used to generate 310 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 25.89 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.59s of the 179.59s of remaining time.\n",
      "\t-2.8274\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 178.72s of the 178.72s of remaining time.\n",
      "\t-2.8154\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 177.69s of the 177.69s of remaining time.\n",
      "\t-2.8272\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 172.06s of the 172.06s of remaining time.\n",
      "\t-2.8071\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 166.26s of the 166.26s of remaining time.\n",
      "\t-2.824\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.59s of the 160.3s of remaining time.\n",
      "\t-2.7946\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 20.23s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels0\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -1.8941508081837042\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -1.8941508081837042,\n",
      "    \"mean_squared_error\": -3.587807284142995,\n",
      "    \"mean_absolute_error\": -1.4616247331789518,\n",
      "    \"r2\": 0.6257063856855948,\n",
      "    \"pearsonr\": 0.8411750589069076,\n",
      "    \"median_absolute_error\": -1.1804917954643799\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels1\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.65 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 626\n",
      "Label Column: 626\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.1352285492, -13.9693144533, 0.15398, 2.87731)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61975.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.28 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 311 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 311 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t311 features in original data used to generate 311 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 25.97 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.59s of the 179.59s of remaining time.\n",
      "\t-1.3414\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 178.37s of the 178.37s of remaining time.\n",
      "\t-1.3126\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.95s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 177.4s of the 177.4s of remaining time.\n",
      "\t-1.3095\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 170.2s of the 170.2s of remaining time.\n",
      "\t-1.3031\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 164.82s of the 164.82s of remaining time.\n",
      "\t-1.3377\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.59s of the 157.83s of remaining time.\n",
      "\t-1.2956\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 22.69s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels1\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.9601327442391595\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.9601327442391595,\n",
      "    \"mean_squared_error\": -0.9218548865602211,\n",
      "    \"mean_absolute_error\": -0.7458327057631123,\n",
      "    \"r2\": 0.8886395690063748,\n",
      "    \"pearsonr\": 0.9439757218692093,\n",
      "    \"median_absolute_error\": -0.6062738863106385\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels2\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.62 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 627\n",
      "Label Column: 627\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.8075130306, -16.2278528549, -1.33905, 3.12343)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61986.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.37 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 312 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 312 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t312 features in original data used to generate 312 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n",
      "\t-1.692\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 177.76s of the 177.76s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.69748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.6907\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 176.71s of the 176.71s of remaining time.\n",
      "\t-1.686\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 169.4s of the 169.4s of remaining time.\n",
      "\t-1.7118\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 164.51s of the 164.51s of remaining time.\n",
      "\t-1.7233\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the 156.44s of remaining time.\n",
      "\t-1.6744\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 24.12s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels2\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -1.1765143846568944\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -1.1765143846568944,\n",
      "    \"mean_squared_error\": -1.3841860973045836,\n",
      "    \"mean_absolute_error\": -0.9095540551849017,\n",
      "    \"r2\": 0.8581028765949991,\n",
      "    \"pearsonr\": 0.9301115207925583,\n",
      "    \"median_absolute_error\": -0.745780048956177\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels3\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.58 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 628\n",
      "Label Column: 628\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.217901209, -16.1567784226, -0.96348, 3.18081)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61977.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.45 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 313 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 313 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t313 features in original data used to generate 313 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.14 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n",
      "\t-1.0157\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 178.19s of the 178.19s of remaining time.\n",
      "\t-0.9859\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 177.08s of the 177.08s of remaining time.\n",
      "\t-0.9953\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.82s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 167.24s of the 167.24s of remaining time.\n",
      "\t-1.003\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 162.3s of the 162.3s of remaining time.\n",
      "\t-1.0068\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the 152.62s of remaining time.\n",
      "\t-0.9822\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 27.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels3\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.7378361205410584\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.7378361205410584,\n",
      "    \"mean_squared_error\": -0.5444021407750795,\n",
      "    \"mean_absolute_error\": -0.5681615237326396,\n",
      "    \"r2\": 0.9461872274500479,\n",
      "    \"pearsonr\": 0.9730238117349118,\n",
      "    \"median_absolute_error\": -0.4533804648825197\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels4\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.55 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 629\n",
      "Label Column: 629\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.1456249724, -15.2965734375, 0.15291, 3.0657)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61997.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.53 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 314 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 314 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t314 features in original data used to generate 314 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.23 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n",
      "\t-0.7484\t = Validation score   (-root_mean_squared_error)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.749224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 177.69s of the 177.69s of remaining time.\n",
      "\t-0.6836\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 176.43s of the 176.43s of remaining time.\n",
      "\t-0.6547\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.88s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 164.53s of the 164.53s of remaining time.\n",
      "\t-0.7007\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 158.47s of the 158.47s of remaining time.\n",
      "\t-0.6876\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.98s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the 146.29s of remaining time.\n",
      "\t-0.6532\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 34.23s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels4\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.40493114002545716\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.40493114002545716,\n",
      "    \"mean_squared_error\": -0.16396922816231674,\n",
      "    \"mean_absolute_error\": -0.3089979059599531,\n",
      "    \"r2\": 0.982552084883737,\n",
      "    \"pearsonr\": 0.991367465390002,\n",
      "    \"median_absolute_error\": -0.2453892119741271\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels5\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.51 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 630\n",
      "Label Column: 630\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.3624606082, -11.8377747384, 1.49968, 2.84195)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62010.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.62 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 315 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 315 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t315 features in original data used to generate 315 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.31 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.922472\n",
      "[2000]\tvalid_set's rmse: 0.916952\n",
      "[3000]\tvalid_set's rmse: 0.916319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9162\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 174.47s of the 174.47s of remaining time.\n",
      "\t-0.8101\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 172.84s of the 172.84s of remaining time.\n",
      "\t-0.7831\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 130.55s of the 130.54s of remaining time.\n",
      "\t-0.8379\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 123.62s of the 123.62s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.836417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8363\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the 107.11s of remaining time.\n",
      "\t-0.7816\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 73.44s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels5\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.31553961940851605\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.31553961940851605,\n",
      "    \"mean_squared_error\": -0.0995652514164712,\n",
      "    \"mean_absolute_error\": -0.20423445148459585,\n",
      "    \"r2\": 0.9876713099618629,\n",
      "    \"pearsonr\": 0.9939577971331863,\n",
      "    \"median_absolute_error\": -0.14108476894394612\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels6\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.46 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 631\n",
      "Label Column: 631\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.1768288955, -17.0715670451, -1.6647, 3.15709)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62007.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.7 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 316 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 316 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t316 features in original data used to generate 316 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.57s of the 179.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.805939\n",
      "[2000]\tvalid_set's rmse: 0.802229\n",
      "[3000]\tvalid_set's rmse: 0.801936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8016\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 174.2s of the 174.2s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.743664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7428\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 171.79s of the 171.78s of remaining time.\n",
      "\t-0.7156\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.99s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 129.35s of the 129.35s of remaining time.\n",
      "\t-0.7901\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 117.38s of the 117.37s of remaining time.\n",
      "\t-0.7851\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.57s of the 103.55s of remaining time.\n",
      "\t-0.7135\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 76.97s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels6\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2624951014116098\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2624951014116098,\n",
      "    \"mean_squared_error\": -0.06890367826509095,\n",
      "    \"mean_absolute_error\": -0.14840878523937995,\n",
      "    \"r2\": 0.9930862882280947,\n",
      "    \"pearsonr\": 0.9965813027530741,\n",
      "    \"median_absolute_error\": -0.0947747011369508\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels7\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.40 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 632\n",
      "Label Column: 632\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.1262186378, -12.7740071915, 1.23112, 2.67989)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62004.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.78 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 317 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 317 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t317 features in original data used to generate 317 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.48 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.672041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6697\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.41s of the 176.41s of remaining time.\n",
      "\t-0.6226\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 174.49s of the 174.49s of remaining time.\n",
      "\t-0.5876\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 146.03s of the 146.02s of remaining time.\n",
      "\t-0.6379\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 134.93s of the 134.93s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.640888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6408\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.59s of the 107.74s of remaining time.\n",
      "\t-0.5869\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 72.81s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels7\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.267476655904424\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.267476655904424,\n",
      "    \"mean_squared_error\": -0.07154376145381322,\n",
      "    \"mean_absolute_error\": -0.18782907283388456,\n",
      "    \"r2\": 0.9900372771444143,\n",
      "    \"pearsonr\": 0.9950894552516591,\n",
      "    \"median_absolute_error\": -0.13738871465556035\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels8\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.35 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 633\n",
      "Label Column: 633\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.6516966399, -17.5736135593, -1.74872, 3.29981)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62009.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.87 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 318 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 318 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t318 features in original data used to generate 318 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.56 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.27997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.2789\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.42s of the 176.41s of remaining time.\n",
      "\t-1.2155\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 175.32s of the 175.32s of remaining time.\n",
      "\t-1.1889\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 161.78s of the 161.77s of remaining time.\n",
      "\t-1.2545\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 156.72s of the 156.72s of remaining time.\n",
      "\t-1.2337\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the 145.91s of remaining time.\n",
      "\t-1.1881\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 34.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels8\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.7802646175742204\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.7802646175742204,\n",
      "    \"mean_squared_error\": -0.6088128734382419,\n",
      "    \"mean_absolute_error\": -0.5929992450475146,\n",
      "    \"r2\": 0.944082580073586,\n",
      "    \"pearsonr\": 0.9723700466452546,\n",
      "    \"median_absolute_error\": -0.47159006560962985\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels9\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.31 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 634\n",
      "Label Column: 634\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.1497964252, -14.5814340646, -0.73731, 3.13449)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61997.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 52.95 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 319 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 319 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t319 features in original data used to generate 319 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.64 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.836663\n",
      "[2000]\tvalid_set's rmse: 0.833347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8329\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 175.42s of the 175.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.793433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7928\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 173.14s of the 173.14s of remaining time.\n",
      "\t-0.8018\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 148.66s of the 148.66s of remaining time.\n",
      "\t-0.8288\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 136.97s of the 136.96s of remaining time.\n",
      "\t-0.8242\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.66s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the 126.1s of remaining time.\n",
      "\t-0.7857\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 54.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels9\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.3246611760141242\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.3246611760141242,\n",
      "    \"mean_squared_error\": -0.10540487921087457,\n",
      "    \"mean_absolute_error\": -0.21184582785744263,\n",
      "    \"r2\": 0.9892707888750467,\n",
      "    \"pearsonr\": 0.9947173280064253,\n",
      "    \"median_absolute_error\": -0.15141175080771485\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels10\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.26 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 635\n",
      "Label Column: 635\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.2194084971, -13.0621347372, 0.2721, 2.86132)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61992.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.04 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 320 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 320 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t320 features in original data used to generate 320 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.73 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.59s of the 179.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.620699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6193\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.46s of the 176.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.576549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5759\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 173.29s of the 173.29s of remaining time.\n",
      "\t-0.5519\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.91s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 128.96s of the 128.95s of remaining time.\n",
      "\t-0.6151\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 115.95s of the 115.95s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.615084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6148\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.59s of the 101.03s of remaining time.\n",
      "\t-0.5494\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 79.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels10\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19848734589223826\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19848734589223826,\n",
      "    \"mean_squared_error\": -0.039397226479345064,\n",
      "    \"mean_absolute_error\": -0.11043755044439123,\n",
      "    \"r2\": 0.9951874621314675,\n",
      "    \"pearsonr\": 0.9976138228666208,\n",
      "    \"median_absolute_error\": -0.0669071877010009\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels11\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.21 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 636\n",
      "Label Column: 636\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.1870126723, -11.9976330308, 0.88436, 2.69225)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61993.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 321 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 321 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t321 features in original data used to generate 321 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.81 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.59s of the 179.59s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.654213\n",
      "[2000]\tvalid_set's rmse: 0.649512\n",
      "[3000]\tvalid_set's rmse: 0.64919\n",
      "[4000]\tvalid_set's rmse: 0.64889\n",
      "[5000]\tvalid_set's rmse: 0.64886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6488\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.56s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 170.44s of the 170.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.62242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.622\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 166.66s of the 166.66s of remaining time.\n",
      "\t-0.5947\t = Validation score   (-root_mean_squared_error)\n",
      "\t57.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 108.99s of the 108.99s of remaining time.\n",
      "\t-0.6462\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 97.07s of the 97.07s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.646912\n",
      "[2000]\tvalid_set's rmse: 0.64685\n",
      "[3000]\tvalid_set's rmse: 0.646848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6468\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.22s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.59s of the 40.69s of remaining time.\n",
      "\t-0.5926\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 139.86s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels11\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.20022254205156959\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.20022254205156959,\n",
      "    \"mean_squared_error\": -0.04008906634559245,\n",
      "    \"mean_absolute_error\": -0.09449371543963983,\n",
      "    \"r2\": 0.9944685819557204,\n",
      "    \"pearsonr\": 0.9972571305152674,\n",
      "    \"median_absolute_error\": -0.047561285028991485\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels12\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.12 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 637\n",
      "Label Column: 637\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.4180191508, -14.8250507156, -0.86629, 3.01342)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62037.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.2 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 322 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 322 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t322 features in original data used to generate 322 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.89 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.41s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.59s of the 179.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.43599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.4345\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.83s of the 176.83s of remaining time.\n",
      "\t-1.4206\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 174.79s of the 174.79s of remaining time.\n",
      "\t-1.3846\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 148.61s of the 148.6s of remaining time.\n",
      "\t-1.4518\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 137.03s of the 137.02s of remaining time.\n",
      "\t-1.4392\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.59s of the 124.0s of remaining time.\n",
      "\t-1.3824\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 56.5s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels12\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.6247380738711223\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.6247380738711223,\n",
      "    \"mean_squared_error\": -0.3902976609442006,\n",
      "    \"mean_absolute_error\": -0.43731306683408233,\n",
      "    \"r2\": 0.9570148522655983,\n",
      "    \"pearsonr\": 0.9792824144707762,\n",
      "    \"median_absolute_error\": -0.32474410881904603\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels13\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.07 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 638\n",
      "Label Column: 638\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.4610758989, -15.9576207324, -0.78476, 3.06585)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61972.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.29 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 323 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 323 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t323 features in original data used to generate 323 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 26.98 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.689759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6887\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 177.41s of the 177.41s of remaining time.\n",
      "\t-0.6831\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.68s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 175.66s of the 175.66s of remaining time.\n",
      "\t-0.6444\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 148.76s of the 148.76s of remaining time.\n",
      "\t-0.72\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 142.21s of the 142.21s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.716774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7167\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the 118.44s of remaining time.\n",
      "\t-0.6438\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 62.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels13\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.311855572788168\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.311855572788168,\n",
      "    \"mean_squared_error\": -0.09725389827903658,\n",
      "    \"mean_absolute_error\": -0.2241961210973395,\n",
      "    \"r2\": 0.9896522428894542,\n",
      "    \"pearsonr\": 0.9948775415832543,\n",
      "    \"median_absolute_error\": -0.16767608543082885\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels14\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   67.02 GB / 2000.36 GB (3.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 639\n",
      "Label Column: 639\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.1521298232, -14.9943525063, -0.47459, 3.14749)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61990.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.37 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 324 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 324 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t324 features in original data used to generate 324 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.57s of the 179.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.551183\n",
      "[2000]\tvalid_set's rmse: 0.551293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5502\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 175.89s of the 175.89s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.522131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5214\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 172.27s of the 172.27s of remaining time.\n",
      "\t-0.4929\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.59s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 146.33s of the 146.33s of remaining time.\n",
      "\t-0.5582\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 137.65s of the 137.65s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.56996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5699\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.57s of the 113.54s of remaining time.\n",
      "\t-0.4897\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 66.99s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels14\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2114597116929981\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2114597116929981,\n",
      "    \"mean_squared_error\": -0.044715209669285744,\n",
      "    \"mean_absolute_error\": -0.1426085527413372,\n",
      "    \"r2\": 0.9954859266832949,\n",
      "    \"pearsonr\": 0.9977632336187069,\n",
      "    \"median_absolute_error\": -0.10381379715755634\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels15\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.97 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 640\n",
      "Label Column: 640\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.7667579581, -16.4055799998, 0.5222, 3.08739)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61946.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.45 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 325 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 325 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t325 features in original data used to generate 325 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.14 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.57s of the 179.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.54332\n",
      "[2000]\tvalid_set's rmse: 0.537227\n",
      "[3000]\tvalid_set's rmse: 0.536597\n",
      "[4000]\tvalid_set's rmse: 0.536551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5365\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.98s of the 171.98s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.514886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5147\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 169.63s of the 169.63s of remaining time.\n",
      "\t-0.4728\t = Validation score   (-root_mean_squared_error)\n",
      "\t72.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 96.59s of the 96.59s of remaining time.\n",
      "\t-0.5594\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 85.96s of the 85.96s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.561893\n",
      "[2000]\tvalid_set's rmse: 0.561812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5618\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.57s of the 48.49s of remaining time.\n",
      "\t-0.4718\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 132.1s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels15\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1568517359795038\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1568517359795038,\n",
      "    \"mean_squared_error\": -0.024602467079783828,\n",
      "    \"mean_absolute_error\": -0.07016336480033658,\n",
      "    \"r2\": 0.9974187127239739,\n",
      "    \"pearsonr\": 0.9987107330928691,\n",
      "    \"median_absolute_error\": -0.03419190014409179\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels16\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.89 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 641\n",
      "Label Column: 641\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.9612537634, -14.8296063973, 2.06576, 2.85081)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61968.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.54 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 326 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 326 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t326 features in original data used to generate 326 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.23 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.57s of the 179.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.624509\n",
      "[2000]\tvalid_set's rmse: 0.62035\n",
      "[3000]\tvalid_set's rmse: 0.619479\n",
      "[4000]\tvalid_set's rmse: 0.619317\n",
      "[5000]\tvalid_set's rmse: 0.619335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6193\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 170.81s of the 170.81s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.578637\n",
      "[2000]\tvalid_set's rmse: 0.577484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5774\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 165.87s of the 165.87s of remaining time.\n",
      "\t-0.5412\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 125.59s of the 125.58s of remaining time.\n",
      "\t-0.5937\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 108.6s of the 108.59s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.605386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6053\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.57s of the 81.78s of remaining time.\n",
      "\t-0.5399\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 98.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels16\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19965891324994886\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19965891324994886,\n",
      "    \"mean_squared_error\": -0.039863681640150764,\n",
      "    \"mean_absolute_error\": -0.11645175309818345,\n",
      "    \"r2\": 0.9950945121020897,\n",
      "    \"pearsonr\": 0.9975610440000006,\n",
      "    \"median_absolute_error\": -0.07308541642077637\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels17\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.82 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 642\n",
      "Label Column: 642\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.9811828991, -10.6250008092, 3.27017, 2.76661)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61969.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.62 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 327 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 327 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t327 features in original data used to generate 327 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.31 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.12191\n",
      "[2000]\tvalid_set's rmse: 1.11684\n",
      "[3000]\tvalid_set's rmse: 1.11525\n",
      "[4000]\tvalid_set's rmse: 1.11482\n",
      "[5000]\tvalid_set's rmse: 1.1147\n",
      "[6000]\tvalid_set's rmse: 1.11471\n",
      "[7000]\tvalid_set's rmse: 1.11469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1147\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.56s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.31s of the 168.31s of remaining time.\n",
      "\t-1.0305\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.68s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 166.56s of the 166.56s of remaining time.\n",
      "\t-1.014\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.99s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 139.14s of the 139.14s of remaining time.\n",
      "\t-1.0498\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 128.18s of the 128.18s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.04689\n",
      "[2000]\tvalid_set's rmse: 1.04675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0467\t = Validation score   (-root_mean_squared_error)\n",
      "\t38.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the 88.27s of remaining time.\n",
      "\t-1.0103\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 92.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels17\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.46158059475721713\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.46158059475721713,\n",
      "    \"mean_squared_error\": -0.21305664545642575,\n",
      "    \"mean_absolute_error\": -0.3197874561688168,\n",
      "    \"r2\": 0.9721617998807887,\n",
      "    \"pearsonr\": 0.9865421215498165,\n",
      "    \"median_absolute_error\": -0.23621521650717803\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels18\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.74 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 643\n",
      "Label Column: 643\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.7886501893, -14.8869968758, -1.27414, 3.03504)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61967.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.7 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 328 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 328 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t328 features in original data used to generate 328 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.39 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.64299\n",
      "[2000]\tvalid_set's rmse: 0.638465\n",
      "[3000]\tvalid_set's rmse: 0.638148\n",
      "[4000]\tvalid_set's rmse: 0.637823\n",
      "[5000]\tvalid_set's rmse: 0.637722\n",
      "[6000]\tvalid_set's rmse: 0.637683\n",
      "[7000]\tvalid_set's rmse: 0.637691\n",
      "[8000]\tvalid_set's rmse: 0.637693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6377\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.89s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.86s of the 166.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.604678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.604\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.34s of the 163.34s of remaining time.\n",
      "\t-0.5743\t = Validation score   (-root_mean_squared_error)\n",
      "\t130.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 32.17s of the 32.17s of remaining time.\n",
      "\t-0.6398\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 20.43s of the 20.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.658074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1391. Best iteration is:\n",
      "\t[1331]\tvalid_set's rmse: 0.65798\n",
      "\t-0.658\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the -0.95s of remaining time.\n",
      "\t-0.5721\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.49s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels18\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18160749647689173\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18160749647689173,\n",
      "    \"mean_squared_error\": -0.03298128277660397,\n",
      "    \"mean_absolute_error\": -0.05412126209843234,\n",
      "    \"r2\": 0.9964191885337277,\n",
      "    \"pearsonr\": 0.9982120007271778,\n",
      "    \"median_absolute_error\": -0.01066006770488881\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels19\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.66 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 644\n",
      "Label Column: 644\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.2648455056, -11.6773946923, 2.47229, 2.8275)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62029.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.79 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 329 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 329 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t329 features in original data used to generate 329 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.48 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.734425\n",
      "[2000]\tvalid_set's rmse: 0.731178\n",
      "[3000]\tvalid_set's rmse: 0.731274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7308\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 174.07s of the 174.07s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.712782\n",
      "[2000]\tvalid_set's rmse: 0.711793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7118\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.75s of the 168.75s of remaining time.\n",
      "\t-0.6889\t = Validation score   (-root_mean_squared_error)\n",
      "\t38.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 130.13s of the 130.13s of remaining time.\n",
      "\t-0.7414\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 117.86s of the 117.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.74163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7416\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the 91.25s of remaining time.\n",
      "\t-0.6866\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 89.3s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels19\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24782610655475676\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24782610655475676,\n",
      "    \"mean_squared_error\": -0.061417779090089566,\n",
      "    \"mean_absolute_error\": -0.1383220861439785,\n",
      "    \"r2\": 0.9923169941166371,\n",
      "    \"pearsonr\": 0.9961979675894537,\n",
      "    \"median_absolute_error\": -0.08435002292532667\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels20\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.59 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 645\n",
      "Label Column: 645\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.4402419084, -16.0767677523, -1.80024, 3.12202)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62040.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.87 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 330 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 330 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t330 features in original data used to generate 330 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.56 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.58s of the 179.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.646245\n",
      "[2000]\tvalid_set's rmse: 0.641889\n",
      "[3000]\tvalid_set's rmse: 0.641241\n",
      "[4000]\tvalid_set's rmse: 0.641047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.641\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.57s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.46s of the 171.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.613267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6128\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.31s of the 168.31s of remaining time.\n",
      "\t-0.5701\t = Validation score   (-root_mean_squared_error)\n",
      "\t75.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 92.65s of the 92.65s of remaining time.\n",
      "\t-0.6384\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 80.89s of the 80.89s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.641749\n",
      "[2000]\tvalid_set's rmse: 0.641687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6417\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.58s of the 42.41s of remaining time.\n",
      "\t-0.5697\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 138.11s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels20\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18549839544215496\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18549839544215496,\n",
      "    \"mean_squared_error\": -0.0344096547116139,\n",
      "    \"mean_absolute_error\": -0.07233899878579413,\n",
      "    \"r2\": 0.9964693775353638,\n",
      "    \"pearsonr\": 0.9982383084540004,\n",
      "    \"median_absolute_error\": -0.029806995249310297\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels21\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.52 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 646\n",
      "Label Column: 646\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.171083407, -12.7491429763, 1.99185, 2.79873)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61991.64 MB\n",
      "\tTrain Data (Original)  Memory Usage: 53.95 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 331 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 331 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t331 features in original data used to generate 331 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.65 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.57s of the 179.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.602096\n",
      "[2000]\tvalid_set's rmse: 0.598797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5986\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 175.2s of the 175.2s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.53815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5379\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 172.5s of the 172.5s of remaining time.\n",
      "\t-0.5165\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.41s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 135.96s of the 135.95s of remaining time.\n",
      "\t-0.5626\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 127.2s of the 127.2s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.569792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5697\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.57s of the 110.23s of remaining time.\n",
      "\t-0.5113\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 70.36s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels21\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2026106338502642\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2026106338502642,\n",
      "    \"mean_squared_error\": -0.04105106894920572,\n",
      "    \"mean_absolute_error\": -0.12975438094099592,\n",
      "    \"r2\": 0.9947586356996116,\n",
      "    \"pearsonr\": 0.9974006600603297,\n",
      "    \"median_absolute_error\": -0.08824343539368884\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels22\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.47 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 647\n",
      "Label Column: 647\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.3686056258, -18.1540354327, -2.22005, 3.35357)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61983.41 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.04 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 332 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 332 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t332 features in original data used to generate 332 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.73 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.776259\n",
      "[2000]\tvalid_set's rmse: 0.771019\n",
      "[3000]\tvalid_set's rmse: 0.769217\n",
      "[4000]\tvalid_set's rmse: 0.76916\n",
      "[5000]\tvalid_set's rmse: 0.769132\n",
      "[6000]\tvalid_set's rmse: 0.769078\n",
      "[7000]\tvalid_set's rmse: 0.769064\n",
      "[8000]\tvalid_set's rmse: 0.769057\n",
      "[9000]\tvalid_set's rmse: 0.769056\n",
      "[10000]\tvalid_set's rmse: 0.769055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7691\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.92s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.42s of the 163.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.725592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7238\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.41s of the 159.41s of remaining time.\n",
      "\t-0.7079\t = Validation score   (-root_mean_squared_error)\n",
      "\t103.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 55.35s of the 55.35s of remaining time.\n",
      "\t-0.7502\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 35.85s of the 35.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.751528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7514\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the 11.8s of remaining time.\n",
      "\t-0.7012\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 168.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels22\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22403654993459643\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22403654993459643,\n",
      "    \"mean_squared_error\": -0.05019237570659662,\n",
      "    \"mean_absolute_error\": -0.07072216944564182,\n",
      "    \"r2\": 0.995536600320558,\n",
      "    \"pearsonr\": 0.9977729316964242,\n",
      "    \"median_absolute_error\": -0.02120038659121093\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels23\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.38 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 648\n",
      "Label Column: 648\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.059997077, -12.3980414642, 1.38754, 2.86024)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61975.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 333 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 333 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t333 features in original data used to generate 333 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.81 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.673738\n",
      "[2000]\tvalid_set's rmse: 0.670648\n",
      "[3000]\tvalid_set's rmse: 0.669725\n",
      "[4000]\tvalid_set's rmse: 0.669526\n",
      "[5000]\tvalid_set's rmse: 0.669468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6694\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.0s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 170.0s of the 169.99s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.646632\n",
      "[2000]\tvalid_set's rmse: 0.644583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6445\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.89s of the 164.89s of remaining time.\n",
      "\t-0.6079\t = Validation score   (-root_mean_squared_error)\n",
      "\t133.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 30.81s of the 30.8s of remaining time.\n",
      "\t-0.6842\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 17.6s of the 17.6s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.671654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1163. Best iteration is:\n",
      "\t[1153]\tvalid_set's rmse: 0.671577\n",
      "\t-0.6716\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the -0.58s of remaining time.\n",
      "\t-0.6072\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels23\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1921285889528227\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1921285889528227,\n",
      "    \"mean_squared_error\": -0.036913394693002664,\n",
      "    \"mean_absolute_error\": -0.049133661128035834,\n",
      "    \"r2\": 0.9954874563002659,\n",
      "    \"pearsonr\": 0.9977423356687083,\n",
      "    \"median_absolute_error\": -0.0037235589949935477\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels24\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.30 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 649\n",
      "Label Column: 649\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.0719402438, -19.0923416148, -1.91458, 3.40938)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61963.7 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.2 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 334 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 334 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t334 features in original data used to generate 334 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.9 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.57s of the 179.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.17141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1704\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.8s of the 176.8s of remaining time.\n",
      "\t-1.0704\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 175.14s of the 175.14s of remaining time.\n",
      "\t-1.0634\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 162.95s of the 162.95s of remaining time.\n",
      "\t-1.0906\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 156.14s of the 156.14s of remaining time.\n",
      "\t-1.0652\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.57s of the 146.62s of remaining time.\n",
      "\t-1.048\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 33.91s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels24\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.5516895738396247\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.5516895738396247,\n",
      "    \"mean_squared_error\": -0.30436138588334677,\n",
      "    \"mean_absolute_error\": -0.40071601114350897,\n",
      "    \"r2\": 0.9738133084552674,\n",
      "    \"pearsonr\": 0.9873012922099393,\n",
      "    \"median_absolute_error\": -0.30911378669821765\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels25\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.26 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 650\n",
      "Label Column: 650\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.8601105449, -17.1053348886, -1.34228, 3.30794)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62023.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.29 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 335 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 335 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t335 features in original data used to generate 335 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 27.98 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.869096\n",
      "[2000]\tvalid_set's rmse: 0.864744\n",
      "[3000]\tvalid_set's rmse: 0.863982\n",
      "[4000]\tvalid_set's rmse: 0.863734\n",
      "[5000]\tvalid_set's rmse: 0.863675\n",
      "[6000]\tvalid_set's rmse: 0.863672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8637\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.67s of the 168.67s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.809959\n",
      "[2000]\tvalid_set's rmse: 0.809519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8094\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.8s of the 163.8s of remaining time.\n",
      "\t-0.7908\t = Validation score   (-root_mean_squared_error)\n",
      "\t133.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 29.39s of the 29.39s of remaining time.\n",
      "\t-0.835\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 13.51s of the 13.51s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 872. Best iteration is:\n",
      "\t[793]\tvalid_set's rmse: 0.850219\n",
      "\t-0.8502\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the -0.43s of remaining time.\n",
      "\t-0.7865\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.04s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels25\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2490029915903085\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2490029915903085,\n",
      "    \"mean_squared_error\": -0.06200248982092315,\n",
      "    \"mean_absolute_error\": -0.0632262800913638,\n",
      "    \"r2\": 0.9943332307980303,\n",
      "    \"pearsonr\": 0.9971658808893591,\n",
      "    \"median_absolute_error\": -0.007568106647979356\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels26\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.18 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 651\n",
      "Label Column: 651\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.989651893, -14.9079491469, -0.48151, 3.1538)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62035.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.37 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 336 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 336 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t336 features in original data used to generate 336 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.714917\n",
      "[2000]\tvalid_set's rmse: 0.71159\n",
      "[3000]\tvalid_set's rmse: 0.710912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7108\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.11s of the 173.11s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.653502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6531\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 170.62s of the 170.62s of remaining time.\n",
      "\t-0.6231\t = Validation score   (-root_mean_squared_error)\n",
      "\t53.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 116.51s of the 116.51s of remaining time.\n",
      "\t-0.6729\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 106.69s of the 106.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.678933\n",
      "[2000]\tvalid_set's rmse: 0.678809\n",
      "[3000]\tvalid_set's rmse: 0.678808\n",
      "[4000]\tvalid_set's rmse: 0.678808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6788\t = Validation score   (-root_mean_squared_error)\n",
      "\t70.54s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the 34.81s of remaining time.\n",
      "\t-0.6212\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 145.76s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels26\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21518605153935463\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21518605153935463,\n",
      "    \"mean_squared_error\": -0.04630503677709762,\n",
      "    \"mean_absolute_error\": -0.10692321081736202,\n",
      "    \"r2\": 0.9953441205746154,\n",
      "    \"pearsonr\": 0.9976877132675321,\n",
      "    \"median_absolute_error\": -0.06187095014129649\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels27\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.09 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 652\n",
      "Label Column: 652\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.8278082787, -13.7978531825, 0.18682, 2.95906)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62059.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.46 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 337 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 337 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t337 features in original data used to generate 337 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.15 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.616645\n",
      "[2000]\tvalid_set's rmse: 0.613712\n",
      "[3000]\tvalid_set's rmse: 0.61293\n",
      "[4000]\tvalid_set's rmse: 0.612877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6128\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.85s of the 171.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.571272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5711\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 169.3s of the 169.3s of remaining time.\n",
      "\t-0.5459\t = Validation score   (-root_mean_squared_error)\n",
      "\t44.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 124.56s of the 124.56s of remaining time.\n",
      "\t-0.6204\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 102.2s of the 102.2s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.612321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6123\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the 73.1s of remaining time.\n",
      "\t-0.5428\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 107.45s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels27\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.20090218292282377\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.20090218292282377,\n",
      "    \"mean_squared_error\": -0.04036168710315594,\n",
      "    \"mean_absolute_error\": -0.11519535779436423,\n",
      "    \"r2\": 0.9953899821888428,\n",
      "    \"pearsonr\": 0.9977058458450196,\n",
      "    \"median_absolute_error\": -0.07389261574022368\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels28\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   66.02 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 653\n",
      "Label Column: 653\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.6994122905, -13.821432338, 0.56435, 2.89701)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62056.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.54 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 338 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 338 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t338 features in original data used to generate 338 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.23 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.57s of the 179.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.605201\n",
      "[2000]\tvalid_set's rmse: 0.602693\n",
      "[3000]\tvalid_set's rmse: 0.60242\n",
      "[4000]\tvalid_set's rmse: 0.602411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6023\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.89s of the 171.89s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.590008\n",
      "[2000]\tvalid_set's rmse: 0.58944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5894\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.23s of the 167.22s of remaining time.\n",
      "\t-0.5492\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.94s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 131.87s of the 131.87s of remaining time.\n",
      "\t-0.6172\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 115.89s of the 115.89s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.628105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6281\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.57s of the 91.61s of remaining time.\n",
      "\t-0.5483\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 88.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels28\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21721317069139479\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21721317069139479,\n",
      "    \"mean_squared_error\": -0.047181561521809134,\n",
      "    \"mean_absolute_error\": -0.1370102961697537,\n",
      "    \"r2\": 0.994377710501457,\n",
      "    \"pearsonr\": 0.9972159980243387,\n",
      "    \"median_absolute_error\": -0.09364474468450945\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels29\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.95 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 654\n",
      "Label Column: 654\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.3324515634, -14.5045370072, 1.07207, 3.0692)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61965.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.62 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 339 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 339 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t339 features in original data used to generate 339 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.31 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.710246\n",
      "[2000]\tvalid_set's rmse: 0.706167\n",
      "[3000]\tvalid_set's rmse: 0.705217\n",
      "[4000]\tvalid_set's rmse: 0.705088\n",
      "[5000]\tvalid_set's rmse: 0.705033\n",
      "[6000]\tvalid_set's rmse: 0.705003\n",
      "[7000]\tvalid_set's rmse: 0.704989\n",
      "[8000]\tvalid_set's rmse: 0.70499\n",
      "[9000]\tvalid_set's rmse: 0.70499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.705\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.32s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.34s of the 164.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.650267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6493\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.74s of the 160.74s of remaining time.\n",
      "\t-0.6149\t = Validation score   (-root_mean_squared_error)\n",
      "\t136.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 24.38s of the 24.38s of remaining time.\n",
      "\t-0.6709\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 2.58s of the 2.58s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 135. Best iteration is:\n",
      "\t[135]\tvalid_set's rmse: 0.717803\n",
      "\t-0.7178\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.67s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the -0.14s of remaining time.\n",
      "\t-0.6139\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.69s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels29\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1946189251705995\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1946189251705995,\n",
      "    \"mean_squared_error\": -0.03787652603455968,\n",
      "    \"mean_absolute_error\": -0.05599580482599959,\n",
      "    \"r2\": 0.9959787372542536,\n",
      "    \"pearsonr\": 0.9979900345395267,\n",
      "    \"median_absolute_error\": -0.00924982258703022\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels30\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.88 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 655\n",
      "Label Column: 655\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.4805798219, -12.8032386775, -0.86789, 3.08915)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61957.6 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.71 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 340 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 340 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t340 features in original data used to generate 340 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.4 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.3973\n",
      "[2000]\tvalid_set's rmse: 1.39098\n",
      "[3000]\tvalid_set's rmse: 1.39003\n",
      "[4000]\tvalid_set's rmse: 1.38958\n",
      "[5000]\tvalid_set's rmse: 1.38949\n",
      "[6000]\tvalid_set's rmse: 1.38955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.3895\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.26s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.65s of the 169.65s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.38391\n",
      "[2000]\tvalid_set's rmse: 1.38028\n",
      "[3000]\tvalid_set's rmse: 1.38008\n",
      "[4000]\tvalid_set's rmse: 1.38001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.38\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.81s of the 160.8s of remaining time.\n",
      "\t-1.3317\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.63s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 114.75s of the 114.75s of remaining time.\n",
      "\t-1.422\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 102.02s of the 102.02s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.43428\n",
      "[2000]\tvalid_set's rmse: 1.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.434\t = Validation score   (-root_mean_squared_error)\n",
      "\t38.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the 63.02s of remaining time.\n",
      "\t-1.3305\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 117.51s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels30\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.4693064487943597\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.4693064487943597,\n",
      "    \"mean_squared_error\": -0.2202485428799724,\n",
      "    \"mean_absolute_error\": -0.25320926927683907,\n",
      "    \"r2\": 0.9769177985239323,\n",
      "    \"pearsonr\": 0.988640108036007,\n",
      "    \"median_absolute_error\": -0.1463257192681945\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels31\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.80 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 656\n",
      "Label Column: 656\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.5852712877, -15.2015080025, -0.76877, 3.02281)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61958.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.79 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 341 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 341 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t341 features in original data used to generate 341 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.48 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.57s of the 179.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.722832\n",
      "[2000]\tvalid_set's rmse: 0.718368\n",
      "[3000]\tvalid_set's rmse: 0.717604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7173\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 174.14s of the 174.14s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.703171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7031\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 171.23s of the 171.23s of remaining time.\n",
      "\t-0.6597\t = Validation score   (-root_mean_squared_error)\n",
      "\t44.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 126.73s of the 126.72s of remaining time.\n",
      "\t-0.7235\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 114.78s of the 114.78s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.721702\n",
      "[2000]\tvalid_set's rmse: 0.721602\n",
      "[3000]\tvalid_set's rmse: 0.7216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7216\t = Validation score   (-root_mean_squared_error)\n",
      "\t56.82s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.57s of the 56.81s of remaining time.\n",
      "\t-0.6593\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 123.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels31\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24910539281162408\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24910539281162408,\n",
      "    \"mean_squared_error\": -0.06205349672783382,\n",
      "    \"mean_absolute_error\": -0.14845632295581734,\n",
      "    \"r2\": 0.9932081958599708,\n",
      "    \"pearsonr\": 0.9966333777342632,\n",
      "    \"median_absolute_error\": -0.09550996872634276\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels32\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.71 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 657\n",
      "Label Column: 657\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.8525746652, -17.3090446371, -0.5153, 2.99368)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61955.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.87 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 342 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 342 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t342 features in original data used to generate 342 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.56 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.55s of the 179.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.584713\n",
      "[2000]\tvalid_set's rmse: 0.580956\n",
      "[3000]\tvalid_set's rmse: 0.580707\n",
      "[4000]\tvalid_set's rmse: 0.58053\n",
      "[5000]\tvalid_set's rmse: 0.580587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5805\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 170.87s of the 170.87s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.545588\n",
      "[2000]\tvalid_set's rmse: 0.544691\n",
      "[3000]\tvalid_set's rmse: 0.544528\n",
      "[4000]\tvalid_set's rmse: 0.544493\n",
      "[5000]\tvalid_set's rmse: 0.544487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5445\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.7s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.58s of the 159.58s of remaining time.\n",
      "\t-0.5164\t = Validation score   (-root_mean_squared_error)\n",
      "\t103.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 55.44s of the 55.44s of remaining time.\n",
      "\t-0.5835\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 40.77s of the 40.77s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.586708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5867\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.55s of the 21.53s of remaining time.\n",
      "\t-0.5131\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 159.07s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels32\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16299441482492139\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16299441482492139,\n",
      "    \"mean_squared_error\": -0.02656717926411852,\n",
      "    \"mean_absolute_error\": -0.04951221961709721,\n",
      "    \"r2\": 0.9970353303326039,\n",
      "    \"pearsonr\": 0.9985180494387578,\n",
      "    \"median_absolute_error\": -0.009577877090561668\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels33\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.64 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 658\n",
      "Label Column: 658\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.9999880082, -17.0670983223, -0.06018, 2.98975)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61954.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 54.96 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 343 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 343 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t343 features in original data used to generate 343 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.65 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.579465\n",
      "[2000]\tvalid_set's rmse: 0.572638\n",
      "[3000]\tvalid_set's rmse: 0.571623\n",
      "[4000]\tvalid_set's rmse: 0.571536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5715\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.23s of the 172.23s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.552434\n",
      "[2000]\tvalid_set's rmse: 0.550527\n",
      "[3000]\tvalid_set's rmse: 0.550391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5504\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.54s of the 164.54s of remaining time.\n",
      "\t-0.4934\t = Validation score   (-root_mean_squared_error)\n",
      "\t136.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 27.18s of the 27.18s of remaining time.\n",
      "\t-0.5866\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 12.36s of the 12.36s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 775. Best iteration is:\n",
      "\t[769]\tvalid_set's rmse: 0.601916\n",
      "\t-0.6019\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the -0.43s of remaining time.\n",
      "\t-0.4934\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.96s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels33\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15619604518991487\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15619604518991487,\n",
      "    \"mean_squared_error\": -0.024397204532969903,\n",
      "    \"mean_absolute_error\": -0.04096547516259071,\n",
      "    \"r2\": 0.9972703274814955,\n",
      "    \"pearsonr\": 0.9986365354308719,\n",
      "    \"median_absolute_error\": -0.004244625426155113\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels34\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.57 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 659\n",
      "Label Column: 659\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.2556991971, -15.6721997835, 0.93712, 2.89977)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61948.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.04 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 344 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 344 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t344 features in original data used to generate 344 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.73 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.563433\n",
      "[2000]\tvalid_set's rmse: 0.557688\n",
      "[3000]\tvalid_set's rmse: 0.55701\n",
      "[4000]\tvalid_set's rmse: 0.556891\n",
      "[5000]\tvalid_set's rmse: 0.556902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5569\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.96s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.96s of the 169.96s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.493544\n",
      "[2000]\tvalid_set's rmse: 0.492922\n",
      "[3000]\tvalid_set's rmse: 0.49279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4928\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.83s of the 163.83s of remaining time.\n",
      "\t-0.4412\t = Validation score   (-root_mean_squared_error)\n",
      "\t67.96s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 95.21s of the 95.21s of remaining time.\n",
      "\t-0.5242\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 84.98s of the 84.98s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.527458\n",
      "[2000]\tvalid_set's rmse: 0.527331\n",
      "[3000]\tvalid_set's rmse: 0.52733\n",
      "[4000]\tvalid_set's rmse: 0.52733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5273\t = Validation score   (-root_mean_squared_error)\n",
      "\t67.87s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the 15.8s of remaining time.\n",
      "\t-0.4412\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 164.78s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels34\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14660323434519104\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14660323434519104,\n",
      "    \"mean_squared_error\": -0.021492508320471,\n",
      "    \"mean_absolute_error\": -0.06531900679226,\n",
      "    \"r2\": 0.9974437594960402,\n",
      "    \"pearsonr\": 0.9987249789724494,\n",
      "    \"median_absolute_error\": -0.030411295076623546\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels35\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.46 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 660\n",
      "Label Column: 660\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.9521051196, -13.234951118, 2.45634, 2.68515)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61939.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 345 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 345 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t345 features in original data used to generate 345 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.81 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.56s of the 179.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.54476\n",
      "[2000]\tvalid_set's rmse: 0.540521\n",
      "[3000]\tvalid_set's rmse: 0.539475\n",
      "[4000]\tvalid_set's rmse: 0.53934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5393\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.51s of the 171.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.495864\n",
      "[2000]\tvalid_set's rmse: 0.494115\n",
      "[3000]\tvalid_set's rmse: 0.493949\n",
      "[4000]\tvalid_set's rmse: 0.493913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4939\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.68s of the 162.68s of remaining time.\n",
      "\t-0.4487\t = Validation score   (-root_mean_squared_error)\n",
      "\t58.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 103.43s of the 103.43s of remaining time.\n",
      "\t-0.5123\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 90.86s of the 90.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.531549\n",
      "[2000]\tvalid_set's rmse: 0.531475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5315\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.56s of the 54.16s of remaining time.\n",
      "\t-0.4487\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 126.36s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels35\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15327993829070916\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15327993829070916,\n",
      "    \"mean_squared_error\": -0.023494739482403715,\n",
      "    \"mean_absolute_error\": -0.07566002630332311,\n",
      "    \"r2\": 0.9967410625591108,\n",
      "    \"pearsonr\": 0.9983755934749226,\n",
      "    \"median_absolute_error\": -0.03956212616134647\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels36\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.38 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 661\n",
      "Label Column: 661\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.5398616571, -11.0858867631, 3.67394, 2.61407)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61934.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.21 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 346 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 346 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t346 features in original data used to generate 346 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.9 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.55s of the 179.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.691596\n",
      "[2000]\tvalid_set's rmse: 0.684815\n",
      "[3000]\tvalid_set's rmse: 0.683989\n",
      "[4000]\tvalid_set's rmse: 0.68388\n",
      "[5000]\tvalid_set's rmse: 0.683862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6838\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.43s of the 169.43s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.636085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6353\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 165.28s of the 165.28s of remaining time.\n",
      "\t-0.6016\t = Validation score   (-root_mean_squared_error)\n",
      "\t81.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 83.26s of the 83.26s of remaining time.\n",
      "\t-0.6545\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 71.28s of the 71.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.665464\n",
      "[2000]\tvalid_set's rmse: 0.665391\n",
      "[3000]\tvalid_set's rmse: 0.66539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6654\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.66s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.55s of the 14.78s of remaining time.\n",
      "\t-0.6012\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 165.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels36\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1942742059462028\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1942742059462028,\n",
      "    \"mean_squared_error\": -0.03774246709602755,\n",
      "    \"mean_absolute_error\": -0.07271548759503359,\n",
      "    \"r2\": 0.9944762094875398,\n",
      "    \"pearsonr\": 0.9972464516767494,\n",
      "    \"median_absolute_error\": -0.02697139052391373\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels37\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.29 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 662\n",
      "Label Column: 662\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.9162224031, -12.4085260541, -0.98412, 3.02955)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62051.68 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.29 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 347 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 347 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t347 features in original data used to generate 347 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 28.98 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.55s of the 179.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.64029\n",
      "[2000]\tvalid_set's rmse: 0.636416\n",
      "[3000]\tvalid_set's rmse: 0.635362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6353\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.67s of the 172.67s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.618769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6187\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 169.66s of the 169.66s of remaining time.\n",
      "\t-0.5855\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 132.97s of the 132.97s of remaining time.\n",
      "\t-0.655\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 112.52s of the 112.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.662024\n",
      "[2000]\tvalid_set's rmse: 0.661948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6619\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.55s of the 78.75s of remaining time.\n",
      "\t-0.5847\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 101.78s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels37\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.23427840191317323\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.23427840191317323,\n",
      "    \"mean_squared_error\": -0.054886369602990474,\n",
      "    \"mean_absolute_error\": -0.15062537687416108,\n",
      "    \"r2\": 0.9940193105616527,\n",
      "    \"pearsonr\": 0.9970334489768061,\n",
      "    \"median_absolute_error\": -0.10382632746387932\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels38\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.22 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 663\n",
      "Label Column: 663\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.8792715433, -11.2120786641, 3.60318, 3.11287)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61944.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.37 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 348 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 348 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t348 features in original data used to generate 348 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.07 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.55s of the 179.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.11548\n",
      "[2000]\tvalid_set's rmse: 1.11193\n",
      "[3000]\tvalid_set's rmse: 1.11052\n",
      "[4000]\tvalid_set's rmse: 1.11056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1104\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.12s of the 172.12s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.05308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0524\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 169.24s of the 169.24s of remaining time.\n",
      "\t-1.0358\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.72s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 128.03s of the 128.03s of remaining time.\n",
      "\t-1.1018\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 118.77s of the 118.76s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.1126\n",
      "[2000]\tvalid_set's rmse: 1.11239\n",
      "[3000]\tvalid_set's rmse: 1.11239\n",
      "[4000]\tvalid_set's rmse: 1.11239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1124\t = Validation score   (-root_mean_squared_error)\n",
      "\t72.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.55s of the 44.92s of remaining time.\n",
      "\t-1.0318\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 135.67s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels38\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.3916507336131944\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.3916507336131944,\n",
      "    \"mean_squared_error\": -0.15339029713975272,\n",
      "    \"mean_absolute_error\": -0.23404548097622507,\n",
      "    \"r2\": 0.9841686295357852,\n",
      "    \"pearsonr\": 0.9922625143629331,\n",
      "    \"median_absolute_error\": -0.15108260062890633\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels39\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.12 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 664\n",
      "Label Column: 664\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.1637184687, -15.5105141457, -1.22137, 2.99338)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61985.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.46 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 349 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 349 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t349 features in original data used to generate 349 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.15 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.55s of the 179.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.554795\n",
      "[2000]\tvalid_set's rmse: 0.550076\n",
      "[3000]\tvalid_set's rmse: 0.54889\n",
      "[4000]\tvalid_set's rmse: 0.548859\n",
      "[5000]\tvalid_set's rmse: 0.548798\n",
      "[6000]\tvalid_set's rmse: 0.5488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5488\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.05s of the 169.04s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.515087\n",
      "[2000]\tvalid_set's rmse: 0.513626\n",
      "[3000]\tvalid_set's rmse: 0.513444\n",
      "[4000]\tvalid_set's rmse: 0.513435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5134\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.51s of the 159.51s of remaining time.\n",
      "\t-0.4857\t = Validation score   (-root_mean_squared_error)\n",
      "\t75.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 83.81s of the 83.81s of remaining time.\n",
      "\t-0.5552\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 60.61s of the 60.6s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.553418\n",
      "[2000]\tvalid_set's rmse: 0.553339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5533\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.55s of the 20.55s of remaining time.\n",
      "\t-0.4837\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 159.99s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels39\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1567415789057668\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1567415789057668,\n",
      "    \"mean_squared_error\": -0.02456792255787266,\n",
      "    \"mean_absolute_error\": -0.061562098633577964,\n",
      "    \"r2\": 0.9972578776907902,\n",
      "    \"pearsonr\": 0.9986303640216626,\n",
      "    \"median_absolute_error\": -0.022523574408142122\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels40\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   65.03 GB / 2000.36 GB (3.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 665\n",
      "Label Column: 665\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.7457281497, -13.0423020623, 2.98025, 3.17824)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61941.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.54 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 350 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 350 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t350 features in original data used to generate 350 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.23 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.55s of the 179.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.788013\n",
      "[2000]\tvalid_set's rmse: 0.781734\n",
      "[3000]\tvalid_set's rmse: 0.781274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7811\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.08s of the 173.08s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.72438\n",
      "[2000]\tvalid_set's rmse: 0.723481\n",
      "[3000]\tvalid_set's rmse: 0.723279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7232\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 166.71s of the 166.71s of remaining time.\n",
      "\t-0.6925\t = Validation score   (-root_mean_squared_error)\n",
      "\t95.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 70.35s of the 70.35s of remaining time.\n",
      "\t-0.7588\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 59.5s of the 59.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.776375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7764\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.55s of the 37.5s of remaining time.\n",
      "\t-0.691\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 143.02s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels40\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22047259772006106\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22047259772006106,\n",
      "    \"mean_squared_error\": -0.04860816634543179,\n",
      "    \"mean_absolute_error\": -0.07118841796741672,\n",
      "    \"r2\": 0.9951874278350856,\n",
      "    \"pearsonr\": 0.9975976461001105,\n",
      "    \"median_absolute_error\": -0.018261578520244598\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels41\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.96 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 666\n",
      "Label Column: 666\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.8738832264, -14.859114461, -1.67353, 3.02599)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61942.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.62 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 351 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 351 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t351 features in original data used to generate 351 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.32 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.54s of the 179.54s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.561073\n",
      "[2000]\tvalid_set's rmse: 0.555922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5559\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 174.33s of the 174.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.548404\n",
      "[2000]\tvalid_set's rmse: 0.547606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5475\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 169.06s of the 169.06s of remaining time.\n",
      "\t-0.497\t = Validation score   (-root_mean_squared_error)\n",
      "\t140.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 27.41s of the 27.41s of remaining time.\n",
      "\t-0.5919\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 4.26s of the 4.26s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 230. Best iteration is:\n",
      "\t[230]\tvalid_set's rmse: 0.604447\n",
      "\t-0.6044\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.54s of the -0.24s of remaining time.\n",
      "\t-0.497\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.77s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels41\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1572731344080201\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1572731344080201,\n",
      "    \"mean_squared_error\": -0.024734838806523125,\n",
      "    \"mean_absolute_error\": -0.04083793082378471,\n",
      "    \"r2\": 0.9972984264500999,\n",
      "    \"pearsonr\": 0.9986490585000242,\n",
      "    \"median_absolute_error\": -0.003323195884146146\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels42\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.90 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 667\n",
      "Label Column: 667\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.6729944113, -14.0276518179, 2.65298, 3.15912)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61913.23 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.71 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 352 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 352 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t352 features in original data used to generate 352 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.4 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.55s of the 179.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.659719\n",
      "[2000]\tvalid_set's rmse: 0.656654\n",
      "[3000]\tvalid_set's rmse: 0.656016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.656\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.59s of the 173.59s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.607981\n",
      "[2000]\tvalid_set's rmse: 0.606006\n",
      "[3000]\tvalid_set's rmse: 0.605696\n",
      "[4000]\tvalid_set's rmse: 0.60566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6057\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.1s of the 164.1s of remaining time.\n",
      "\t-0.5537\t = Validation score   (-root_mean_squared_error)\n",
      "\t139.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 23.81s of the 23.81s of remaining time.\n",
      "\t-0.6447\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 7.46s of the 7.46s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 432. Best iteration is:\n",
      "\t[429]\tvalid_set's rmse: 0.639193\n",
      "\t-0.6392\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.59s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.55s of the -0.29s of remaining time.\n",
      "\t-0.5536\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.84s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels42\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17513420372501814\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17513420372501814,\n",
      "    \"mean_squared_error\": -0.030671989314396294,\n",
      "    \"mean_absolute_error\": -0.04413826893522628,\n",
      "    \"r2\": 0.9969263745533343,\n",
      "    \"pearsonr\": 0.9984643230466588,\n",
      "    \"median_absolute_error\": -0.002532212561999647\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels43\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.84 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 668\n",
      "Label Column: 668\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.9262519068, -16.2225193739, -2.21085, 3.26565)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61924.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.79 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 353 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 353 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t353 features in original data used to generate 353 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.48 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.54s of the 179.54s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.653789\n",
      "[2000]\tvalid_set's rmse: 0.648005\n",
      "[3000]\tvalid_set's rmse: 0.646913\n",
      "[4000]\tvalid_set's rmse: 0.646781\n",
      "[5000]\tvalid_set's rmse: 0.646744\n",
      "[6000]\tvalid_set's rmse: 0.646708\n",
      "[7000]\tvalid_set's rmse: 0.646695\n",
      "[8000]\tvalid_set's rmse: 0.64669\n",
      "[9000]\tvalid_set's rmse: 0.646688\n",
      "[10000]\tvalid_set's rmse: 0.646688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6467\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.26s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.03s of the 162.03s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.647593\n",
      "[2000]\tvalid_set's rmse: 0.645321\n",
      "[3000]\tvalid_set's rmse: 0.644909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6449\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 154.23s of the 154.23s of remaining time.\n",
      "\t-0.5798\t = Validation score   (-root_mean_squared_error)\n",
      "\t73.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 80.42s of the 80.42s of remaining time.\n",
      "\t-0.6595\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 57.03s of the 57.03s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.670792\n",
      "[2000]\tvalid_set's rmse: 0.67067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6707\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.54s of the 21.11s of remaining time.\n",
      "\t-0.5798\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 159.45s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels43\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19040629714121854\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19040629714121854,\n",
      "    \"mean_squared_error\": -0.036254557991030056,\n",
      "    \"mean_absolute_error\": -0.08020285248809012,\n",
      "    \"r2\": 0.9966001119749067,\n",
      "    \"pearsonr\": 0.9982996549529466,\n",
      "    \"median_absolute_error\": -0.03472827870282591\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels44\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.73 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 669\n",
      "Label Column: 669\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.7968096753, -13.7097565395, 2.1942, 3.1552)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61918.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.88 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 354 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 354 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t354 features in original data used to generate 354 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.57 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.54s of the 179.54s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.603094\n",
      "[2000]\tvalid_set's rmse: 0.597882\n",
      "[3000]\tvalid_set's rmse: 0.596994\n",
      "[4000]\tvalid_set's rmse: 0.596712\n",
      "[5000]\tvalid_set's rmse: 0.596683\n",
      "[6000]\tvalid_set's rmse: 0.596665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5966\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.1s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.71s of the 167.71s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.569842\n",
      "[2000]\tvalid_set's rmse: 0.567764\n",
      "[3000]\tvalid_set's rmse: 0.567501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5675\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.85s of the 159.85s of remaining time.\n",
      "\t-0.5308\t = Validation score   (-root_mean_squared_error)\n",
      "\t120.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 39.16s of the 39.16s of remaining time.\n",
      "\t-0.6178\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 18.01s of the 18.01s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.627615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1097. Best iteration is:\n",
      "\t[1073]\tvalid_set's rmse: 0.627563\n",
      "\t-0.6276\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.54s of the -0.88s of remaining time.\n",
      "\t-0.529\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.51s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels44\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16791598292601992\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16791598292601992,\n",
      "    \"mean_squared_error\": -0.028195777322011263,\n",
      "    \"mean_absolute_error\": -0.04971056567547089,\n",
      "    \"r2\": 0.9971674930697695,\n",
      "    \"pearsonr\": 0.9985839919304447,\n",
      "    \"median_absolute_error\": -0.008887605985717695\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels45\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.65 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 670\n",
      "Label Column: 670\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.004832185, -16.6828364714, -2.10895, 3.40391)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62043.9 MB\n",
      "\tTrain Data (Original)  Memory Usage: 55.96 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 355 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 355 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t355 features in original data used to generate 355 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.65 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.53s of the 179.53s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.775625\n",
      "[2000]\tvalid_set's rmse: 0.76952\n",
      "[3000]\tvalid_set's rmse: 0.768211\n",
      "[4000]\tvalid_set's rmse: 0.76803\n",
      "[5000]\tvalid_set's rmse: 0.76798\n",
      "[6000]\tvalid_set's rmse: 0.767938\n",
      "[7000]\tvalid_set's rmse: 0.767937\n",
      "[8000]\tvalid_set's rmse: 0.767934\n",
      "[9000]\tvalid_set's rmse: 0.767934\n",
      "[10000]\tvalid_set's rmse: 0.767934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7679\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.39s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.56s of the 161.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.723285\n",
      "[2000]\tvalid_set's rmse: 0.721632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7216\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.9s of the 155.9s of remaining time.\n",
      "\t-0.682\t = Validation score   (-root_mean_squared_error)\n",
      "\t142.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 12.69s of the 12.69s of remaining time.\n",
      "\t-0.7727\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.53s of the -0.54s of remaining time.\n",
      "\t-0.6811\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels45\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21547453129292427\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21547453129292427,\n",
      "    \"mean_squared_error\": -0.046429273635905426,\n",
      "    \"mean_absolute_error\": -0.05487092924950755,\n",
      "    \"r2\": 0.9959924674909613,\n",
      "    \"pearsonr\": 0.9979964373306651,\n",
      "    \"median_absolute_error\": -0.0038265325804869854\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels46\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.57 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 671\n",
      "Label Column: 671\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.5069850625, -16.2614576504, 1.94652, 3.26669)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61930.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.04 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 356 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 356 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t356 features in original data used to generate 356 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.73 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.53s of the 179.53s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.701745\n",
      "[2000]\tvalid_set's rmse: 0.694259\n",
      "[3000]\tvalid_set's rmse: 0.694182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6939\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.43s of the 173.43s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.659404\n",
      "[2000]\tvalid_set's rmse: 0.65887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6588\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.64s of the 168.63s of remaining time.\n",
      "\t-0.6246\t = Validation score   (-root_mean_squared_error)\n",
      "\t71.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 96.16s of the 96.16s of remaining time.\n",
      "\t-0.6894\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 81.63s of the 81.63s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.676222\n",
      "[2000]\tvalid_set's rmse: 0.676062\n",
      "[3000]\tvalid_set's rmse: 0.676061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6761\t = Validation score   (-root_mean_squared_error)\n",
      "\t57.91s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.53s of the 22.62s of remaining time.\n",
      "\t-0.6226\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 157.96s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels46\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2034690894838984\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2034690894838984,\n",
      "    \"mean_squared_error\": -0.041399670375406775,\n",
      "    \"mean_absolute_error\": -0.08352529960348887,\n",
      "    \"r2\": 0.9961200757474659,\n",
      "    \"pearsonr\": 0.9980690435789287,\n",
      "    \"median_absolute_error\": -0.0346501582179537\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels47\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.48 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 672\n",
      "Label Column: 672\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.2611821202, -18.216882584, -1.28251, 3.47414)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61932.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.13 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 357 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 357 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t357 features in original data used to generate 357 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.82 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.54s of the 179.54s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.27856\n",
      "[2000]\tvalid_set's rmse: 1.27519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.2748\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 174.35s of the 174.35s of remaining time.\n",
      "\t-1.2372\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.74s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 172.55s of the 172.55s of remaining time.\n",
      "\t-1.2011\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 156.43s of the 156.43s of remaining time.\n",
      "\t-1.2739\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 148.46s of the 148.45s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.24296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.2429\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.54s of the 129.36s of remaining time.\n",
      "\t-1.1994\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 51.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels47\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.6890385591372713\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.6890385591372713,\n",
      "    \"mean_squared_error\": -0.474774135977966,\n",
      "    \"mean_absolute_error\": -0.5061456798962558,\n",
      "    \"r2\": 0.9606599605102004,\n",
      "    \"pearsonr\": 0.9809105411729739,\n",
      "    \"median_absolute_error\": -0.3846423964341797\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels48\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.43 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 673\n",
      "Label Column: 673\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.2938678883, -18.4722024577, -0.81802, 3.27456)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62038.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.21 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 358 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 358 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t358 features in original data used to generate 358 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.9 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.53s of the 179.53s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.820276\n",
      "[2000]\tvalid_set's rmse: 0.817933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8175\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 175.1s of the 175.1s of remaining time.\n",
      "\t-0.8039\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 173.04s of the 173.04s of remaining time.\n",
      "\t-0.7515\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 124.51s of the 124.51s of remaining time.\n",
      "\t-0.8404\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 114.36s of the 114.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.826028\n",
      "[2000]\tvalid_set's rmse: 0.825886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8259\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.53s of the 79.86s of remaining time.\n",
      "\t-0.7514\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 100.69s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels48\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2791888030836414\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2791888030836414,\n",
      "    \"mean_squared_error\": -0.07794638776727658,\n",
      "    \"mean_absolute_error\": -0.16074410787724952,\n",
      "    \"r2\": 0.9927300568902675,\n",
      "    \"pearsonr\": 0.9963849519270853,\n",
      "    \"median_absolute_error\": -0.09860660026965329\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels49\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.37 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 674\n",
      "Label Column: 674\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.9497558604, -15.3969312896, -0.37459, 3.1064)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61928.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.29 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 359 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 359 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t359 features in original data used to generate 359 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 29.98 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.54s of the 179.54s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.693139\n",
      "[2000]\tvalid_set's rmse: 0.687651\n",
      "[3000]\tvalid_set's rmse: 0.686579\n",
      "[4000]\tvalid_set's rmse: 0.686371\n",
      "[5000]\tvalid_set's rmse: 0.686291\n",
      "[6000]\tvalid_set's rmse: 0.686263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6863\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.41s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.37s of the 167.37s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.663411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6634\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.21s of the 164.21s of remaining time.\n",
      "\t-0.6108\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 124.09s of the 124.09s of remaining time.\n",
      "\t-0.6789\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 106.33s of the 106.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.700486\n",
      "[2000]\tvalid_set's rmse: 0.700397\n",
      "[3000]\tvalid_set's rmse: 0.700395\n",
      "[4000]\tvalid_set's rmse: 0.700395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7004\t = Validation score   (-root_mean_squared_error)\n",
      "\t75.78s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.54s of the 28.95s of remaining time.\n",
      "\t-0.6108\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 151.65s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels49\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24726084548259578\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24726084548259578,\n",
      "    \"mean_squared_error\": -0.06113792570876816,\n",
      "    \"mean_absolute_error\": -0.15920089729119752,\n",
      "    \"r2\": 0.9936636545650472,\n",
      "    \"pearsonr\": 0.9968543365040108,\n",
      "    \"median_absolute_error\": -0.10797210438624266\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels50\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.26 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 675\n",
      "Label Column: 675\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.7025354052, -13.5315571594, -0.02145, 3.07468)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    62027.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.38 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 360 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 360 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t360 features in original data used to generate 360 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.07 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.52s of the 179.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.65049\n",
      "[2000]\tvalid_set's rmse: 0.647191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6471\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 174.23s of the 174.23s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.63561\n",
      "[2000]\tvalid_set's rmse: 0.63454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6343\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 169.12s of the 169.12s of remaining time.\n",
      "\t-0.5882\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 122.21s of the 122.21s of remaining time.\n",
      "\t-0.682\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 101.27s of the 101.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.68529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6852\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.53s of the 71.37s of remaining time.\n",
      "\t-0.5871\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 109.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels50\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21299426823789522\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21299426823789522,\n",
      "    \"mean_squared_error\": -0.045366558302196394,\n",
      "    \"mean_absolute_error\": -0.11953488662329545,\n",
      "    \"r2\": 0.9952007038157727,\n",
      "    \"pearsonr\": 0.9976123663792861,\n",
      "    \"median_absolute_error\": -0.07271093384934069\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels51\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.20 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 676\n",
      "Label Column: 676\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.0917165546, -16.5481769445, 0.19672, 3.10344)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61918.2 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.46 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 361 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 361 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t361 features in original data used to generate 361 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.15 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.53s of the 179.53s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.604554\n",
      "[2000]\tvalid_set's rmse: 0.601345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.601\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.98s of the 173.98s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.587112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5863\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 170.13s of the 170.13s of remaining time.\n",
      "\t-0.5392\t = Validation score   (-root_mean_squared_error)\n",
      "\t54.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 115.6s of the 115.6s of remaining time.\n",
      "\t-0.6117\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.1s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 91.08s of the 91.08s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.616092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.616\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.53s of the 59.23s of remaining time.\n",
      "\t-0.5384\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 121.63s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels51\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1921259110028338\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1921259110028338,\n",
      "    \"mean_squared_error\": -0.03691236567866892,\n",
      "    \"mean_absolute_error\": -0.10527300346666038,\n",
      "    \"r2\": 0.9961670950969644,\n",
      "    \"pearsonr\": 0.9980888346044008,\n",
      "    \"median_absolute_error\": -0.062310948833128404\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels52\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.13 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 677\n",
      "Label Column: 677\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.986229173, -16.4801472825, 0.47786, 3.17995)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61323.31 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.54 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 362 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 362 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t362 features in original data used to generate 362 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.23 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.51s of the 179.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.633312\n",
      "[2000]\tvalid_set's rmse: 0.626747\n",
      "[3000]\tvalid_set's rmse: 0.625697\n",
      "[4000]\tvalid_set's rmse: 0.625287\n",
      "[5000]\tvalid_set's rmse: 0.625277\n",
      "[6000]\tvalid_set's rmse: 0.625276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6253\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.62s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.95s of the 166.95s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.592052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5918\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.32s of the 163.32s of remaining time.\n",
      "\t-0.5389\t = Validation score   (-root_mean_squared_error)\n",
      "\t161.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1.23s of the 1.23s of remaining time.\n",
      "\t-0.7087\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.51s of the -0.57s of remaining time.\n",
      "\t-0.5386\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels52\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1707519796204286\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1707519796204286,\n",
      "    \"mean_squared_error\": -0.029156238544295315,\n",
      "    \"mean_absolute_error\": -0.049969321238108454,\n",
      "    \"r2\": 0.9971164078304142,\n",
      "    \"pearsonr\": 0.9985590141326711,\n",
      "    \"median_absolute_error\": -0.00837210199887084\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels53\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   64.07 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 678\n",
      "Label Column: 678\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.4040089406, -16.1502118156, 1.04514, 3.32794)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60948.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.63 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 363 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 363 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t363 features in original data used to generate 363 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.32 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.51s of the 179.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.667972\n",
      "[2000]\tvalid_set's rmse: 0.661098\n",
      "[3000]\tvalid_set's rmse: 0.66027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6602\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.05s of the 171.04s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.624046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6234\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 165.84s of the 165.84s of remaining time.\n",
      "\t-0.5858\t = Validation score   (-root_mean_squared_error)\n",
      "\t103.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 61.38s of the 61.38s of remaining time.\n",
      "\t-0.6585\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 40.8s of the 40.79s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.683058\n",
      "[2000]\tvalid_set's rmse: 0.682954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2479. Best iteration is:\n",
      "\t[2332]\tvalid_set's rmse: 0.682953\n",
      "\t-0.683\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.11s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.51s of the -2.79s of remaining time.\n",
      "\t-0.5841\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 185.91s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels53\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18852695582542245\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18852695582542245,\n",
      "    \"mean_squared_error\": -0.035542413072800795,\n",
      "    \"mean_absolute_error\": -0.06941858223874137,\n",
      "    \"r2\": 0.9967904930324927,\n",
      "    \"pearsonr\": 0.9984000725586493,\n",
      "    \"median_absolute_error\": -0.02461124288699046\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels54\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.98 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 679\n",
      "Label Column: 679\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.8229033932, -13.7104171859, -0.62098, 3.22257)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60350.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.71 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 364 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 364 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t364 features in original data used to generate 364 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.4 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.53s of the 179.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.862812\n",
      "[2000]\tvalid_set's rmse: 0.859682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8594\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.74s of the 173.74s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.802747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8022\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 170.11s of the 170.11s of remaining time.\n",
      "\t-0.7718\t = Validation score   (-root_mean_squared_error)\n",
      "\t91.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 78.12s of the 78.12s of remaining time.\n",
      "\t-0.8344\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.75s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 67.15s of the 67.15s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.817923\n",
      "[2000]\tvalid_set's rmse: 0.817813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8178\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.53s of the 32.99s of remaining time.\n",
      "\t-0.7697\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 149.22s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels54\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.25208767272346055\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.25208767272346055,\n",
      "    \"mean_squared_error\": -0.06354819473913058,\n",
      "    \"mean_absolute_error\": -0.10561382346733594,\n",
      "    \"r2\": 0.9938801463766449,\n",
      "    \"pearsonr\": 0.9969622563291123,\n",
      "    \"median_absolute_error\": -0.044773206085265915\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels55\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.92 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 680\n",
      "Label Column: 680\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.899998684, -14.0283994232, -0.77741, 3.07007)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61223.57 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.79 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 365 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 365 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t365 features in original data used to generate 365 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.48 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.51s of the 179.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.69306\n",
      "[2000]\tvalid_set's rmse: 0.687555\n",
      "[3000]\tvalid_set's rmse: 0.686647\n",
      "[4000]\tvalid_set's rmse: 0.686553\n",
      "[5000]\tvalid_set's rmse: 0.686505\n",
      "[6000]\tvalid_set's rmse: 0.686487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6865\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.84s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.95s of the 166.95s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.647506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6474\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.46s of the 163.46s of remaining time.\n",
      "\t-0.6175\t = Validation score   (-root_mean_squared_error)\n",
      "\t141.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 21.18s of the 21.17s of remaining time.\n",
      "\t-0.6694\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.51s of the -0.67s of remaining time.\n",
      "\t-0.6152\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels55\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1967847407062605\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1967847407062605,\n",
      "    \"mean_squared_error\": -0.03872423417483022,\n",
      "    \"mean_absolute_error\": -0.0673387951152642,\n",
      "    \"r2\": 0.9958910858519437,\n",
      "    \"pearsonr\": 0.9979447227000371,\n",
      "    \"median_absolute_error\": -0.020983098675610434\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels56\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.85 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 681\n",
      "Label Column: 681\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.3849783004, -15.6938585959, -0.58703, 2.99264)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61212.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.88 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 366 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 366 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t366 features in original data used to generate 366 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.57 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.55s of the 179.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.604393\n",
      "[2000]\tvalid_set's rmse: 0.600693\n",
      "[3000]\tvalid_set's rmse: 0.599797\n",
      "[4000]\tvalid_set's rmse: 0.599751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5997\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.83s of the 171.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.565154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5642\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.75s of the 167.75s of remaining time.\n",
      "\t-0.5245\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 15.99s of the 15.99s of remaining time.\n",
      "\t-0.5847\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.13s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.55s of the -0.29s of remaining time.\n",
      "\t-0.523\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.87s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels56\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16555933158316144\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16555933158316144,\n",
      "    \"mean_squared_error\": -0.02740989227426324,\n",
      "    \"mean_absolute_error\": -0.043808967139974564,\n",
      "    \"r2\": 0.9969391710202242,\n",
      "    \"pearsonr\": 0.9984692915703994,\n",
      "    \"median_absolute_error\": -0.004879116110702353\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels57\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.79 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 682\n",
      "Label Column: 682\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.8650593906, -15.1606046956, -0.15051, 2.95924)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61208.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 56.96 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 367 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 367 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t367 features in original data used to generate 367 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.65 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.48s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.52s of the 179.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.562691\n",
      "[2000]\tvalid_set's rmse: 0.556628\n",
      "[3000]\tvalid_set's rmse: 0.555846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5558\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.77s of the 171.76s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.546068\n",
      "[2000]\tvalid_set's rmse: 0.545011\n",
      "[3000]\tvalid_set's rmse: 0.54483\n",
      "[4000]\tvalid_set's rmse: 0.5448\n",
      "[5000]\tvalid_set's rmse: 0.544797\n",
      "[6000]\tvalid_set's rmse: 0.544797\n",
      "[7000]\tvalid_set's rmse: 0.544797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5448\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.71s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.34s of the 155.34s of remaining time.\n",
      "\t-0.5093\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3.37s of the 3.37s of remaining time.\n",
      "\t-0.5871\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.52s of the -0.21s of remaining time.\n",
      "\t-0.5076\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.78s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels57\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16054581893858855\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16054581893858855,\n",
      "    \"mean_squared_error\": -0.02577495997866207,\n",
      "    \"mean_absolute_error\": -0.03984253568763375,\n",
      "    \"r2\": 0.997056406156142,\n",
      "    \"pearsonr\": 0.9985283983682731,\n",
      "    \"median_absolute_error\": -0.0021644327422241094\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels58\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.73 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 683\n",
      "Label Column: 683\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (9.9821122298, -14.1108038795, 0.64254, 2.82507)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61200.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.04 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 368 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 368 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t368 features in original data used to generate 368 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.74 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.51s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.548247\n",
      "[2000]\tvalid_set's rmse: 0.542981\n",
      "[3000]\tvalid_set's rmse: 0.542208\n",
      "[4000]\tvalid_set's rmse: 0.541964\n",
      "[5000]\tvalid_set's rmse: 0.541916\n",
      "[6000]\tvalid_set's rmse: 0.541894\n",
      "[7000]\tvalid_set's rmse: 0.541895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5419\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.93s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.76s of the 164.76s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.476453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4761\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.09s of the 160.09s of remaining time.\n",
      "\t-0.4487\t = Validation score   (-root_mean_squared_error)\n",
      "\t92.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 67.22s of the 67.21s of remaining time.\n",
      "\t-0.5171\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.63s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 42.42s of the 42.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.530074\n",
      "[2000]\tvalid_set's rmse: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2563. Best iteration is:\n",
      "\t[2264]\tvalid_set's rmse: 0.529999\n",
      "\t-0.53\t = Validation score   (-root_mean_squared_error)\n",
      "\t42.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.51s of the -1.25s of remaining time.\n",
      "\t-0.4474\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.88s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels58\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14462322976068204\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14462322976068204,\n",
      "    \"mean_squared_error\": -0.020915878586410915,\n",
      "    \"mean_absolute_error\": -0.054397793421805736,\n",
      "    \"r2\": 0.9973790497266648,\n",
      "    \"pearsonr\": 0.9986919844794258,\n",
      "    \"median_absolute_error\": -0.019718337344629022\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels59\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.63 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 684\n",
      "Label Column: 684\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.2258290449, -14.0197035301, 1.83191, 2.57797)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61132.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.13 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 369 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 369 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t369 features in original data used to generate 369 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.82 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.51s of the 179.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.56645\n",
      "[2000]\tvalid_set's rmse: 0.562986\n",
      "[3000]\tvalid_set's rmse: 0.562454\n",
      "[4000]\tvalid_set's rmse: 0.562286\n",
      "[5000]\tvalid_set's rmse: 0.562178\n",
      "[6000]\tvalid_set's rmse: 0.562173\n",
      "[7000]\tvalid_set's rmse: 0.562167\n",
      "[8000]\tvalid_set's rmse: 0.562163\n",
      "[9000]\tvalid_set's rmse: 0.562162\n",
      "[10000]\tvalid_set's rmse: 0.562161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5622\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.44s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.75s of the 159.75s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.485942\n",
      "[2000]\tvalid_set's rmse: 0.485595\n",
      "[3000]\tvalid_set's rmse: 0.48553\n",
      "[4000]\tvalid_set's rmse: 0.485523\n",
      "[5000]\tvalid_set's rmse: 0.485515\n",
      "[6000]\tvalid_set's rmse: 0.485514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4855\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 145.1s of the 145.1s of remaining time.\n",
      "\t-0.481\t = Validation score   (-root_mean_squared_error)\n",
      "\t88.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 55.89s of the 55.89s of remaining time.\n",
      "\t-0.5113\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 43.98s of the 43.98s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.518797\n",
      "[2000]\tvalid_set's rmse: 0.518751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2699. Best iteration is:\n",
      "\t[2604]\tvalid_set's rmse: 0.51875\n",
      "\t-0.5188\t = Validation score   (-root_mean_squared_error)\n",
      "\t44.38s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.51s of the -1.53s of remaining time.\n",
      "\t-0.4725\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.12s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels59\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15046598766974043\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15046598766974043,\n",
      "    \"mean_squared_error\": -0.022640013445430502,\n",
      "    \"mean_absolute_error\": -0.04576983302013614,\n",
      "    \"r2\": 0.996593064494324,\n",
      "    \"pearsonr\": 0.9982967972920014,\n",
      "    \"median_absolute_error\": -0.011453441859906044\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels60\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.51 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 685\n",
      "Label Column: 685\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.3724343946, -10.9125667451, 2.89129, 2.51199)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60991.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.21 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 370 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 370 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t370 features in original data used to generate 370 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.9 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.53s of the 179.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.684731\n",
      "[2000]\tvalid_set's rmse: 0.680664\n",
      "[3000]\tvalid_set's rmse: 0.679338\n",
      "[4000]\tvalid_set's rmse: 0.679186\n",
      "[5000]\tvalid_set's rmse: 0.6791\n",
      "[6000]\tvalid_set's rmse: 0.679107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6791\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.26s of the 166.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.636773\n",
      "[2000]\tvalid_set's rmse: 0.634472\n",
      "[3000]\tvalid_set's rmse: 0.634461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6344\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 158.76s of the 158.76s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9904.\n",
      "\t-0.6113\t = Validation score   (-root_mean_squared_error)\n",
      "\t158.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.53s of the -1.17s of remaining time.\n",
      "\t-0.6101\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.7s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels60\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19309665718145158\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19309665718145158,\n",
      "    \"mean_squared_error\": -0.037286319014650876,\n",
      "    \"mean_absolute_error\": -0.049252387231306864,\n",
      "    \"r2\": 0.9940904202446943,\n",
      "    \"pearsonr\": 0.9970429539609496,\n",
      "    \"median_absolute_error\": -0.004857873738110197\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels61\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.45 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 686\n",
      "Label Column: 686\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.1019532325, -12.8988439826, 3.27737, 2.93622)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60703.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.29 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 371 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 371 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t371 features in original data used to generate 371 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.99 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.05608\n",
      "[2000]\tvalid_set's rmse: 1.05144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0511\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.8s of the 169.8s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.966124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9649\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.27s of the 163.27s of remaining time.\n",
      "\t-0.951\t = Validation score   (-root_mean_squared_error)\n",
      "\t120.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 42.73s of the 42.73s of remaining time.\n",
      "\t-1.0018\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.55s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 25.03s of the 25.03s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.03547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1142. Best iteration is:\n",
      "\t[1091]\tvalid_set's rmse: 1.03543\n",
      "\t-1.0354\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -0.57s of remaining time.\n",
      "\t-0.9445\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels61\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.3037477685459772\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.3037477685459772,\n",
      "    \"mean_squared_error\": -0.09226270689666072,\n",
      "    \"mean_absolute_error\": -0.10779415221524076,\n",
      "    \"r2\": 0.9892973479071013,\n",
      "    \"pearsonr\": 0.9946594085886002,\n",
      "    \"median_absolute_error\": -0.036527815438201994\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels62\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.38 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 687\n",
      "Label Column: 687\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.7256313754, -11.9411725298, -1.12953, 3.17769)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60860.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.38 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 372 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 372 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t372 features in original data used to generate 372 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.07 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.921072\n",
      "[2000]\tvalid_set's rmse: 0.918531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9182\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.65s of the 173.65s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.874812\n",
      "[2000]\tvalid_set's rmse: 0.873661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8736\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.74s of the 167.74s of remaining time.\n",
      "\t-0.8614\t = Validation score   (-root_mean_squared_error)\n",
      "\t60.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 107.51s of the 107.51s of remaining time.\n",
      "\t-0.9063\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 96.07s of the 96.07s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.909093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.909\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the 76.0s of remaining time.\n",
      "\t-0.8547\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 104.59s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels62\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2864368648201157\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2864368648201157,\n",
      "    \"mean_squared_error\": -0.08204607752797731,\n",
      "    \"mean_absolute_error\": -0.13221858655725827,\n",
      "    \"r2\": 0.991873994460579,\n",
      "    \"pearsonr\": 0.9959542859979177,\n",
      "    \"median_absolute_error\": -0.06519318718787837\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels63\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.32 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 688\n",
      "Label Column: 688\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.5089293204, -11.215110149, 3.87328, 2.99523)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60872.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.46 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 373 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 373 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t373 features in original data used to generate 373 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.15 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.526896\n",
      "[2000]\tvalid_set's rmse: 0.524806\n",
      "[3000]\tvalid_set's rmse: 0.524679\n",
      "[4000]\tvalid_set's rmse: 0.524749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5246\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.23s of the 171.23s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.45031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4496\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 166.8s of the 166.79s of remaining time.\n",
      "\t-0.4223\t = Validation score   (-root_mean_squared_error)\n",
      "\t103.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 63.19s of the 63.19s of remaining time.\n",
      "\t-0.4732\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 47.91s of the 47.91s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.49241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4924\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the 18.37s of remaining time.\n",
      "\t-0.4203\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 162.23s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels63\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13466773928327716\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13466773928327716,\n",
      "    \"mean_squared_error\": -0.018135400003668735,\n",
      "    \"mean_absolute_error\": -0.042679801634579725,\n",
      "    \"r2\": 0.9979783334153112,\n",
      "    \"pearsonr\": 0.9989945130902463,\n",
      "    \"median_absolute_error\": -0.01439615123255611\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels64\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.25 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 689\n",
      "Label Column: 689\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.9609145423, -12.3460255307, -1.38208, 3.05922)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60878.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.55 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 374 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 374 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t374 features in original data used to generate 374 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.24 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.602789\n",
      "[2000]\tvalid_set's rmse: 0.59867\n",
      "[3000]\tvalid_set's rmse: 0.598128\n",
      "[4000]\tvalid_set's rmse: 0.597928\n",
      "[5000]\tvalid_set's rmse: 0.597897\n",
      "[6000]\tvalid_set's rmse: 0.597855\n",
      "[7000]\tvalid_set's rmse: 0.59785\n",
      "[8000]\tvalid_set's rmse: 0.597848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5978\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.78s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.83s of the 162.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.566074\n",
      "[2000]\tvalid_set's rmse: 0.564863\n",
      "[3000]\tvalid_set's rmse: 0.56456\n",
      "[4000]\tvalid_set's rmse: 0.564524\n",
      "[5000]\tvalid_set's rmse: 0.564516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5645\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.48s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 148.72s of the 148.72s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9585.\n",
      "\t-0.5266\t = Validation score   (-root_mean_squared_error)\n",
      "\t148.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the -0.94s of remaining time.\n",
      "\t-0.526\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels64\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16637639247569058\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16637639247569058,\n",
      "    \"mean_squared_error\": -0.027681103973225006,\n",
      "    \"mean_absolute_error\": -0.042310045713262204,\n",
      "    \"r2\": 0.9970419633316105,\n",
      "    \"pearsonr\": 0.9985208123809776,\n",
      "    \"median_absolute_error\": -0.0019968108541382756\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels65\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.17 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 690\n",
      "Label Column: 690\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.3992148488, -12.1115323208, 3.50769, 3.58377)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60838.5 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.63 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 375 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 375 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t375 features in original data used to generate 375 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.32 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.16631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1641\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.09s of the 176.09s of remaining time.\n",
      "\t-1.1315\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 173.91s of the 173.91s of remaining time.\n",
      "\t-1.0759\t = Validation score   (-root_mean_squared_error)\n",
      "\t60.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 113.16s of the 113.15s of remaining time.\n",
      "\t-1.1513\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 100.03s of the 100.02s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.15984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1598\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the 77.61s of remaining time.\n",
      "\t-1.0758\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 102.97s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels65\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.3890317742695205\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.3890317742695205,\n",
      "    \"mean_squared_error\": -0.1513457213912917,\n",
      "    \"mean_absolute_error\": -0.21853812350675117,\n",
      "    \"r2\": 0.9882149476284025,\n",
      "    \"pearsonr\": 0.9941810912119857,\n",
      "    \"median_absolute_error\": -0.1302645707192136\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels66\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.12 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 691\n",
      "Label Column: 691\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.2438306711, -12.9628735338, -1.47694, 2.95277)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60823.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.71 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 376 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 376 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t376 features in original data used to generate 376 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.4 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.526702\n",
      "[2000]\tvalid_set's rmse: 0.520977\n",
      "[3000]\tvalid_set's rmse: 0.520311\n",
      "[4000]\tvalid_set's rmse: 0.520146\n",
      "[5000]\tvalid_set's rmse: 0.520135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5201\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.33s of the 168.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.515564\n",
      "[2000]\tvalid_set's rmse: 0.514566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5145\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.21s of the 162.21s of remaining time.\n",
      "\t-0.468\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 25.77s of the 25.77s of remaining time.\n",
      "\t-0.5304\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.08s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 9.56s of the 9.56s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 516. Best iteration is:\n",
      "\t[516]\tvalid_set's rmse: 0.541318\n",
      "\t-0.5413\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the -0.32s of remaining time.\n",
      "\t-0.4677\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels66\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1484754861568276\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1484754861568276,\n",
      "    \"mean_squared_error\": -0.022044969989506353,\n",
      "    \"mean_absolute_error\": -0.04487471638529751,\n",
      "    \"r2\": 0.9974713380850176,\n",
      "    \"pearsonr\": 0.9987364976291186,\n",
      "    \"median_absolute_error\": -0.007735801400244102\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels67\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   63.05 GB / 2000.36 GB (3.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 692\n",
      "Label Column: 692\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.7196984432, -14.4702425791, 3.42648, 3.56738)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60986.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.8 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 377 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 377 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t377 features in original data used to generate 377 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.49 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.788011\n",
      "[2000]\tvalid_set's rmse: 0.782693\n",
      "[3000]\tvalid_set's rmse: 0.781938\n",
      "[4000]\tvalid_set's rmse: 0.781491\n",
      "[5000]\tvalid_set's rmse: 0.78148\n",
      "[6000]\tvalid_set's rmse: 0.781472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7815\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.67s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.14s of the 166.13s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.72313\n",
      "[2000]\tvalid_set's rmse: 0.720831\n",
      "[3000]\tvalid_set's rmse: 0.720328\n",
      "[4000]\tvalid_set's rmse: 0.720283\n",
      "[5000]\tvalid_set's rmse: 0.720269\n",
      "[6000]\tvalid_set's rmse: 0.720266\n",
      "[7000]\tvalid_set's rmse: 0.720266\n",
      "[8000]\tvalid_set's rmse: 0.720266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7203\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.36s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 146.67s of the 146.67s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9488.\n",
      "\t-0.6804\t = Validation score   (-root_mean_squared_error)\n",
      "\t146.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the -0.67s of remaining time.\n",
      "\t-0.6796\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.24s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels67\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21498481773531683\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21498481773531683,\n",
      "    \"mean_squared_error\": -0.04621847185668737,\n",
      "    \"mean_absolute_error\": -0.05424518083924314,\n",
      "    \"r2\": 0.9963678976527505,\n",
      "    \"pearsonr\": 0.9981854551801637,\n",
      "    \"median_absolute_error\": -0.0035181201755123404\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels68\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.97 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 693\n",
      "Label Column: 693\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.2461025533, -13.1458278766, -1.7068, 2.92526)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60930.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.88 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 378 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 378 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t378 features in original data used to generate 378 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.57 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.559111\n",
      "[2000]\tvalid_set's rmse: 0.553789\n",
      "[3000]\tvalid_set's rmse: 0.553196\n",
      "[4000]\tvalid_set's rmse: 0.553175\n",
      "[5000]\tvalid_set's rmse: 0.553116\n",
      "[6000]\tvalid_set's rmse: 0.553084\n",
      "[7000]\tvalid_set's rmse: 0.553079\n",
      "[8000]\tvalid_set's rmse: 0.553081\n",
      "[9000]\tvalid_set's rmse: 0.55308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5531\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.93s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.64s of the 161.64s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.537614\n",
      "[2000]\tvalid_set's rmse: 0.536588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5366\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.36s of the 156.36s of remaining time.\n",
      "\t-0.4935\t = Validation score   (-root_mean_squared_error)\n",
      "\t104.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 51.3s of the 51.3s of remaining time.\n",
      "\t-0.5733\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.29s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 22.51s of the 22.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.583072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1327. Best iteration is:\n",
      "\t[1271]\tvalid_set's rmse: 0.583\n",
      "\t-0.583\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the -0.91s of remaining time.\n",
      "\t-0.493\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels68\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1584737269771918\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1584737269771918,\n",
      "    \"mean_squared_error\": -0.02511392214204165,\n",
      "    \"mean_absolute_error\": -0.057443598704353865,\n",
      "    \"r2\": 0.9970648705371847,\n",
      "    \"pearsonr\": 0.9985343521398672,\n",
      "    \"median_absolute_error\": -0.018430194665002286\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels69\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.89 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 694\n",
      "Label Column: 694\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.3185218726, -14.183344536, 3.35779, 3.46763)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60702.7 MB\n",
      "\tTrain Data (Original)  Memory Usage: 57.96 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 379 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 379 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t379 features in original data used to generate 379 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.65 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.707188\n",
      "[2000]\tvalid_set's rmse: 0.700229\n",
      "[3000]\tvalid_set's rmse: 0.699183\n",
      "[4000]\tvalid_set's rmse: 0.698814\n",
      "[5000]\tvalid_set's rmse: 0.698762\n",
      "[6000]\tvalid_set's rmse: 0.698748\n",
      "[7000]\tvalid_set's rmse: 0.698744\n",
      "[8000]\tvalid_set's rmse: 0.698736\n",
      "[9000]\tvalid_set's rmse: 0.698733\n",
      "[10000]\tvalid_set's rmse: 0.698733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6987\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.18s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.18s of the 160.18s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.651695\n",
      "[2000]\tvalid_set's rmse: 0.650009\n",
      "[3000]\tvalid_set's rmse: 0.649585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6496\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.54s of the 150.54s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9550.\n",
      "\t-0.5959\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -0.66s of remaining time.\n",
      "\t-0.5957\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels69\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18871253565210597\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18871253565210597,\n",
      "    \"mean_squared_error\": -0.03561242111224735,\n",
      "    \"mean_absolute_error\": -0.051757460856129055,\n",
      "    \"r2\": 0.997038044109813,\n",
      "    \"pearsonr\": 0.9985202018862712,\n",
      "    \"median_absolute_error\": -0.006443431258325472\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels70\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.81 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 695\n",
      "Label Column: 695\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.0278483999, -15.2266995215, -2.16339, 3.14281)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60850.9 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.05 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 380 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 380 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t380 features in original data used to generate 380 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.74 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.664495\n",
      "[2000]\tvalid_set's rmse: 0.658824\n",
      "[3000]\tvalid_set's rmse: 0.658015\n",
      "[4000]\tvalid_set's rmse: 0.65758\n",
      "[5000]\tvalid_set's rmse: 0.657485\n",
      "[6000]\tvalid_set's rmse: 0.65749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6575\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 165.25s of the 165.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.628185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.628\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.95s of the 160.95s of remaining time.\n",
      "\t-0.5883\t = Validation score   (-root_mean_squared_error)\n",
      "\t160.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 0.04s of the 0.04s of remaining time.\n",
      "\t-3.6968\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the -0.39s of remaining time.\n",
      "\t-0.5873\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.02s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels70\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18606304373742466\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18606304373742466,\n",
      "    \"mean_squared_error\": -0.03461945624483474,\n",
      "    \"mean_absolute_error\": -0.05311708957015388,\n",
      "    \"r2\": 0.9964946993032083,\n",
      "    \"pearsonr\": 0.9982488460366981,\n",
      "    \"median_absolute_error\": -0.007743153650439538\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels71\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.75 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 696\n",
      "Label Column: 696\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.8611481735, -14.5267936623, 3.09061, 3.35341)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60967.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.13 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 381 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 381 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t381 features in original data used to generate 381 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.82 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.646542\n",
      "[2000]\tvalid_set's rmse: 0.640296\n",
      "[3000]\tvalid_set's rmse: 0.6398\n",
      "[4000]\tvalid_set's rmse: 0.63978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6397\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.25s of the 171.24s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.61667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6158\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.0s of the 168.0s of remaining time.\n",
      "\t-0.554\t = Validation score   (-root_mean_squared_error)\n",
      "\t162.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 5.45s of the 5.45s of remaining time.\n",
      "\t-0.6481\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the -0.18s of remaining time.\n",
      "\t-0.5539\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.82s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels71\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17528883152538416\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17528883152538416,\n",
      "    \"mean_squared_error\": -0.030726174457534575,\n",
      "    \"mean_absolute_error\": -0.046870235618578844,\n",
      "    \"r2\": 0.9972674019401633,\n",
      "    \"pearsonr\": 0.9986337193414041,\n",
      "    \"median_absolute_error\": -0.004473603389605785\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels72\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.69 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 697\n",
      "Label Column: 697\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.5996449543, -17.1972637633, -2.15146, 3.33303)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60913.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.21 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 382 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 382 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t382 features in original data used to generate 382 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.9 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.708051\n",
      "[2000]\tvalid_set's rmse: 0.702527\n",
      "[3000]\tvalid_set's rmse: 0.701249\n",
      "[4000]\tvalid_set's rmse: 0.701048\n",
      "[5000]\tvalid_set's rmse: 0.701068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.701\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.46s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.4s of the 168.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.67456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6736\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.02s of the 164.02s of remaining time.\n",
      "\t-0.6236\t = Validation score   (-root_mean_squared_error)\n",
      "\t140.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 23.05s of the 23.05s of remaining time.\n",
      "\t-0.7177\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 8.94s of the 8.94s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 463. Best iteration is:\n",
      "\t[463]\tvalid_set's rmse: 0.722306\n",
      "\t-0.7223\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the -0.29s of remaining time.\n",
      "\t-0.6234\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels72\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19781560559454878\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19781560559454878,\n",
      "    \"mean_squared_error\": -0.03913101381673788,\n",
      "    \"mean_absolute_error\": -0.058193914565490557,\n",
      "    \"r2\": 0.9964772350951745,\n",
      "    \"pearsonr\": 0.9982402451540521,\n",
      "    \"median_absolute_error\": -0.010262374927783036\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels73\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.63 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 698\n",
      "Label Column: 698\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.8460486413, -15.172768076, 2.82718, 3.35895)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61046.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.3 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 383 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 383 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t383 features in original data used to generate 383 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 31.99 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.747151\n",
      "[2000]\tvalid_set's rmse: 0.74004\n",
      "[3000]\tvalid_set's rmse: 0.739183\n",
      "[4000]\tvalid_set's rmse: 0.739131\n",
      "[5000]\tvalid_set's rmse: 0.739045\n",
      "[6000]\tvalid_set's rmse: 0.739015\n",
      "[7000]\tvalid_set's rmse: 0.739035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.739\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.96s of the 164.96s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.691256\n",
      "[2000]\tvalid_set's rmse: 0.690203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6901\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 158.55s of the 158.55s of remaining time.\n",
      "\t-0.6579\t = Validation score   (-root_mean_squared_error)\n",
      "\t85.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 72.26s of the 72.26s of remaining time.\n",
      "\t-0.7155\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.2s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 56.93s of the 56.93s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.717518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7174\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the 24.5s of remaining time.\n",
      "\t-0.6555\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 156.06s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels73\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21297426599296013\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21297426599296013,\n",
      "    \"mean_squared_error\": -0.04535803797524001,\n",
      "    \"mean_absolute_error\": -0.08500741285014812,\n",
      "    \"r2\": 0.9959794173364057,\n",
      "    \"pearsonr\": 0.9979953245207374,\n",
      "    \"median_absolute_error\": -0.032355738563678016\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels74\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.54 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 699\n",
      "Label Column: 699\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.2694855704, -16.4167010807, -1.61261, 3.52914)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60988.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.38 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 384 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 384 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t384 features in original data used to generate 384 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.07 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.822005\n",
      "[2000]\tvalid_set's rmse: 0.814874\n",
      "[3000]\tvalid_set's rmse: 0.814094\n",
      "[4000]\tvalid_set's rmse: 0.813947\n",
      "[5000]\tvalid_set's rmse: 0.813789\n",
      "[6000]\tvalid_set's rmse: 0.813791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8138\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.35s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 165.48s of the 165.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.75999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7592\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 161.32s of the 161.31s of remaining time.\n",
      "\t-0.7211\t = Validation score   (-root_mean_squared_error)\n",
      "\t49.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 111.94s of the 111.94s of remaining time.\n",
      "\t-0.81\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 94.66s of the 94.66s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.796431\n",
      "[2000]\tvalid_set's rmse: 0.796337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7963\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the 47.79s of remaining time.\n",
      "\t-0.7197\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 132.78s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels74\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.27999944568799434\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.27999944568799434,\n",
      "    \"mean_squared_error\": -0.07839968958558403,\n",
      "    \"mean_absolute_error\": -0.1736295793030707,\n",
      "    \"r2\": 0.9937046607224298,\n",
      "    \"pearsonr\": 0.9968952232402819,\n",
      "    \"median_absolute_error\": -0.11593895414302369\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels75\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.46 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 700\n",
      "Label Column: 700\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.8049160198, -17.734483957, 1.85252, 3.42164)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60792.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.46 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 385 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 385 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t385 features in original data used to generate 385 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.16 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.655295\n",
      "[2000]\tvalid_set's rmse: 0.648319\n",
      "[3000]\tvalid_set's rmse: 0.647848\n",
      "[4000]\tvalid_set's rmse: 0.647597\n",
      "[5000]\tvalid_set's rmse: 0.647556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6475\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.52s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.35s of the 168.35s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.596389\n",
      "[2000]\tvalid_set's rmse: 0.595196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5951\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.21s of the 162.21s of remaining time.\n",
      "\t-0.5442\t = Validation score   (-root_mean_squared_error)\n",
      "\t92.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 69.45s of the 69.45s of remaining time.\n",
      "\t-0.6209\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 56.32s of the 56.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.627145\n",
      "[2000]\tvalid_set's rmse: 0.627014\n",
      "[3000]\tvalid_set's rmse: 0.627014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 3298. Best iteration is:\n",
      "\t[3296]\tvalid_set's rmse: 0.627014\n",
      "\t-0.627\t = Validation score   (-root_mean_squared_error)\n",
      "\t56.79s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -2.04s of remaining time.\n",
      "\t-0.5439\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.64s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels75\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1769890922132086\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1769890922132086,\n",
      "    \"mean_squared_error\": -0.03132513876245589,\n",
      "    \"mean_absolute_error\": -0.07090665709645373,\n",
      "    \"r2\": 0.9973241221774768,\n",
      "    \"pearsonr\": 0.9986639367426069,\n",
      "    \"median_absolute_error\": -0.027405829127954195\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels76\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.36 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 701\n",
      "Label Column: 701\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (21.4457968966, -19.1027646088, 0.23467, 3.87377)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60987.31 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.55 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 386 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 386 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t386 features in original data used to generate 386 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.24 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.56257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.5612\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.18s of the 176.18s of remaining time.\n",
      "\t-1.5379\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.82s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 174.33s of the 174.33s of remaining time.\n",
      "\t-1.5141\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.56s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 146.36s of the 146.36s of remaining time.\n",
      "\t-1.5617\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.39s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 134.84s of the 134.83s of remaining time.\n",
      "\t-1.5649\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the 117.91s of remaining time.\n",
      "\t-1.5075\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 62.7s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels76\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.7462290306771533\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.7462290306771533,\n",
      "    \"mean_squared_error\": -0.5568577662253638,\n",
      "    \"mean_absolute_error\": -0.5224425763454221,\n",
      "    \"r2\": 0.9628876856820521,\n",
      "    \"pearsonr\": 0.9820871248476484,\n",
      "    \"median_absolute_error\": -0.3793994867467896\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels77\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.31 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 702\n",
      "Label Column: 702\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (21.8900048523, -16.6893813387, 1.13169, 3.36703)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61061.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.63 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 387 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 387 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t387 features in original data used to generate 387 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.32 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.907573\n",
      "[2000]\tvalid_set's rmse: 0.904963\n",
      "[3000]\tvalid_set's rmse: 0.904266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9042\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.52s of the 171.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.865602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8651\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.61s of the 168.61s of remaining time.\n",
      "\t-0.836\t = Validation score   (-root_mean_squared_error)\n",
      "\t71.66s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 96.43s of the 96.43s of remaining time.\n",
      "\t-0.9118\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.69s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 83.25s of the 83.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.895805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8957\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the 54.89s of remaining time.\n",
      "\t-0.8311\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 125.68s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels77\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.28569914533664453\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.28569914533664453,\n",
      "    \"mean_squared_error\": -0.0816240016460889,\n",
      "    \"mean_absolute_error\": -0.14196048220690832,\n",
      "    \"r2\": 0.9927994370744537,\n",
      "    \"pearsonr\": 0.9964267779488971,\n",
      "    \"median_absolute_error\": -0.07656932810948502\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels78\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.24 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 703\n",
      "Label Column: 703\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.0937377887, -13.8995262885, 1.25149, 2.95212)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61202.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.71 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 388 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 388 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t388 features in original data used to generate 388 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.742474\n",
      "[2000]\tvalid_set's rmse: 0.737095\n",
      "[3000]\tvalid_set's rmse: 0.736433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7362\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.83s of the 172.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.738288\n",
      "[2000]\tvalid_set's rmse: 0.736416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7364\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 166.35s of the 166.35s of remaining time.\n",
      "\t-0.6921\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 110.44s of the 110.44s of remaining time.\n",
      "\t-0.7632\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.14s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 90.16s of the 90.16s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.785806\n",
      "[2000]\tvalid_set's rmse: 0.78565\n",
      "[3000]\tvalid_set's rmse: 0.785648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7856\t = Validation score   (-root_mean_squared_error)\n",
      "\t60.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the 28.19s of remaining time.\n",
      "\t-0.6896\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 152.43s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels78\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24075032344674072\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24075032344674072,\n",
      "    \"mean_squared_error\": -0.057960718239710066,\n",
      "    \"mean_absolute_error\": -0.1253550960432059,\n",
      "    \"r2\": 0.9933486988871773,\n",
      "    \"pearsonr\": 0.9966907898975303,\n",
      "    \"median_absolute_error\": -0.06920037423156128\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels79\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.15 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 704\n",
      "Label Column: 704\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.7525608652, -13.1527447348, 0.62892, 2.96057)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61207.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.8 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 389 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 389 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t389 features in original data used to generate 389 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.49 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.703495\n",
      "[2000]\tvalid_set's rmse: 0.699032\n",
      "[3000]\tvalid_set's rmse: 0.698779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6986\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.09s of the 173.09s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.65063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6498\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.85s of the 168.85s of remaining time.\n",
      "\t-0.6238\t = Validation score   (-root_mean_squared_error)\n",
      "\t125.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 42.81s of the 42.81s of remaining time.\n",
      "\t-0.6849\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 22.99s of the 22.99s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.706536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1292. Best iteration is:\n",
      "\t[1200]\tvalid_set's rmse: 0.70649\n",
      "\t-0.7065\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -0.68s of remaining time.\n",
      "\t-0.6205\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.28s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels79\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19862534224625766\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19862534224625766,\n",
      "    \"mean_squared_error\": -0.039452026582442956,\n",
      "    \"mean_absolute_error\": -0.06918393353061207,\n",
      "    \"r2\": 0.995498462402272,\n",
      "    \"pearsonr\": 0.9977513965342724,\n",
      "    \"median_absolute_error\": -0.020453385326229923\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels80\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.08 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 705\n",
      "Label Column: 705\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.1678683421, -16.5574296163, 0.12279, 3.24778)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61016.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.88 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 390 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 390 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t390 features in original data used to generate 390 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.57 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.618249\n",
      "[2000]\tvalid_set's rmse: 0.613585\n",
      "[3000]\tvalid_set's rmse: 0.613059\n",
      "[4000]\tvalid_set's rmse: 0.612917\n",
      "[5000]\tvalid_set's rmse: 0.612816\n",
      "[6000]\tvalid_set's rmse: 0.612822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6128\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.68s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.15s of the 167.15s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.591517\n",
      "[2000]\tvalid_set's rmse: 0.590306\n",
      "[3000]\tvalid_set's rmse: 0.590102\n",
      "[4000]\tvalid_set's rmse: 0.590084\n",
      "[5000]\tvalid_set's rmse: 0.590083\n",
      "[6000]\tvalid_set's rmse: 0.590083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5901\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.95s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.43s of the 150.43s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7143.\n",
      "\t-0.5312\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.22s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the -1.2s of remaining time.\n",
      "\t-0.5311\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.99s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels80\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16894602241293527\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16894602241293527,\n",
      "    \"mean_squared_error\": -0.028542758489152036,\n",
      "    \"mean_absolute_error\": -0.0524647513175464,\n",
      "    \"r2\": 0.9972937770288823,\n",
      "    \"pearsonr\": 0.9986469907763499,\n",
      "    \"median_absolute_error\": -0.010980799749651937\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels81\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   62.01 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 706\n",
      "Label Column: 706\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.1844265313, -14.9974395483, 0.38889, 3.3947)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    55923.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 58.97 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 391 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 391 | ['83', '84', '85', '86', '87', ...]\n",
      "\t1.9s = Fit runtime\n",
      "\t391 features in original data used to generate 391 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.66 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.48s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 177.52s of the 177.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.625573\n",
      "[2000]\tvalid_set's rmse: 0.620873\n",
      "[3000]\tvalid_set's rmse: 0.620324\n",
      "[4000]\tvalid_set's rmse: 0.619911\n",
      "[5000]\tvalid_set's rmse: 0.619857\n",
      "[6000]\tvalid_set's rmse: 0.619823\n",
      "[7000]\tvalid_set's rmse: 0.619804\n",
      "[8000]\tvalid_set's rmse: 0.619804\n",
      "[9000]\tvalid_set's rmse: 0.619803\n",
      "[10000]\tvalid_set's rmse: 0.619803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6198\t = Validation score   (-root_mean_squared_error)\n",
      "\t96.14s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 80.05s of the 80.04s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.600365\n",
      "[2000]\tvalid_set's rmse: 0.598567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5985\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.92s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 62.52s of the 62.52s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 3263.\n",
      "\t-0.5729\t = Validation score   (-root_mean_squared_error)\n",
      "\t62.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 177.52s of the -0.73s of remaining time.\n",
      "\t-0.5685\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels81\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19380191093681523\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19380191093681523,\n",
      "    \"mean_squared_error\": -0.03755918068276136,\n",
      "    \"mean_absolute_error\": -0.0953425911176408,\n",
      "    \"r2\": 0.9967404757082897,\n",
      "    \"pearsonr\": 0.9983826587681821,\n",
      "    \"median_absolute_error\": -0.04935441206226798\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels82\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.94 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 707\n",
      "Label Column: 707\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.9151075578, -16.2504186973, 1.05269, 3.50514)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    59622.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.05 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 392 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 392 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t392 features in original data used to generate 392 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.74 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.36s of the 179.35s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.652717\n",
      "[2000]\tvalid_set's rmse: 0.647283\n",
      "[3000]\tvalid_set's rmse: 0.646488\n",
      "[4000]\tvalid_set's rmse: 0.646509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6465\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.08s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.72s of the 167.72s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.617595\n",
      "[2000]\tvalid_set's rmse: 0.616016\n",
      "[3000]\tvalid_set's rmse: 0.615658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6156\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 157.0s of the 156.99s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9463.\n",
      "\t-0.5718\t = Validation score   (-root_mean_squared_error)\n",
      "\t157.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.36s of the -0.82s of remaining time.\n",
      "\t-0.5699\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.44s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels82\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.180272848676935\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.180272848676935,\n",
      "    \"mean_squared_error\": -0.032498299970097004,\n",
      "    \"mean_absolute_error\": -0.04582578522724848,\n",
      "    \"r2\": 0.9973546032884937,\n",
      "    \"pearsonr\": 0.9986792356140666,\n",
      "    \"median_absolute_error\": -0.002473552290753245\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels83\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.88 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 708\n",
      "Label Column: 708\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.4799516249, -16.5623283027, 1.76242, 3.55132)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60715.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.13 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 393 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 393 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t393 features in original data used to generate 393 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.82 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.761251\n",
      "[2000]\tvalid_set's rmse: 0.755584\n",
      "[3000]\tvalid_set's rmse: 0.754137\n",
      "[4000]\tvalid_set's rmse: 0.753991\n",
      "[5000]\tvalid_set's rmse: 0.753958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7539\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.6s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.38s of the 168.38s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.696123\n",
      "[2000]\tvalid_set's rmse: 0.695388\n",
      "[3000]\tvalid_set's rmse: 0.69527\n",
      "[4000]\tvalid_set's rmse: 0.695243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6952\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.95s of the 156.95s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9642.\n",
      "\t-0.6554\t = Validation score   (-root_mean_squared_error)\n",
      "\t157.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the -0.8s of remaining time.\n",
      "\t-0.6542\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.39s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels83\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.206934148047738\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.206934148047738,\n",
      "    \"mean_squared_error\": -0.042821741628242885,\n",
      "    \"mean_absolute_error\": -0.051673258022631216,\n",
      "    \"r2\": 0.9966043141703073,\n",
      "    \"pearsonr\": 0.9983017427697037,\n",
      "    \"median_absolute_error\": -0.00232907240289304\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels84\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.81 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 709\n",
      "Label Column: 709\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.6628854357, -12.8503595479, -0.13177, 3.37268)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60958.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.22 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 394 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 394 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t394 features in original data used to generate 394 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.91 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.51s of the 179.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.9121\n",
      "[2000]\tvalid_set's rmse: 0.90645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9059\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.69s of the 173.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.812063\n",
      "[2000]\tvalid_set's rmse: 0.811344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8113\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.84s of the 167.84s of remaining time.\n",
      "\t-0.7956\t = Validation score   (-root_mean_squared_error)\n",
      "\t66.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 101.02s of the 101.02s of remaining time.\n",
      "\t-0.8428\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.54s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 79.94s of the 79.94s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.843534\n",
      "[2000]\tvalid_set's rmse: 0.843424\n",
      "[3000]\tvalid_set's rmse: 0.843423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8434\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.08s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.51s of the 23.83s of remaining time.\n",
      "\t-0.7895\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 156.83s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels84\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2646778640070723\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2646778640070723,\n",
      "    \"mean_squared_error\": -0.07005437169534608,\n",
      "    \"mean_absolute_error\": -0.12303828615565106,\n",
      "    \"r2\": 0.9938407623860603,\n",
      "    \"pearsonr\": 0.9969308368711962,\n",
      "    \"median_absolute_error\": -0.06008837647164311\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels85\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.73 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 710\n",
      "Label Column: 710\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.9645690727, -15.5297960808, -0.62061, 3.13733)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60679.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.3 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 395 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 395 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t395 features in original data used to generate 395 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 32.99 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.660894\n",
      "[2000]\tvalid_set's rmse: 0.654885\n",
      "[3000]\tvalid_set's rmse: 0.653541\n",
      "[4000]\tvalid_set's rmse: 0.653215\n",
      "[5000]\tvalid_set's rmse: 0.653174\n",
      "[6000]\tvalid_set's rmse: 0.653148\n",
      "[7000]\tvalid_set's rmse: 0.653146\n",
      "[8000]\tvalid_set's rmse: 0.653145\n",
      "[9000]\tvalid_set's rmse: 0.653143\n",
      "[10000]\tvalid_set's rmse: 0.653143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6531\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.27s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.02s of the 159.02s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.61693\n",
      "[2000]\tvalid_set's rmse: 0.615596\n",
      "[3000]\tvalid_set's rmse: 0.615405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6154\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.47s of the 150.46s of remaining time.\n",
      "\t-0.5868\t = Validation score   (-root_mean_squared_error)\n",
      "\t104.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 44.88s of the 44.88s of remaining time.\n",
      "\t-0.6489\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.5s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 27.96s of the 27.96s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.664746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1563. Best iteration is:\n",
      "\t[1451]\tvalid_set's rmse: 0.664692\n",
      "\t-0.6647\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -0.96s of remaining time.\n",
      "\t-0.5833\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.6s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels85\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18656783985558467\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18656783985558467,\n",
      "    \"mean_squared_error\": -0.034807558868379086,\n",
      "    \"mean_absolute_error\": -0.06317050780203388,\n",
      "    \"r2\": 0.996463335048433,\n",
      "    \"pearsonr\": 0.9982306164772388,\n",
      "    \"median_absolute_error\": -0.017760496614099175\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels86\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.63 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 711\n",
      "Label Column: 711\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.1620119798, -18.8862127171, -0.82192, 3.0892)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60883.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.38 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 396 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 396 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t396 features in original data used to generate 396 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.07 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.603375\n",
      "[2000]\tvalid_set's rmse: 0.600872\n",
      "[3000]\tvalid_set's rmse: 0.600849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6006\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.04s of the 172.04s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.58763\n",
      "[2000]\tvalid_set's rmse: 0.586266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5861\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 166.66s of the 166.66s of remaining time.\n",
      "\t-0.5423\t = Validation score   (-root_mean_squared_error)\n",
      "\t72.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 93.74s of the 93.73s of remaining time.\n",
      "\t-0.6175\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 78.67s of the 78.67s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.619963\n",
      "[2000]\tvalid_set's rmse: 0.619907\n",
      "[3000]\tvalid_set's rmse: 0.619905\n",
      "[4000]\tvalid_set's rmse: 0.619905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4470. Best iteration is:\n",
      "\t[3478]\tvalid_set's rmse: 0.619905\n",
      "\t-0.6199\t = Validation score   (-root_mean_squared_error)\n",
      "\t79.13s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the -1.95s of remaining time.\n",
      "\t-0.5415\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.57s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels86\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18033272188763125\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18033272188763125,\n",
      "    \"mean_squared_error\": -0.03251989058340188,\n",
      "    \"mean_absolute_error\": -0.08045713033381534,\n",
      "    \"r2\": 0.9965920033063173,\n",
      "    \"pearsonr\": 0.9982996035655145,\n",
      "    \"median_absolute_error\": -0.03785010885355222\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels87\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.53 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 712\n",
      "Label Column: 712\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.7292499132, -17.4039107913, -0.69712, 3.1382)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60713.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 397 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 397 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t397 features in original data used to generate 397 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.16 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.46s of the 179.45s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.594947\n",
      "[2000]\tvalid_set's rmse: 0.59036\n",
      "[3000]\tvalid_set's rmse: 0.589262\n",
      "[4000]\tvalid_set's rmse: 0.589084\n",
      "[5000]\tvalid_set's rmse: 0.589037\n",
      "[6000]\tvalid_set's rmse: 0.588997\n",
      "[7000]\tvalid_set's rmse: 0.588988\n",
      "[8000]\tvalid_set's rmse: 0.588989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.589\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.96s of the 161.96s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.543748\n",
      "[2000]\tvalid_set's rmse: 0.543134\n",
      "[3000]\tvalid_set's rmse: 0.543036\n",
      "[4000]\tvalid_set's rmse: 0.543027\n",
      "[5000]\tvalid_set's rmse: 0.543023\n",
      "[6000]\tvalid_set's rmse: 0.543022\n",
      "[7000]\tvalid_set's rmse: 0.543022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.543\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 143.23s of the 143.23s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8776.\n",
      "\t-0.4978\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.46s of the -0.64s of remaining time.\n",
      "\t-0.4971\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.24s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels87\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15750843295325712\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15750843295325712,\n",
      "    \"mean_squared_error\": -0.024808906451390833,\n",
      "    \"mean_absolute_error\": -0.0438406209739774,\n",
      "    \"r2\": 0.9974806586751088,\n",
      "    \"pearsonr\": 0.9987411035419858,\n",
      "    \"median_absolute_error\": -0.005896669254711773\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels88\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.45 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 713\n",
      "Label Column: 713\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.8931930201, -14.0189257308, -0.30327, 3.00684)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60747.8 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.55 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 398 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 398 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t398 features in original data used to generate 398 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.24 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.527669\n",
      "[2000]\tvalid_set's rmse: 0.522495\n",
      "[3000]\tvalid_set's rmse: 0.52176\n",
      "[4000]\tvalid_set's rmse: 0.521718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5217\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.94s of the 169.93s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.464013\n",
      "[2000]\tvalid_set's rmse: 0.462963\n",
      "[3000]\tvalid_set's rmse: 0.462883\n",
      "[4000]\tvalid_set's rmse: 0.462843\n",
      "[5000]\tvalid_set's rmse: 0.462838\n",
      "[6000]\tvalid_set's rmse: 0.462839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4628\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.54s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 151.78s of the 151.78s of remaining time.\n",
      "\t-0.4471\t = Validation score   (-root_mean_squared_error)\n",
      "\t70.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 81.01s of the 81.0s of remaining time.\n",
      "\t-0.4964\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 66.74s of the 66.74s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.496293\n",
      "[2000]\tvalid_set's rmse: 0.496222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4962\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the 29.59s of remaining time.\n",
      "\t-0.4416\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 151.0s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels88\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14609902045361134\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14609902045361134,\n",
      "    \"mean_squared_error\": -0.02134492377750459,\n",
      "    \"mean_absolute_error\": -0.0633760184517088,\n",
      "    \"r2\": 0.9976388915765099,\n",
      "    \"pearsonr\": 0.9988237960344295,\n",
      "    \"median_absolute_error\": -0.0297808782914184\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels89\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.37 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 714\n",
      "Label Column: 714\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.8120517235, -13.68975357, 0.3101, 2.71508)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60690.73 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.63 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 399 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 399 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t399 features in original data used to generate 399 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.32 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.563873\n",
      "[2000]\tvalid_set's rmse: 0.557753\n",
      "[3000]\tvalid_set's rmse: 0.557396\n",
      "[4000]\tvalid_set's rmse: 0.557371\n",
      "[5000]\tvalid_set's rmse: 0.557373\n",
      "[6000]\tvalid_set's rmse: 0.557376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5573\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.03s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.88s of the 166.87s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.516081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5156\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.29s of the 162.28s of remaining time.\n",
      "\t-0.4847\t = Validation score   (-root_mean_squared_error)\n",
      "\t87.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 74.35s of the 74.35s of remaining time.\n",
      "\t-0.5392\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 57.44s of the 57.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.541722\n",
      "[2000]\tvalid_set's rmse: 0.541679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5417\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the 20.09s of remaining time.\n",
      "\t-0.4834\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 160.55s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels89\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15776337387224382\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15776337387224382,\n",
      "    \"mean_squared_error\": -0.024889282135553387,\n",
      "    \"mean_absolute_error\": -0.06265839251712355,\n",
      "    \"r2\": 0.9966233338431091,\n",
      "    \"pearsonr\": 0.9983167884659958,\n",
      "    \"median_absolute_error\": -0.026065172655560243\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels90\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.29 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 715\n",
      "Label Column: 715\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.676859413, -13.5937352101, 1.0998, 2.65227)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60242.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.72 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 400 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 400 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t400 features in original data used to generate 400 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.604729\n",
      "[2000]\tvalid_set's rmse: 0.600279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5996\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.86s of the 172.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.56936\n",
      "[2000]\tvalid_set's rmse: 0.567857\n",
      "[3000]\tvalid_set's rmse: 0.567763\n",
      "[4000]\tvalid_set's rmse: 0.567716\n",
      "[5000]\tvalid_set's rmse: 0.567713\n",
      "[6000]\tvalid_set's rmse: 0.567711\n",
      "[7000]\tvalid_set's rmse: 0.567711\n",
      "[8000]\tvalid_set's rmse: 0.567711\n",
      "[9000]\tvalid_set's rmse: 0.567711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5677\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.34s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 148.3s of the 148.3s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9254.\n",
      "\t-0.5279\t = Validation score   (-root_mean_squared_error)\n",
      "\t148.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the -0.69s of remaining time.\n",
      "\t-0.5267\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.27s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels90\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16660350464598825\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16660350464598825,\n",
      "    \"mean_squared_error\": -0.02775672776032595,\n",
      "    \"mean_absolute_error\": -0.04215630933768386,\n",
      "    \"r2\": 0.9960538470161595,\n",
      "    \"pearsonr\": 0.9980274916123213,\n",
      "    \"median_absolute_error\": -0.0020302300018859576\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels91\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.21 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 716\n",
      "Label Column: 716\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.1598595059, -14.4828830258, 1.84961, 3.02313)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    59941.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.8 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 401 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 401 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t401 features in original data used to generate 401 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.49 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.737016\n",
      "[2000]\tvalid_set's rmse: 0.731884\n",
      "[3000]\tvalid_set's rmse: 0.731696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7315\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.35s of the 172.35s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.68503\n",
      "[2000]\tvalid_set's rmse: 0.684054\n",
      "[3000]\tvalid_set's rmse: 0.683948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6839\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.16s of the 163.16s of remaining time.\n",
      "\t-0.6456\t = Validation score   (-root_mean_squared_error)\n",
      "\t62.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 99.98s of the 99.98s of remaining time.\n",
      "\t-0.7291\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 86.39s of the 86.39s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.742074\n",
      "[2000]\tvalid_set's rmse: 0.742056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7421\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the 42.37s of remaining time.\n",
      "\t-0.6448\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 138.23s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels91\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2205888701291625\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2205888701291625,\n",
      "    \"mean_squared_error\": -0.04865944962486033,\n",
      "    \"mean_absolute_error\": -0.10915683189220651,\n",
      "    \"r2\": 0.9946753015289699,\n",
      "    \"pearsonr\": 0.9973509879137901,\n",
      "    \"median_absolute_error\": -0.05869270665191967\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels92\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.13 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 717\n",
      "Label Column: 717\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.5731269911, -12.0223788853, -0.52802, 3.38327)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    59476.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.88 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 402 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 402 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t402 features in original data used to generate 402 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.58 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.627224\n",
      "[2000]\tvalid_set's rmse: 0.621304\n",
      "[3000]\tvalid_set's rmse: 0.620947\n",
      "[4000]\tvalid_set's rmse: 0.620895\n",
      "[5000]\tvalid_set's rmse: 0.620888\n",
      "[6000]\tvalid_set's rmse: 0.620864\n",
      "[7000]\tvalid_set's rmse: 0.620849\n",
      "[8000]\tvalid_set's rmse: 0.620847\n",
      "[9000]\tvalid_set's rmse: 0.620847\n",
      "[10000]\tvalid_set's rmse: 0.620847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6208\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.72s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.68s of the 159.68s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.56232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.562\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.38s of the 156.38s of remaining time.\n",
      "\t-0.5393\t = Validation score   (-root_mean_squared_error)\n",
      "\t104.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 51.57s of the 51.57s of remaining time.\n",
      "\t-0.5993\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.5s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 34.67s of the 34.67s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.603196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1909. Best iteration is:\n",
      "\t[1866]\tvalid_set's rmse: 0.603154\n",
      "\t-0.6032\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the -1.11s of remaining time.\n",
      "\t-0.5357\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.73s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels92\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17587237934277966\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17587237934277966,\n",
      "    \"mean_squared_error\": -0.030931093815690713,\n",
      "    \"mean_absolute_error\": -0.07434605164667127,\n",
      "    \"r2\": 0.9972975139602747,\n",
      "    \"pearsonr\": 0.9986536533433285,\n",
      "    \"median_absolute_error\": -0.03325709503277596\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels93\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   61.03 GB / 2000.36 GB (3.1%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 718\n",
      "Label Column: 718\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.141683481, -13.6510109511, 3.21053, 3.47723)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    59113.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 59.97 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 403 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 403 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t403 features in original data used to generate 403 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.66 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.08993\n",
      "[2000]\tvalid_set's rmse: 1.086\n",
      "[3000]\tvalid_set's rmse: 1.0855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0854\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.61s of the 171.61s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.02998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0283\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.0s of the 166.99s of remaining time.\n",
      "\t-0.9997\t = Validation score   (-root_mean_squared_error)\n",
      "\t50.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 116.33s of the 116.33s of remaining time.\n",
      "\t-1.0792\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 102.58s of the 102.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.06927\n",
      "[2000]\tvalid_set's rmse: 1.06911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0691\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.3s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the 56.44s of remaining time.\n",
      "\t-0.9953\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 124.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels93\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.36062421373650894\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.36062421373650894,\n",
      "    \"mean_squared_error\": -0.13004982353307545,\n",
      "    \"mean_absolute_error\": -0.2016576404688106,\n",
      "    \"r2\": 0.9892431577835084,\n",
      "    \"pearsonr\": 0.9947047887563314,\n",
      "    \"median_absolute_error\": -0.1251056001793272\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels94\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.96 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 719\n",
      "Label Column: 719\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.0787450205, -14.8034764465, -1.37398, 3.19665)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    58965.64 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.05 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 404 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 404 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t404 features in original data used to generate 404 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.74 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.99648\n",
      "[2000]\tvalid_set's rmse: 0.990712\n",
      "[3000]\tvalid_set's rmse: 0.990514\n",
      "[4000]\tvalid_set's rmse: 0.990104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9901\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.49s of the 169.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.93894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9386\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 166.14s of the 166.14s of remaining time.\n",
      "\t-0.9196\t = Validation score   (-root_mean_squared_error)\n",
      "\t65.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 100.57s of the 100.57s of remaining time.\n",
      "\t-0.9706\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 83.23s of the 83.23s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.973936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9738\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the 61.66s of remaining time.\n",
      "\t-0.916\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 118.95s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels94\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.3223160331996753\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.3223160331996753,\n",
      "    \"mean_squared_error\": -0.10388762525757429,\n",
      "    \"mean_absolute_error\": -0.17139295946938954,\n",
      "    \"r2\": 0.9898324751153947,\n",
      "    \"pearsonr\": 0.9949699902743863,\n",
      "    \"median_absolute_error\": -0.0989796873697022\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels95\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.89 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 720\n",
      "Label Column: 720\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.0313652081, -10.8068222704, 3.49219, 3.53286)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60869.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.13 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 405 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 405 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t405 features in original data used to generate 405 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.83 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.509777\n",
      "[2000]\tvalid_set's rmse: 0.507636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5074\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 174.7s of the 174.7s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.425184\n",
      "[2000]\tvalid_set's rmse: 0.424708\n",
      "[3000]\tvalid_set's rmse: 0.424566\n",
      "[4000]\tvalid_set's rmse: 0.424552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4245\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.51s of the 163.51s of remaining time.\n",
      "\t-0.3832\t = Validation score   (-root_mean_squared_error)\n",
      "\t84.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 78.38s of the 78.38s of remaining time.\n",
      "\t-0.4458\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.56s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 58.38s of the 58.38s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.475892\n",
      "[2000]\tvalid_set's rmse: 0.475824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4758\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the 11.98s of remaining time.\n",
      "\t-0.3815\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 168.63s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels95\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.12411792469944422\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.12411792469944422,\n",
      "    \"mean_squared_error\": -0.015405259231696928,\n",
      "    \"mean_absolute_error\": -0.04828901098393773,\n",
      "    \"r2\": 0.9987655975601387,\n",
      "    \"pearsonr\": 0.9993842851881345,\n",
      "    \"median_absolute_error\": -0.019696052252466068\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels96\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.81 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 721\n",
      "Label Column: 721\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.515940865, -12.681063796, -1.85949, 3.03776)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60877.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.22 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 406 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 406 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t406 features in original data used to generate 406 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.91 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.589992\n",
      "[2000]\tvalid_set's rmse: 0.587414\n",
      "[3000]\tvalid_set's rmse: 0.586967\n",
      "[4000]\tvalid_set's rmse: 0.586889\n",
      "[5000]\tvalid_set's rmse: 0.586884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5868\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.7s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.26s of the 168.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.569875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5694\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.0s of the 164.0s of remaining time.\n",
      "\t-0.5372\t = Validation score   (-root_mean_squared_error)\n",
      "\t79.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 84.3s of the 84.3s of remaining time.\n",
      "\t-0.605\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 68.1s of the 68.1s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.607279\n",
      "[2000]\tvalid_set's rmse: 0.607168\n",
      "[3000]\tvalid_set's rmse: 0.607166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6072\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.78s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the 11.64s of remaining time.\n",
      "\t-0.5357\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 168.97s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels96\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1788580856473354\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1788580856473354,\n",
      "    \"mean_squared_error\": -0.03199021480142968,\n",
      "    \"mean_absolute_error\": -0.08203949282908526,\n",
      "    \"r2\": 0.996533006299559,\n",
      "    \"pearsonr\": 0.998271289178764,\n",
      "    \"median_absolute_error\": -0.03848630533669739\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels97\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.72 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 722\n",
      "Label Column: 722\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.2268848759, -14.4051163791, 3.81776, 3.97829)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60847.4 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.3 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 407 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 407 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t407 features in original data used to generate 407 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 33.99 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.3374\n",
      "[2000]\tvalid_set's rmse: 1.33231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.3315\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 174.95s of the 174.95s of remaining time.\n",
      "\t-1.2644\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 172.67s of the 172.67s of remaining time.\n",
      "\t-1.223\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.74s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 140.52s of the 140.51s of remaining time.\n",
      "\t-1.282\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 120.18s of the 120.18s of remaining time.\n",
      "\t-1.2994\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the 105.32s of remaining time.\n",
      "\t-1.2217\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 75.27s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels97\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.5779691098461429\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.5779691098461429,\n",
      "    \"mean_squared_error\": -0.33404829193634344,\n",
      "    \"mean_absolute_error\": -0.4107958772568716,\n",
      "    \"r2\": 0.9788914989523032,\n",
      "    \"pearsonr\": 0.9896465589782182,\n",
      "    \"median_absolute_error\": -0.3086062954486357\n",
      "}\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels98\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.67 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 723\n",
      "Label Column: 723\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.4127630467, -13.293536074, -1.90979, 2.85978)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60854.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.39 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 408 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 408 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t408 features in original data used to generate 408 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.08 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.524592\n",
      "[2000]\tvalid_set's rmse: 0.519618\n",
      "[3000]\tvalid_set's rmse: 0.519101\n",
      "[4000]\tvalid_set's rmse: 0.518801\n",
      "[5000]\tvalid_set's rmse: 0.518685\n",
      "[6000]\tvalid_set's rmse: 0.518685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5187\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.79s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.07s of the 166.07s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.483405\n",
      "[2000]\tvalid_set's rmse: 0.481959\n",
      "[3000]\tvalid_set's rmse: 0.481783\n",
      "[4000]\tvalid_set's rmse: 0.481735\n",
      "[5000]\tvalid_set's rmse: 0.481728\n",
      "[6000]\tvalid_set's rmse: 0.481726\n",
      "[7000]\tvalid_set's rmse: 0.481726\n",
      "[8000]\tvalid_set's rmse: 0.481726\n",
      "[9000]\tvalid_set's rmse: 0.481726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4817\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.3s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 143.62s of the 143.62s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8949.\n",
      "\t-0.4575\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -0.67s of remaining time.\n",
      "\t-0.4549\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.24s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels98\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14387604761933587\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14387604761933587,\n",
      "    \"mean_squared_error\": -0.020700317078561452,\n",
      "    \"mean_absolute_error\": -0.037265607418232406,\n",
      "    \"r2\": 0.9974686508218796,\n",
      "    \"pearsonr\": 0.9987346952741752,\n",
      "    \"median_absolute_error\": -0.001842225874743697\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels99\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.58 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 724\n",
      "Label Column: 724\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.8014384697, -11.8618031817, 4.01508, 3.79902)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60936.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 409 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 409 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t409 features in original data used to generate 409 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.16 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.5s of the 179.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.815115\n",
      "[2000]\tvalid_set's rmse: 0.810974\n",
      "[3000]\tvalid_set's rmse: 0.810504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8104\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.86s of the 171.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.771791\n",
      "[2000]\tvalid_set's rmse: 0.770634\n",
      "[3000]\tvalid_set's rmse: 0.77034\n",
      "[4000]\tvalid_set's rmse: 0.770302\n",
      "[5000]\tvalid_set's rmse: 0.770291\n",
      "[6000]\tvalid_set's rmse: 0.770291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7703\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.53s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.72s of the 156.72s of remaining time.\n",
      "\t-0.7308\t = Validation score   (-root_mean_squared_error)\n",
      "\t71.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 85.22s of the 85.22s of remaining time.\n",
      "\t-0.8135\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 56.38s of the 56.38s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.812774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8126\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.5s of the 18.77s of remaining time.\n",
      "\t-0.729\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 161.84s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels99\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24325393092164996\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24325393092164996,\n",
      "    \"mean_squared_error\": -0.05917247490883472,\n",
      "    \"mean_absolute_error\": -0.11152036161175032,\n",
      "    \"r2\": 0.9958996712817693,\n",
      "    \"pearsonr\": 0.9979536884581621,\n",
      "    \"median_absolute_error\": -0.05252770321955591\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels100\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.50 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 725\n",
      "Label Column: 725\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (9.7282392561, -12.2225861214, -1.84814, 2.7761)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60859.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.55 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 410 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 410 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t410 features in original data used to generate 410 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.24 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.530641\n",
      "[2000]\tvalid_set's rmse: 0.524735\n",
      "[3000]\tvalid_set's rmse: 0.524022\n",
      "[4000]\tvalid_set's rmse: 0.52383\n",
      "[5000]\tvalid_set's rmse: 0.523752\n",
      "[6000]\tvalid_set's rmse: 0.523747\n",
      "[7000]\tvalid_set's rmse: 0.523733\n",
      "[8000]\tvalid_set's rmse: 0.523731\n",
      "[9000]\tvalid_set's rmse: 0.523729\n",
      "[10000]\tvalid_set's rmse: 0.523728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5237\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.18s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 158.76s of the 158.76s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.515991\n",
      "[2000]\tvalid_set's rmse: 0.514241\n",
      "[3000]\tvalid_set's rmse: 0.51412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5141\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 149.96s of the 149.95s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9188.\n",
      "\t-0.469\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -0.9s of remaining time.\n",
      "\t-0.469\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.54s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels100\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14837423766175156\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14837423766175156,\n",
      "    \"mean_squared_error\": -0.022014914401705995,\n",
      "    \"mean_absolute_error\": -0.03927103824362196,\n",
      "    \"r2\": 0.9971431419213633,\n",
      "    \"pearsonr\": 0.9985744357601872,\n",
      "    \"median_absolute_error\": -0.0028035919571898216\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels101\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.42 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 726\n",
      "Label Column: 726\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.276083419, -13.7779359642, 3.92126, 3.55782)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60847.2 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.64 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 411 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 411 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t411 features in original data used to generate 411 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.33 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.698487\n",
      "[2000]\tvalid_set's rmse: 0.693155\n",
      "[3000]\tvalid_set's rmse: 0.692144\n",
      "[4000]\tvalid_set's rmse: 0.691909\n",
      "[5000]\tvalid_set's rmse: 0.691849\n",
      "[6000]\tvalid_set's rmse: 0.69183\n",
      "[7000]\tvalid_set's rmse: 0.691834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6918\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.18s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.63s of the 164.63s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.66974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6696\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 161.08s of the 161.08s of remaining time.\n",
      "\t-0.6319\t = Validation score   (-root_mean_squared_error)\n",
      "\t94.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 66.89s of the 66.89s of remaining time.\n",
      "\t-0.7084\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 39.05s of the 39.05s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.695799\n",
      "[2000]\tvalid_set's rmse: 0.695716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2091. Best iteration is:\n",
      "\t[2082]\tvalid_set's rmse: 0.695715\n",
      "\t-0.6957\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -1.19s of remaining time.\n",
      "\t-0.6293\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.81s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels101\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.20700054399691026\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.20700054399691026,\n",
      "    \"mean_squared_error\": -0.04284922521501687,\n",
      "    \"mean_absolute_error\": -0.08887379501421383,\n",
      "    \"r2\": 0.9966145528748328,\n",
      "    \"pearsonr\": 0.9983099697946702,\n",
      "    \"median_absolute_error\": -0.038896483021081396\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels102\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.32 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 727\n",
      "Label Column: 727\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.4117449025, -13.552226814, -2.05909, 2.99947)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60864.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.72 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 412 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 412 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t412 features in original data used to generate 412 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.627678\n",
      "[2000]\tvalid_set's rmse: 0.62324\n",
      "[3000]\tvalid_set's rmse: 0.622587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6223\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.22s of the 172.21s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.607716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6064\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.29s of the 167.29s of remaining time.\n",
      "\t-0.5636\t = Validation score   (-root_mean_squared_error)\n",
      "\t93.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 73.08s of the 73.07s of remaining time.\n",
      "\t-0.6403\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.13s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 45.49s of the 45.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.651507\n",
      "[2000]\tvalid_set's rmse: 0.651403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2448. Best iteration is:\n",
      "\t[2444]\tvalid_set's rmse: 0.651402\n",
      "\t-0.6514\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -1.34s of remaining time.\n",
      "\t-0.5628\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.93s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels102\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1829931644738653\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1829931644738653,\n",
      "    \"mean_squared_error\": -0.03348649824415898,\n",
      "    \"mean_absolute_error\": -0.07286285403945132,\n",
      "    \"r2\": 0.9962776022896912,\n",
      "    \"pearsonr\": 0.9981492496046089,\n",
      "    \"median_absolute_error\": -0.027499464123144746\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels103\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.23 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 728\n",
      "Label Column: 728\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.5264311731, -14.012892955, 3.56886, 3.37101)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60936.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.8 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 413 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 413 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t413 features in original data used to generate 413 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.49 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.700277\n",
      "[2000]\tvalid_set's rmse: 0.696023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6955\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.92s of the 172.92s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.656529\n",
      "[2000]\tvalid_set's rmse: 0.655359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6553\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.24s of the 167.24s of remaining time.\n",
      "\t-0.6201\t = Validation score   (-root_mean_squared_error)\n",
      "\t108.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 58.24s of the 58.24s of remaining time.\n",
      "\t-0.7034\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 40.06s of the 40.05s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.696593\n",
      "[2000]\tvalid_set's rmse: 0.696507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2143. Best iteration is:\n",
      "\t[1851]\tvalid_set's rmse: 0.696506\n",
      "\t-0.6965\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the -1.11s of remaining time.\n",
      "\t-0.6177\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.75s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels103\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1982205749412048\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1982205749412048,\n",
      "    \"mean_squared_error\": -0.03929139633002168,\n",
      "    \"mean_absolute_error\": -0.06970049945115434,\n",
      "    \"r2\": 0.9965420399129526,\n",
      "    \"pearsonr\": 0.9982727433754527,\n",
      "    \"median_absolute_error\": -0.021964891153613664\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels104\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.15 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 729\n",
      "Label Column: 729\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.1105527449, -17.0031182893, -2.00075, 3.24283)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60847.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.89 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 414 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 414 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t414 features in original data used to generate 414 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.58 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.672589\n",
      "[2000]\tvalid_set's rmse: 0.66854\n",
      "[3000]\tvalid_set's rmse: 0.668406\n",
      "[4000]\tvalid_set's rmse: 0.668152\n",
      "[5000]\tvalid_set's rmse: 0.66816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6681\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.7s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.9s of the 166.9s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.658802\n",
      "[2000]\tvalid_set's rmse: 0.657161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.657\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.47s of the 160.46s of remaining time.\n",
      "\t-0.613\t = Validation score   (-root_mean_squared_error)\n",
      "\t120.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 38.89s of the 38.89s of remaining time.\n",
      "\t-0.6832\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.53s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 11.19s of the 11.19s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 554. Best iteration is:\n",
      "\t[553]\tvalid_set's rmse: 0.702781\n",
      "\t-0.7028\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -0.34s of remaining time.\n",
      "\t-0.6125\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.96s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels104\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19500101453076898\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19500101453076898,\n",
      "    \"mean_squared_error\": -0.038025395668029434,\n",
      "    \"mean_absolute_error\": -0.060570758095492214,\n",
      "    \"r2\": 0.9963836795375789,\n",
      "    \"pearsonr\": 0.9981944573770143,\n",
      "    \"median_absolute_error\": -0.014239681087805156\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels105\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.08 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 730\n",
      "Label Column: 730\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.5939802257, -12.2967403492, 3.23353, 3.34567)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60859.43 MB\n",
      "\tTrain Data (Original)  Memory Usage: 60.97 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 415 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 415 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t415 features in original data used to generate 415 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.66 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.797467\n",
      "[2000]\tvalid_set's rmse: 0.79195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7918\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.3s of the 173.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.735889\n",
      "[2000]\tvalid_set's rmse: 0.734645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7344\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.4s of the 167.4s of remaining time.\n",
      "\t-0.7179\t = Validation score   (-root_mean_squared_error)\n",
      "\t79.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 87.98s of the 87.97s of remaining time.\n",
      "\t-0.7674\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 70.58s of the 70.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.771209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7711\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the 32.5s of remaining time.\n",
      "\t-0.7115\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 148.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels105\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.23347590847632768\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.23347590847632768,\n",
      "    \"mean_squared_error\": -0.05451099983884666,\n",
      "    \"mean_absolute_error\": -0.09810862058775595,\n",
      "    \"r2\": 0.9951296597667927,\n",
      "    \"pearsonr\": 0.9975735685450509,\n",
      "    \"median_absolute_error\": -0.04126602246990663\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels106\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   60.01 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 731\n",
      "Label Column: 731\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.7388051725, -16.4479096746, -1.61611, 3.47158)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60849.9 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.05 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 416 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 416 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t416 features in original data used to generate 416 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.74 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.882347\n",
      "[2000]\tvalid_set's rmse: 0.877916\n",
      "[3000]\tvalid_set's rmse: 0.877329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8772\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.15s of the 171.14s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.837856\n",
      "[2000]\tvalid_set's rmse: 0.836524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8365\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.05s of the 164.05s of remaining time.\n",
      "\t-0.8004\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 115.21s of the 115.21s of remaining time.\n",
      "\t-0.891\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 94.62s of the 94.62s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.890448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8903\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the 59.59s of remaining time.\n",
      "\t-0.7987\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 121.02s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels106\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2948268838359229\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2948268838359229,\n",
      "    \"mean_squared_error\": -0.08692289143240087,\n",
      "    \"mean_absolute_error\": -0.16942724049686392,\n",
      "    \"r2\": 0.9927868948331432,\n",
      "    \"pearsonr\": 0.9964238048448577,\n",
      "    \"median_absolute_error\": -0.10512592017300415\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels107\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.93 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 732\n",
      "Label Column: 732\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.4842246901, -14.8422299354, 2.59451, 3.42066)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60862.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.14 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 417 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 417 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t417 features in original data used to generate 417 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.83 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.613314\n",
      "[2000]\tvalid_set's rmse: 0.608159\n",
      "[3000]\tvalid_set's rmse: 0.60785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6078\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.81s of the 171.81s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.576091\n",
      "[2000]\tvalid_set's rmse: 0.575177\n",
      "[3000]\tvalid_set's rmse: 0.574899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5749\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 161.48s of the 161.48s of remaining time.\n",
      "\t-0.5224\t = Validation score   (-root_mean_squared_error)\n",
      "\t69.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 92.01s of the 92.01s of remaining time.\n",
      "\t-0.6163\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 78.84s of the 78.84s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.623949\n",
      "[2000]\tvalid_set's rmse: 0.623907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6239\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the 38.24s of remaining time.\n",
      "\t-0.5219\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 142.42s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels107\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1783032727224836\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1783032727224836,\n",
      "    \"mean_squared_error\": -0.03179205706354832,\n",
      "    \"mean_absolute_error\": -0.08847741665131448,\n",
      "    \"r2\": 0.9972826899220578,\n",
      "    \"pearsonr\": 0.9986448276374351,\n",
      "    \"median_absolute_error\": -0.046969161966552875\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels108\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.85 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 733\n",
      "Label Column: 733\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (21.61072417, -20.7587657901, -0.68563, 4.12722)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60868.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.22 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 418 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 418 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t418 features in original data used to generate 418 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.91 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.906711\n",
      "[2000]\tvalid_set's rmse: 0.902982\n",
      "[3000]\tvalid_set's rmse: 0.902537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9023\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.17s of the 172.17s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.824712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8242\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.62s of the 168.62s of remaining time.\n",
      "\t-0.7806\t = Validation score   (-root_mean_squared_error)\n",
      "\t44.59s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 123.89s of the 123.89s of remaining time.\n",
      "\t-0.8479\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.92s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 99.51s of the 99.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.854771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8547\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the 76.78s of remaining time.\n",
      "\t-0.7771\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 103.87s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels108\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.30442674449070706\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.30442674449070706,\n",
      "    \"mean_squared_error\": -0.09267564276121015,\n",
      "    \"mean_absolute_error\": -0.18404265501067904,\n",
      "    \"r2\": 0.9945588447514508,\n",
      "    \"pearsonr\": 0.9973056250877759,\n",
      "    \"median_absolute_error\": -0.12178704512098382\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels109\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.79 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 734\n",
      "Label Column: 734\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.1607432678, -15.8679733219, 2.3238, 3.50373)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60767.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.3 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 419 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 419 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t419 features in original data used to generate 419 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.0 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.803996\n",
      "[2000]\tvalid_set's rmse: 0.80082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8004\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.67s of the 173.67s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.723022\n",
      "[2000]\tvalid_set's rmse: 0.722053\n",
      "[3000]\tvalid_set's rmse: 0.721792\n",
      "[4000]\tvalid_set's rmse: 0.721754\n",
      "[5000]\tvalid_set's rmse: 0.721757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7218\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.49s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 158.65s of the 158.65s of remaining time.\n",
      "\t-0.704\t = Validation score   (-root_mean_squared_error)\n",
      "\t66.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 91.92s of the 91.92s of remaining time.\n",
      "\t-0.7673\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 73.51s of the 73.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.759333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7593\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the 44.49s of remaining time.\n",
      "\t-0.6978\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 136.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels109\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.23250965848587285\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.23250965848587285,\n",
      "    \"mean_squared_error\": -0.05406074128921735,\n",
      "    \"mean_absolute_error\": -0.10658997296894876,\n",
      "    \"r2\": 0.9955958529666993,\n",
      "    \"pearsonr\": 0.9978056355845963,\n",
      "    \"median_absolute_error\": -0.05173369842363884\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels110\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.71 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 735\n",
      "Label Column: 735\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (24.7979332211, -24.8359294426, 1.32877, 4.51126)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60633.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.39 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 420 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 420 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t420 features in original data used to generate 420 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.08 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.60977\n",
      "[2000]\tvalid_set's rmse: 1.60587\n",
      "[3000]\tvalid_set's rmse: 1.60512\n",
      "[4000]\tvalid_set's rmse: 1.60521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.6047\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.66s of the 169.66s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.57796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.5769\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 165.28s of the 165.27s of remaining time.\n",
      "\t-1.5495\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 138.94s of the 138.94s of remaining time.\n",
      "\t-1.59\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 128.71s of the 128.7s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.62569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.6256\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the 104.16s of remaining time.\n",
      "\t-1.5411\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 76.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels110\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.7139239116153624\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.7139239116153624,\n",
      "    \"mean_squared_error\": -0.5096873515761787,\n",
      "    \"mean_absolute_error\": -0.48287845283048114,\n",
      "    \"r2\": 0.9749533692305782,\n",
      "    \"pearsonr\": 0.9877979345129524,\n",
      "    \"median_absolute_error\": -0.3430915650975663\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels111\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.65 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 736\n",
      "Label Column: 736\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (25.6635344909, -19.6932241999, 2.27902, 3.75375)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60922.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 421 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 421 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t421 features in original data used to generate 421 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.16 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.816016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8137\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 175.87s of the 175.87s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.803776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8034\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 171.7s of the 171.69s of remaining time.\n",
      "\t-0.7601\t = Validation score   (-root_mean_squared_error)\n",
      "\t72.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 99.02s of the 99.02s of remaining time.\n",
      "\t-0.8259\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 85.3s of the 85.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.832454\n",
      "[2000]\tvalid_set's rmse: 0.832353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8324\t = Validation score   (-root_mean_squared_error)\n",
      "\t52.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the 32.12s of remaining time.\n",
      "\t-0.7513\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 148.5s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels111\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2634194693057782\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2634194693057782,\n",
      "    \"mean_squared_error\": -0.06938981680933759,\n",
      "    \"mean_absolute_error\": -0.13822215921678152,\n",
      "    \"r2\": 0.9950749962933888,\n",
      "    \"pearsonr\": 0.9975509305459795,\n",
      "    \"median_absolute_error\": -0.0794966483731949\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels112\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.57 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 737\n",
      "Label Column: 737\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.7342818599, -16.3046086041, 2.18114, 3.03487)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60829.7 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.55 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 422 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 422 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t422 features in original data used to generate 422 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.25 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.703282\n",
      "[2000]\tvalid_set's rmse: 0.695519\n",
      "[3000]\tvalid_set's rmse: 0.694808\n",
      "[4000]\tvalid_set's rmse: 0.694596\n",
      "[5000]\tvalid_set's rmse: 0.694511\n",
      "[6000]\tvalid_set's rmse: 0.694535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6945\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 165.84s of the 165.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.695615\n",
      "[2000]\tvalid_set's rmse: 0.694296\n",
      "[3000]\tvalid_set's rmse: 0.694078\n",
      "[4000]\tvalid_set's rmse: 0.694052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.694\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 154.41s of the 154.41s of remaining time.\n",
      "\t-0.6483\t = Validation score   (-root_mean_squared_error)\n",
      "\t103.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 49.75s of the 49.75s of remaining time.\n",
      "\t-0.711\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 36.94s of the 36.93s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.734206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1932. Best iteration is:\n",
      "\t[1890]\tvalid_set's rmse: 0.734095\n",
      "\t-0.7341\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the -0.82s of remaining time.\n",
      "\t-0.6458\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels112\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.20769868756692547\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.20769868756692547,\n",
      "    \"mean_squared_error\": -0.04313874481702316,\n",
      "    \"mean_absolute_error\": -0.07507853482703335,\n",
      "    \"r2\": 0.9953158819284821,\n",
      "    \"pearsonr\": 0.9976573064365007,\n",
      "    \"median_absolute_error\": -0.02456686844228506\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels113\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.48 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 738\n",
      "Label Column: 738\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.2464464452, -12.6726456884, 1.11858, 2.8189)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60845.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.64 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 423 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 423 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t423 features in original data used to generate 423 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.33 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.660378\n",
      "[2000]\tvalid_set's rmse: 0.656953\n",
      "[3000]\tvalid_set's rmse: 0.656111\n",
      "[4000]\tvalid_set's rmse: 0.655795\n",
      "[5000]\tvalid_set's rmse: 0.655691\n",
      "[6000]\tvalid_set's rmse: 0.655664\n",
      "[7000]\tvalid_set's rmse: 0.655654\n",
      "[8000]\tvalid_set's rmse: 0.655655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6557\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.18s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.51s of the 161.51s of remaining time.\n",
      "\t-0.6773\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 158.48s of the 158.48s of remaining time.\n",
      "\t-0.6098\t = Validation score   (-root_mean_squared_error)\n",
      "\t72.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 85.72s of the 85.72s of remaining time.\n",
      "\t-0.7119\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 72.32s of the 72.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.711025\n",
      "[2000]\tvalid_set's rmse: 0.710911\n",
      "[3000]\tvalid_set's rmse: 0.71091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7109\t = Validation score   (-root_mean_squared_error)\n",
      "\t63.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the 7.7s of remaining time.\n",
      "\t-0.6097\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 172.94s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels113\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2079021675499393\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2079021675499393,\n",
      "    \"mean_squared_error\": -0.04322331127196317,\n",
      "    \"mean_absolute_error\": -0.10219534158267422,\n",
      "    \"r2\": 0.9945599844523672,\n",
      "    \"pearsonr\": 0.99729040693963,\n",
      "    \"median_absolute_error\": -0.05342363079903567\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels114\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.38 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 739\n",
      "Label Column: 739\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.7736653946, -16.9944465515, 0.34083, 3.18551)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60923.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.72 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 424 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 424 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t424 features in original data used to generate 424 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.634823\n",
      "[2000]\tvalid_set's rmse: 0.63203\n",
      "[3000]\tvalid_set's rmse: 0.63161\n",
      "[4000]\tvalid_set's rmse: 0.631396\n",
      "[5000]\tvalid_set's rmse: 0.631363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6313\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.11s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.83s of the 166.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.626036\n",
      "[2000]\tvalid_set's rmse: 0.624488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6243\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 158.83s of the 158.83s of remaining time.\n",
      "\t-0.5783\t = Validation score   (-root_mean_squared_error)\n",
      "\t119.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 38.93s of the 38.93s of remaining time.\n",
      "\t-0.6492\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.2s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 10.28s of the 10.28s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 487. Best iteration is:\n",
      "\t[486]\tvalid_set's rmse: 0.661156\n",
      "\t-0.6612\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the -0.37s of remaining time.\n",
      "\t-0.5762\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.96s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels114\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18371564707275462\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18371564707275462,\n",
      "    \"mean_squared_error\": -0.03375143897936088,\n",
      "    \"mean_absolute_error\": -0.05943996128365913,\n",
      "    \"r2\": 0.9966735820043632,\n",
      "    \"pearsonr\": 0.9983377397863067,\n",
      "    \"median_absolute_error\": -0.014897106714575337\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels115\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.31 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 740\n",
      "Label Column: 740\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.0385758733, -14.377204978, 0.53374, 3.40925)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60959.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.8 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 425 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 425 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t425 features in original data used to generate 425 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.5 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.727772\n",
      "[2000]\tvalid_set's rmse: 0.725483\n",
      "[3000]\tvalid_set's rmse: 0.724628\n",
      "[4000]\tvalid_set's rmse: 0.724423\n",
      "[5000]\tvalid_set's rmse: 0.724407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7244\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.18s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.76s of the 167.76s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.687803\n",
      "[2000]\tvalid_set's rmse: 0.686899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6868\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.29s of the 160.29s of remaining time.\n",
      "\t-0.6448\t = Validation score   (-root_mean_squared_error)\n",
      "\t68.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 91.83s of the 91.82s of remaining time.\n",
      "\t-0.7227\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.35s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 74.06s of the 74.05s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.731241\n",
      "[2000]\tvalid_set's rmse: 0.731201\n",
      "[3000]\tvalid_set's rmse: 0.7312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7312\t = Validation score   (-root_mean_squared_error)\n",
      "\t57.26s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the 15.86s of remaining time.\n",
      "\t-0.6436\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 164.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels115\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21814037063909972\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21814037063909972,\n",
      "    \"mean_squared_error\": -0.04758522130256368,\n",
      "    \"mean_absolute_error\": -0.10248981600776556,\n",
      "    \"r2\": 0.9959055537104927,\n",
      "    \"pearsonr\": 0.9979664959337688,\n",
      "    \"median_absolute_error\": -0.05347113068539111\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels116\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.21 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 741\n",
      "Label Column: 741\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.0936629876, -15.261262073, 1.09927, 3.55139)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60852.5 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.89 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 426 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 426 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t426 features in original data used to generate 426 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.58 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.49s of the 179.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.757754\n",
      "[2000]\tvalid_set's rmse: 0.752397\n",
      "[3000]\tvalid_set's rmse: 0.752053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7519\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.19s of the 171.19s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.704087\n",
      "[2000]\tvalid_set's rmse: 0.703365\n",
      "[3000]\tvalid_set's rmse: 0.703193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7032\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.5s of the 160.5s of remaining time.\n",
      "\t-0.664\t = Validation score   (-root_mean_squared_error)\n",
      "\t63.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 97.23s of the 97.22s of remaining time.\n",
      "\t-0.7417\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.79s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 80.03s of the 80.02s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.773983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7739\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.49s of the 43.07s of remaining time.\n",
      "\t-0.6627\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 137.58s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels116\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2269012634990347\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2269012634990347,\n",
      "    \"mean_squared_error\": -0.05148418337745805,\n",
      "    \"mean_absolute_error\": -0.1112880038660204,\n",
      "    \"r2\": 0.995917569548678,\n",
      "    \"pearsonr\": 0.9979729398312625,\n",
      "    \"median_absolute_error\": -0.060965434156365905\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels117\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.13 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 742\n",
      "Label Column: 742\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.1498638997, -14.0782896939, 1.65918, 3.62388)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60861.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.97 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 427 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 427 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t427 features in original data used to generate 427 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.66 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.788977\n",
      "[2000]\tvalid_set's rmse: 0.78125\n",
      "[3000]\tvalid_set's rmse: 0.779902\n",
      "[4000]\tvalid_set's rmse: 0.779713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7797\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.33s of the 169.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.712798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7123\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.6s of the 164.59s of remaining time.\n",
      "\t-0.6713\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 12.7s of the 12.7s of remaining time.\n",
      "\t-0.7713\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the -0.26s of remaining time.\n",
      "\t-0.6689\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.92s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels117\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21285957614410903\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21285957614410903,\n",
      "    \"mean_squared_error\": -0.04530919915624968,\n",
      "    \"mean_absolute_error\": -0.06521974235351002,\n",
      "    \"r2\": 0.9965495083274113,\n",
      "    \"pearsonr\": 0.9982779763989018,\n",
      "    \"median_absolute_error\": -0.015878700351800668\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels118\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.07 GB / 2000.36 GB (3.0%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 743\n",
      "Label Column: 743\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.6950189356, -13.1472542446, 0.40695, 3.34679)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60854.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.06 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 428 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 428 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t428 features in original data used to generate 428 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.75 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.914673\n",
      "[2000]\tvalid_set's rmse: 0.909869\n",
      "[3000]\tvalid_set's rmse: 0.909008\n",
      "[4000]\tvalid_set's rmse: 0.908878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9088\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 170.43s of the 170.43s of remaining time.\n",
      "\t-0.8141\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.82s of the 167.82s of remaining time.\n",
      "\t-0.7767\t = Validation score   (-root_mean_squared_error)\n",
      "\t125.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 41.47s of the 41.47s of remaining time.\n",
      "\t-0.8264\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.93s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 26.15s of the 26.15s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.821348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1349. Best iteration is:\n",
      "\t[1189]\tvalid_set's rmse: 0.821261\n",
      "\t-0.8213\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the -0.9s of remaining time.\n",
      "\t-0.7756\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.51s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels118\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24762839371488438\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24762839371488438,\n",
      "    \"mean_squared_error\": -0.06131982137381346,\n",
      "    \"mean_absolute_error\": -0.08374476545584827,\n",
      "    \"r2\": 0.9945249957065295,\n",
      "    \"pearsonr\": 0.9972606074010304,\n",
      "    \"median_absolute_error\": -0.022317405316210737\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels119\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   59.00 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 744\n",
      "Label Column: 744\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.5681365147, -16.3626307425, -0.22963, 3.07163)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60872.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.14 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 429 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 429 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t429 features in original data used to generate 429 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.83 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.620016\n",
      "[2000]\tvalid_set's rmse: 0.616907\n",
      "[3000]\tvalid_set's rmse: 0.616067\n",
      "[4000]\tvalid_set's rmse: 0.615828\n",
      "[5000]\tvalid_set's rmse: 0.615835\n",
      "[6000]\tvalid_set's rmse: 0.6158\n",
      "[7000]\tvalid_set's rmse: 0.615786\n",
      "[8000]\tvalid_set's rmse: 0.615785\n",
      "[9000]\tvalid_set's rmse: 0.615784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6158\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.38s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 158.91s of the 158.91s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.585988\n",
      "[2000]\tvalid_set's rmse: 0.584913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5849\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 151.58s of the 151.57s of remaining time.\n",
      "\t-0.5488\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 15.31s of the 15.31s of remaining time.\n",
      "\t-0.5997\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.46s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the -0.59s of remaining time.\n",
      "\t-0.5471\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.24s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels119\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1739715904968823\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1739715904968823,\n",
      "    \"mean_squared_error\": -0.030266114300014784,\n",
      "    \"mean_absolute_error\": -0.054506493437873185,\n",
      "    \"r2\": 0.9967918126593511,\n",
      "    \"pearsonr\": 0.9983947317406119,\n",
      "    \"median_absolute_error\": -0.011326570656963367\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels120\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.93 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 745\n",
      "Label Column: 745\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.5008679837, -24.1274194804, -0.80537, 3.06014)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60853.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.22 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 430 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 430 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t430 features in original data used to generate 430 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 35.91 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.48s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.576241\n",
      "[2000]\tvalid_set's rmse: 0.572757\n",
      "[3000]\tvalid_set's rmse: 0.571789\n",
      "[4000]\tvalid_set's rmse: 0.571331\n",
      "[5000]\tvalid_set's rmse: 0.571352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5713\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.51s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.42s of the 167.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.532963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5316\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 161.5s of the 161.5s of remaining time.\n",
      "\t-0.4939\t = Validation score   (-root_mean_squared_error)\n",
      "\t99.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 61.72s of the 61.72s of remaining time.\n",
      "\t-0.5709\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 46.39s of the 46.39s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.580812\n",
      "[2000]\tvalid_set's rmse: 0.580727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2378. Best iteration is:\n",
      "\t[2367]\tvalid_set's rmse: 0.580726\n",
      "\t-0.5807\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.71s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.48s of the -1.29s of remaining time.\n",
      "\t-0.4923\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.97s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels120\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15952831045783988\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15952831045783988,\n",
      "    \"mean_squared_error\": -0.02544928183753296,\n",
      "    \"mean_absolute_error\": -0.06125411721870747,\n",
      "    \"r2\": 0.9972820915744707,\n",
      "    \"pearsonr\": 0.9986434922784315,\n",
      "    \"median_absolute_error\": -0.022564193260873472\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels121\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.83 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 746\n",
      "Label Column: 746\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.5580287468, -21.8315877753, -1.04869, 3.22621)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60849.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.31 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 431 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 431 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t431 features in original data used to generate 431 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.0 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.558358\n",
      "[2000]\tvalid_set's rmse: 0.55363\n",
      "[3000]\tvalid_set's rmse: 0.552859\n",
      "[4000]\tvalid_set's rmse: 0.552792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5528\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.91s of the 169.91s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.508083\n",
      "[2000]\tvalid_set's rmse: 0.506986\n",
      "[3000]\tvalid_set's rmse: 0.506853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5068\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 161.13s of the 161.13s of remaining time.\n",
      "\t-0.4543\t = Validation score   (-root_mean_squared_error)\n",
      "\t99.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 61.31s of the 61.31s of remaining time.\n",
      "\t-0.5247\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.87s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 43.02s of the 43.02s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.56009\n",
      "[2000]\tvalid_set's rmse: 0.560034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2183. Best iteration is:\n",
      "\t[2176]\tvalid_set's rmse: 0.560033\n",
      "\t-0.56\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.33s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the -1.27s of remaining time.\n",
      "\t-0.4539\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.91s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels121\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14673848210391896\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14673848210391896,\n",
      "    \"mean_squared_error\": -0.02153218213016205,\n",
      "    \"mean_absolute_error\": -0.055259605582328834,\n",
      "    \"r2\": 0.9979310724338619,\n",
      "    \"pearsonr\": 0.9989668840727054,\n",
      "    \"median_absolute_error\": -0.020237742329269393\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels122\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.74 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 747\n",
      "Label Column: 747\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.6647541655, -14.6270476468, -1.03685, 3.17489)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60945.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.39 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 432 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 432 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t432 features in original data used to generate 432 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.08 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.513538\n",
      "[2000]\tvalid_set's rmse: 0.509883\n",
      "[3000]\tvalid_set's rmse: 0.509314\n",
      "[4000]\tvalid_set's rmse: 0.509224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5092\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.94s of the 168.93s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.482367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4823\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.63s of the 164.63s of remaining time.\n",
      "\t-0.4411\t = Validation score   (-root_mean_squared_error)\n",
      "\t113.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 50.79s of the 50.79s of remaining time.\n",
      "\t-0.4896\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 24.49s of the 24.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.51411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1211. Best iteration is:\n",
      "\t[1210]\tvalid_set's rmse: 0.514066\n",
      "\t-0.5141\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the -0.62s of remaining time.\n",
      "\t-0.4394\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.29s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels122\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14060074236145056\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14060074236145056,\n",
      "    \"mean_squared_error\": -0.01976856875259098,\n",
      "    \"mean_absolute_error\": -0.04851705341995562,\n",
      "    \"r2\": 0.9980386356914172,\n",
      "    \"pearsonr\": 0.9990195567233406,\n",
      "    \"median_absolute_error\": -0.013687515558663987\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels123\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.67 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 748\n",
      "Label Column: 748\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.7645709398, -13.9204568199, -0.81276, 2.94921)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60849.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.47 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 433 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 433 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t433 features in original data used to generate 433 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.16 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.520654\n",
      "[2000]\tvalid_set's rmse: 0.5162\n",
      "[3000]\tvalid_set's rmse: 0.515315\n",
      "[4000]\tvalid_set's rmse: 0.51509\n",
      "[5000]\tvalid_set's rmse: 0.515093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5151\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.82s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.17s of the 168.17s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.501633\n",
      "[2000]\tvalid_set's rmse: 0.500222\n",
      "[3000]\tvalid_set's rmse: 0.500084\n",
      "[4000]\tvalid_set's rmse: 0.500022\n",
      "[5000]\tvalid_set's rmse: 0.500017\n",
      "[6000]\tvalid_set's rmse: 0.500017\n",
      "[7000]\tvalid_set's rmse: 0.500016\n",
      "[8000]\tvalid_set's rmse: 0.500016\n",
      "[9000]\tvalid_set's rmse: 0.500016\n",
      "[10000]\tvalid_set's rmse: 0.500016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.42s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 140.68s of the 140.68s of remaining time.\n",
      "\t-0.4653\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 4.86s of the 4.86s of remaining time.\n",
      "\t-0.5351\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the -0.17s of remaining time.\n",
      "\t-0.4644\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.84s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels123\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14756416033560002\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14756416033560002,\n",
      "    \"mean_squared_error\": -0.02177518141555056,\n",
      "    \"mean_absolute_error\": -0.04506789243019133,\n",
      "    \"r2\": 0.9974962438497266,\n",
      "    \"pearsonr\": 0.9987482945334134,\n",
      "    \"median_absolute_error\": -0.008853170472962912\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels124\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.58 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 749\n",
      "Label Column: 749\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.0942733085, -14.1562012722, -0.31285, 2.98414)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60375.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.56 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 434 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 434 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t434 features in original data used to generate 434 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.25 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.46s of the 179.45s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.606115\n",
      "[2000]\tvalid_set's rmse: 0.601923\n",
      "[3000]\tvalid_set's rmse: 0.601268\n",
      "[4000]\tvalid_set's rmse: 0.60108\n",
      "[5000]\tvalid_set's rmse: 0.601038\n",
      "[6000]\tvalid_set's rmse: 0.601024\n",
      "[7000]\tvalid_set's rmse: 0.601011\n",
      "[8000]\tvalid_set's rmse: 0.601007\n",
      "[9000]\tvalid_set's rmse: 0.601007\n",
      "[10000]\tvalid_set's rmse: 0.601007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.601\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.05s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 157.44s of the 157.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.595297\n",
      "[2000]\tvalid_set's rmse: 0.594006\n",
      "[3000]\tvalid_set's rmse: 0.593898\n",
      "[4000]\tvalid_set's rmse: 0.593839\n",
      "[5000]\tvalid_set's rmse: 0.593836\n",
      "[6000]\tvalid_set's rmse: 0.593835\n",
      "[7000]\tvalid_set's rmse: 0.593835\n",
      "[8000]\tvalid_set's rmse: 0.593835\n",
      "[9000]\tvalid_set's rmse: 0.593835\n",
      "[10000]\tvalid_set's rmse: 0.593835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5938\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.61s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 128.46s of the 128.46s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7688.\n",
      "\t-0.5583\t = Validation score   (-root_mean_squared_error)\n",
      "\t128.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.46s of the -0.85s of remaining time.\n",
      "\t-0.5567\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels124\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17616927771985066\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17616927771985066,\n",
      "    \"mean_squared_error\": -0.031035614412333757,\n",
      "    \"mean_absolute_error\": -0.04427261604160598,\n",
      "    \"r2\": 0.9965145048998526,\n",
      "    \"pearsonr\": 0.9982569624249069,\n",
      "    \"median_absolute_error\": -0.004003554235766371\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels125\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.49 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 750\n",
      "Label Column: 750\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.2780606872, -16.2328831875, 0.57845, 3.23284)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60636.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.64 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 435 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 435 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t435 features in original data used to generate 435 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.33 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.47s of the 179.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.70935\n",
      "[2000]\tvalid_set's rmse: 0.703223\n",
      "[3000]\tvalid_set's rmse: 0.702121\n",
      "[4000]\tvalid_set's rmse: 0.701928\n",
      "[5000]\tvalid_set's rmse: 0.701916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7019\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.95s of the 166.95s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.656315\n",
      "[2000]\tvalid_set's rmse: 0.654713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6546\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.08s of the 159.08s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9214.\n",
      "\t-0.6315\t = Validation score   (-root_mean_squared_error)\n",
      "\t159.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.47s of the -1.16s of remaining time.\n",
      "\t-0.6276\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.76s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels125\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.198669021626574\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.198669021626574,\n",
      "    \"mean_squared_error\": -0.03946938015406018,\n",
      "    \"mean_absolute_error\": -0.05313733409551121,\n",
      "    \"r2\": 0.9962231135525054,\n",
      "    \"pearsonr\": 0.9981115978666314,\n",
      "    \"median_absolute_error\": -0.005460048675781204\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels126\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.42 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 751\n",
      "Label Column: 751\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.3990022825, -13.5149830288, 0.22978, 3.54479)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60685.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.72 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 436 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 436 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t436 features in original data used to generate 436 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.46s of the 179.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.33289\n",
      "[2000]\tvalid_set's rmse: 1.32919\n",
      "[3000]\tvalid_set's rmse: 1.32762\n",
      "[4000]\tvalid_set's rmse: 1.32724\n",
      "[5000]\tvalid_set's rmse: 1.32708\n",
      "[6000]\tvalid_set's rmse: 1.32706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.327\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.08s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 165.51s of the 165.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.29661\n",
      "[2000]\tvalid_set's rmse: 1.29498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.2948\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.1s of the 159.1s of remaining time.\n",
      "\t-1.2841\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 120.98s of the 120.98s of remaining time.\n",
      "\t-1.3483\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.66s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 105.17s of the 105.16s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.35929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.3593\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.46s of the 76.7s of remaining time.\n",
      "\t-1.2739\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 103.95s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels126\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.480330925987526\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.480330925987526,\n",
      "    \"mean_squared_error\": -0.23071779846003368,\n",
      "    \"mean_absolute_error\": -0.29236647593167764,\n",
      "    \"r2\": 0.981637068868154,\n",
      "    \"pearsonr\": 0.9910458968349859,\n",
      "    \"median_absolute_error\": -0.19114712985115978\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels127\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.35 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 752\n",
      "Label Column: 752\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.9765752034, -11.7604223543, 0.229, 3.57577)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60602.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.81 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 437 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 437 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t437 features in original data used to generate 437 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.5 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.45s of the 179.45s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.412157\n",
      "[2000]\tvalid_set's rmse: 0.408945\n",
      "[3000]\tvalid_set's rmse: 0.40845\n",
      "[4000]\tvalid_set's rmse: 0.408422\n",
      "[5000]\tvalid_set's rmse: 0.408409\n",
      "[6000]\tvalid_set's rmse: 0.408405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4084\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 165.71s of the 165.71s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.379778\n",
      "[2000]\tvalid_set's rmse: 0.37877\n",
      "[3000]\tvalid_set's rmse: 0.378528\n",
      "[4000]\tvalid_set's rmse: 0.378508\n",
      "[5000]\tvalid_set's rmse: 0.378504\n",
      "[6000]\tvalid_set's rmse: 0.378504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3785\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.09s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 148.0s of the 147.99s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8688.\n",
      "\t-0.3291\t = Validation score   (-root_mean_squared_error)\n",
      "\t148.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.45s of the -0.63s of remaining time.\n",
      "\t-0.3282\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels127\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1038838034845598\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1038838034845598,\n",
      "    \"mean_squared_error\": -0.010791844626418625,\n",
      "    \"mean_absolute_error\": -0.02763230447615715,\n",
      "    \"r2\": 0.9991558897629462,\n",
      "    \"pearsonr\": 0.9995778762821024,\n",
      "    \"median_absolute_error\": -0.002570156065972884\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels128\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.27 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 753\n",
      "Label Column: 753\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.439199048, -15.1628740822, 2.55403, 3.42319)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60623.11 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.89 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 438 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 438 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t438 features in original data used to generate 438 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.58 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.46s of the 179.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.770814\n",
      "[2000]\tvalid_set's rmse: 0.765286\n",
      "[3000]\tvalid_set's rmse: 0.764336\n",
      "[4000]\tvalid_set's rmse: 0.764289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7643\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.25s of the 169.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.678213\n",
      "[2000]\tvalid_set's rmse: 0.676708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6766\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.08s of the 162.08s of remaining time.\n",
      "\t-0.6385\t = Validation score   (-root_mean_squared_error)\n",
      "\t78.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 82.91s of the 82.91s of remaining time.\n",
      "\t-0.7266\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.11s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 66.7s of the 66.7s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.731224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7311\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.46s of the 29.0s of remaining time.\n",
      "\t-0.6359\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 151.66s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels128\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2096203042864394\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2096203042864394,\n",
      "    \"mean_squared_error\": -0.04394067196913946,\n",
      "    \"mean_absolute_error\": -0.08741979934760098,\n",
      "    \"r2\": 0.9962498621338104,\n",
      "    \"pearsonr\": 0.9981332053108174,\n",
      "    \"median_absolute_error\": -0.04068460323419526\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels129\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.19 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 754\n",
      "Label Column: 754\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.6687455014, -13.4525271927, -0.52675, 3.41379)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60623.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 62.97 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 439 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 439 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t439 features in original data used to generate 439 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.67 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.461015\n",
      "[2000]\tvalid_set's rmse: 0.458387\n",
      "[3000]\tvalid_set's rmse: 0.457804\n",
      "[4000]\tvalid_set's rmse: 0.457789\n",
      "[5000]\tvalid_set's rmse: 0.457731\n",
      "[6000]\tvalid_set's rmse: 0.457727\n",
      "[7000]\tvalid_set's rmse: 0.457726\n",
      "[8000]\tvalid_set's rmse: 0.457726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4577\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.77s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.84s of the 159.84s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.360646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3605\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.8s of the 155.8s of remaining time.\n",
      "\t-0.3366\t = Validation score   (-root_mean_squared_error)\n",
      "\t144.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 11.03s of the 11.03s of remaining time.\n",
      "\t-0.3787\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the -0.24s of remaining time.\n",
      "\t-0.3338\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.89s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels129\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10759199889741697\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10759199889741697,\n",
      "    \"mean_squared_error\": -0.011576038226741761,\n",
      "    \"mean_absolute_error\": -0.040099895625630703,\n",
      "    \"r2\": 0.9990065937522051,\n",
      "    \"pearsonr\": 0.999503897079316,\n",
      "    \"median_absolute_error\": -0.014999338084606961\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels130\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.12 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 755\n",
      "Label Column: 755\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.459649777, -15.0700623327, 3.44724, 3.88795)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60621.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.06 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 440 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 440 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t440 features in original data used to generate 440 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.75 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.45s of the 179.45s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.18503\n",
      "[2000]\tvalid_set's rmse: 1.17644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1761\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.07s of the 173.07s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.10471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1038\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.45s of the 168.45s of remaining time.\n",
      "\t-1.0726\t = Validation score   (-root_mean_squared_error)\n",
      "\t120.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 47.8s of the 47.8s of remaining time.\n",
      "\t-1.1458\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 21.32s of the 21.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.15951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1035. Best iteration is:\n",
      "\t[1035]\tvalid_set's rmse: 1.15946\n",
      "\t-1.1595\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.45s of the -0.84s of remaining time.\n",
      "\t-1.0669\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels130\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.3452480479159497\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.3452480479159497,\n",
      "    \"mean_squared_error\": -0.11919621458977368,\n",
      "    \"mean_absolute_error\": -0.13054602698815193,\n",
      "    \"r2\": 0.992113882647608,\n",
      "    \"pearsonr\": 0.9960696774522243,\n",
      "    \"median_absolute_error\": -0.04987708546967762\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels131\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   58.05 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 756\n",
      "Label Column: 756\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.3969181645, -16.2342986856, -1.60409, 3.18124)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60874.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.14 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 441 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 441 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t441 features in original data used to generate 441 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.83 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.03448\n",
      "[2000]\tvalid_set's rmse: 1.03117\n",
      "[3000]\tvalid_set's rmse: 1.02981\n",
      "[4000]\tvalid_set's rmse: 1.0296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0296\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.28s of the 168.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.98627\n",
      "[2000]\tvalid_set's rmse: 0.984534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9844\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.82s of the 160.82s of remaining time.\n",
      "\t-0.9474\t = Validation score   (-root_mean_squared_error)\n",
      "\t88.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 71.57s of the 71.57s of remaining time.\n",
      "\t-1.0191\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 58.71s of the 58.71s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.01962\n",
      "[2000]\tvalid_set's rmse: 1.01951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2944. Best iteration is:\n",
      "\t[2430]\tvalid_set's rmse: 1.01951\n",
      "\t-1.0195\t = Validation score   (-root_mean_squared_error)\n",
      "\t59.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the -1.34s of remaining time.\n",
      "\t-0.9461\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.01s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels131\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.3092261772029391\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.3092261772029391,\n",
      "    \"mean_squared_error\": -0.09562082866754333,\n",
      "    \"mean_absolute_error\": -0.12656537216156197,\n",
      "    \"r2\": 0.9905506685490189,\n",
      "    \"pearsonr\": 0.9953034056308866,\n",
      "    \"median_absolute_error\": -0.052085001788025\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels132\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.96 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 757\n",
      "Label Column: 757\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.1887147365, -13.4004593254, 3.58042, 4.00279)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60958.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.22 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 442 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 442 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t442 features in original data used to generate 442 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 36.92 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.45s of the 179.45s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.549433\n",
      "[2000]\tvalid_set's rmse: 0.546838\n",
      "[3000]\tvalid_set's rmse: 0.546709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5466\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 170.69s of the 170.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.460112\n",
      "[2000]\tvalid_set's rmse: 0.458416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4583\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.3s of the 162.3s of remaining time.\n",
      "\t-0.4294\t = Validation score   (-root_mean_squared_error)\n",
      "\t81.92s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 79.95s of the 79.95s of remaining time.\n",
      "\t-0.4971\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.04s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 55.48s of the 55.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.494395\n",
      "[2000]\tvalid_set's rmse: 0.494349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4943\t = Validation score   (-root_mean_squared_error)\n",
      "\t52.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.45s of the 2.29s of remaining time.\n",
      "\t-0.4259\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 178.33s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels132\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13948687792157974\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13948687792157974,\n",
      "    \"mean_squared_error\": -0.019456589112309713,\n",
      "    \"mean_absolute_error\": -0.056763755225423984,\n",
      "    \"r2\": 0.9987855438630425,\n",
      "    \"pearsonr\": 0.9993941531593625,\n",
      "    \"median_absolute_error\": -0.02461851327373088\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels133\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.87 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 758\n",
      "Label Column: 758\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.6214788123, -14.2488729211, -2.339, 3.04305)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60879.57 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.31 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 443 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 443 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t443 features in original data used to generate 443 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.0 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.45s of the 179.45s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.663022\n",
      "[2000]\tvalid_set's rmse: 0.658637\n",
      "[3000]\tvalid_set's rmse: 0.657941\n",
      "[4000]\tvalid_set's rmse: 0.657711\n",
      "[5000]\tvalid_set's rmse: 0.657674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6576\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.32s of the 166.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.622303\n",
      "[2000]\tvalid_set's rmse: 0.621203\n",
      "[3000]\tvalid_set's rmse: 0.621105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6211\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.54s of the 155.53s of remaining time.\n",
      "\t-0.5943\t = Validation score   (-root_mean_squared_error)\n",
      "\t80.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 73.97s of the 73.96s of remaining time.\n",
      "\t-0.6395\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.84s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 52.01s of the 52.01s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.681126\n",
      "[2000]\tvalid_set's rmse: 0.681099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6811\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.45s of the 2.47s of remaining time.\n",
      "\t-0.5919\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 178.22s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels133\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19395859176549882\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19395859176549882,\n",
      "    \"mean_squared_error\": -0.037619935319655434,\n",
      "    \"mean_absolute_error\": -0.08014113573430734,\n",
      "    \"r2\": 0.9959370520737798,\n",
      "    \"pearsonr\": 0.9979732812434337,\n",
      "    \"median_absolute_error\": -0.0345859448877075\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels134\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.78 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 759\n",
      "Label Column: 759\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.1152453904, -12.5536919106, 4.34683, 4.20229)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60981.73 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.39 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 444 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 444 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t444 features in original data used to generate 444 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.08 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.32843\n",
      "[2000]\tvalid_set's rmse: 1.32135\n",
      "[3000]\tvalid_set's rmse: 1.31984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.3195\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.41s of the 171.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.26122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.2603\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.68s of the 167.68s of remaining time.\n",
      "\t-1.2235\t = Validation score   (-root_mean_squared_error)\n",
      "\t42.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 124.86s of the 124.86s of remaining time.\n",
      "\t-1.297\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.05s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 112.7s of the 112.7s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.29042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.2904\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the 85.0s of remaining time.\n",
      "\t-1.2209\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 95.62s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels134\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.5153174547983933\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.5153174547983933,\n",
      "    \"mean_squared_error\": -0.26555207921989377,\n",
      "    \"mean_absolute_error\": -0.34313593947726945,\n",
      "    \"r2\": 0.9849610166296763,\n",
      "    \"pearsonr\": 0.9926214375053437,\n",
      "    \"median_absolute_error\": -0.2448650096841675\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels135\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.72 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 760\n",
      "Label Column: 760\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (9.580162087, -12.8383374757, -2.43848, 2.79552)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60881.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.48 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 445 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 445 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t445 features in original data used to generate 445 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.17 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.524678\n",
      "[2000]\tvalid_set's rmse: 0.521883\n",
      "[3000]\tvalid_set's rmse: 0.521039\n",
      "[4000]\tvalid_set's rmse: 0.520972\n",
      "[5000]\tvalid_set's rmse: 0.520947\n",
      "[6000]\tvalid_set's rmse: 0.520935\n",
      "[7000]\tvalid_set's rmse: 0.520934\n",
      "[8000]\tvalid_set's rmse: 0.52093\n",
      "[9000]\tvalid_set's rmse: 0.52093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5209\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.14s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 157.1s of the 157.1s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.495936\n",
      "[2000]\tvalid_set's rmse: 0.494929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4949\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 148.5s of the 148.5s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8492.\n",
      "\t-0.4642\t = Validation score   (-root_mean_squared_error)\n",
      "\t148.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the -0.75s of remaining time.\n",
      "\t-0.4627\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.38s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels135\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14647941934456787\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14647941934456787,\n",
      "    \"mean_squared_error\": -0.02145622029152172,\n",
      "    \"mean_absolute_error\": -0.03985695510148761,\n",
      "    \"r2\": 0.9972541968431982,\n",
      "    \"pearsonr\": 0.9986268405911047,\n",
      "    \"median_absolute_error\": -0.004065204509503179\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels136\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.64 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 761\n",
      "Label Column: 761\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.3273046445, -10.4920611368, 4.21674, 3.81485)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60887.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.56 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 446 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 446 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t446 features in original data used to generate 446 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.25 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.45s of the 179.45s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.840335\n",
      "[2000]\tvalid_set's rmse: 0.83609\n",
      "[3000]\tvalid_set's rmse: 0.835615\n",
      "[4000]\tvalid_set's rmse: 0.835538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8355\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.03s of the 169.03s of remaining time.\n",
      "\t-0.7992\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 165.54s of the 165.54s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.799358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7433\t = Validation score   (-root_mean_squared_error)\n",
      "\t88.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 76.11s of the 76.11s of remaining time.\n",
      "\t-0.828\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 54.7s of the 54.7s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.838292\n",
      "[2000]\tvalid_set's rmse: 0.838209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2691. Best iteration is:\n",
      "\t[2590]\tvalid_set's rmse: 0.838208\n",
      "\t-0.8382\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.05s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.45s of the -1.35s of remaining time.\n",
      "\t-0.7428\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.04s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels136\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24749158915464556\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24749158915464556,\n",
      "    \"mean_squared_error\": -0.061252086702291854,\n",
      "    \"mean_absolute_error\": -0.11189230142005628,\n",
      "    \"r2\": 0.9957907283062337,\n",
      "    \"pearsonr\": 0.9979004493056254,\n",
      "    \"median_absolute_error\": -0.053723416362097265\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels137\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.55 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 762\n",
      "Label Column: 762\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (9.2656352594, -11.7517010983, -2.07748, 2.63065)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60976.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.64 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 447 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 447 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t447 features in original data used to generate 447 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.33 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.43s of the 179.43s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.51854\n",
      "[2000]\tvalid_set's rmse: 0.514795\n",
      "[3000]\tvalid_set's rmse: 0.514519\n",
      "[4000]\tvalid_set's rmse: 0.514475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5144\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.21s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.77s of the 167.76s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.50325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5032\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.12s of the 163.11s of remaining time.\n",
      "\t-0.4736\t = Validation score   (-root_mean_squared_error)\n",
      "\t51.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 111.62s of the 111.61s of remaining time.\n",
      "\t-0.5384\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 98.92s of the 98.91s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.537765\n",
      "[2000]\tvalid_set's rmse: 0.537713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5377\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.43s of the 54.72s of remaining time.\n",
      "\t-0.4721\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 125.92s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels137\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18250989518500008\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18250989518500008,\n",
      "    \"mean_squared_error\": -0.033309861840439686,\n",
      "    \"mean_absolute_error\": -0.11352295353356878,\n",
      "    \"r2\": 0.995186204039867,\n",
      "    \"pearsonr\": 0.9976105806982994,\n",
      "    \"median_absolute_error\": -0.07459024924650892\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels138\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.47 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 763\n",
      "Label Column: 763\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.0535072058, -13.8274723604, 3.80734, 3.44068)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60984.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.73 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 448 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 448 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t448 features in original data used to generate 448 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.42 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.694683\n",
      "[2000]\tvalid_set's rmse: 0.690178\n",
      "[3000]\tvalid_set's rmse: 0.6891\n",
      "[4000]\tvalid_set's rmse: 0.689096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.689\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.35s of the 169.35s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.667361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6671\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.62s of the 163.62s of remaining time.\n",
      "\t-0.6097\t = Validation score   (-root_mean_squared_error)\n",
      "\t83.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 79.99s of the 79.99s of remaining time.\n",
      "\t-0.7128\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.14s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 49.37s of the 49.37s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.723708\n",
      "[2000]\tvalid_set's rmse: 0.723652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2426. Best iteration is:\n",
      "\t[2256]\tvalid_set's rmse: 0.723651\n",
      "\t-0.7237\t = Validation score   (-root_mean_squared_error)\n",
      "\t49.71s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the -1.33s of remaining time.\n",
      "\t-0.6097\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.03s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels138\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2056552480786767\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2056552480786767,\n",
      "    \"mean_squared_error\": -0.04229408106230207,\n",
      "    \"mean_absolute_error\": -0.09816211227360166,\n",
      "    \"r2\": 0.9964270119245897,\n",
      "    \"pearsonr\": 0.9982198471778438,\n",
      "    \"median_absolute_error\": -0.04828296526076213\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels139\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.38 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 764\n",
      "Label Column: 764\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.1987752357, -13.9174869309, -1.89653, 2.84218)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60987.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.81 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 449 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 449 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t449 features in original data used to generate 449 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.5 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.641118\n",
      "[2000]\tvalid_set's rmse: 0.63609\n",
      "[3000]\tvalid_set's rmse: 0.635431\n",
      "[4000]\tvalid_set's rmse: 0.635302\n",
      "[5000]\tvalid_set's rmse: 0.635292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6353\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.59s of the 166.59s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.6349\n",
      "[2000]\tvalid_set's rmse: 0.632698\n",
      "[3000]\tvalid_set's rmse: 0.632472\n",
      "[4000]\tvalid_set's rmse: 0.632432\n",
      "[5000]\tvalid_set's rmse: 0.632436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6324\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.47s of the 150.47s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8595.\n",
      "\t-0.5751\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the -0.63s of remaining time.\n",
      "\t-0.5751\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels139\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18233041335631803\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18233041335631803,\n",
      "    \"mean_squared_error\": -0.03324437963468571,\n",
      "    \"mean_absolute_error\": -0.05331072722176211,\n",
      "    \"r2\": 0.9958841727011211,\n",
      "    \"pearsonr\": 0.9979448285357058,\n",
      "    \"median_absolute_error\": -0.007995402020632869\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels140\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.31 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 765\n",
      "Label Column: 765\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.3197041924, -13.6976789103, 3.37189, 3.29342)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60941.54 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.89 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 450 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 450 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t450 features in original data used to generate 450 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.58 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.704686\n",
      "[2000]\tvalid_set's rmse: 0.698855\n",
      "[3000]\tvalid_set's rmse: 0.697837\n",
      "[4000]\tvalid_set's rmse: 0.697718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6977\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.87s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.04s of the 168.04s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.690486\n",
      "[2000]\tvalid_set's rmse: 0.689359\n",
      "[3000]\tvalid_set's rmse: 0.689197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6892\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 157.06s of the 157.05s of remaining time.\n",
      "\t-0.6359\t = Validation score   (-root_mean_squared_error)\n",
      "\t69.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 87.6s of the 87.6s of remaining time.\n",
      "\t-0.728\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.33s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 66.66s of the 66.66s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.719976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7198\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the 28.72s of remaining time.\n",
      "\t-0.6357\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 151.9s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels140\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22410568762040478\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22410568762040478,\n",
      "    \"mean_squared_error\": -0.050223359223814476,\n",
      "    \"mean_absolute_error\": -0.121722724153107,\n",
      "    \"r2\": 0.995369224992937,\n",
      "    \"pearsonr\": 0.9977031848420985,\n",
      "    \"median_absolute_error\": -0.06946067548460699\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels141\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.23 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 766\n",
      "Label Column: 766\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.7389856901, -16.3498803894, -1.65859, 3.13626)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    61013.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.98 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 451 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 451 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t451 features in original data used to generate 451 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.67 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.668845\n",
      "[2000]\tvalid_set's rmse: 0.6637\n",
      "[3000]\tvalid_set's rmse: 0.663221\n",
      "[4000]\tvalid_set's rmse: 0.663161\n",
      "[5000]\tvalid_set's rmse: 0.663111\n",
      "[6000]\tvalid_set's rmse: 0.66308\n",
      "[7000]\tvalid_set's rmse: 0.663069\n",
      "[8000]\tvalid_set's rmse: 0.663069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6631\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.08s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.56s of the 159.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.643701\n",
      "[2000]\tvalid_set's rmse: 0.642122\n",
      "[3000]\tvalid_set's rmse: 0.641962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6419\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 147.37s of the 147.36s of remaining time.\n",
      "\t-0.5958\t = Validation score   (-root_mean_squared_error)\n",
      "\t104.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 41.81s of the 41.8s of remaining time.\n",
      "\t-0.6725\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.11s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 22.5s of the 22.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.670298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1056. Best iteration is:\n",
      "\t[1052]\tvalid_set's rmse: 0.670266\n",
      "\t-0.6703\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the -0.56s of remaining time.\n",
      "\t-0.5955\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels141\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1921743474020881\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1921743474020881,\n",
      "    \"mean_squared_error\": -0.036930979799418534,\n",
      "    \"mean_absolute_error\": -0.07044197761655155,\n",
      "    \"r2\": 0.9962450142367011,\n",
      "    \"pearsonr\": 0.9981268086890953,\n",
      "    \"median_absolute_error\": -0.02528171952622038\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels142\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.13 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 767\n",
      "Label Column: 767\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.0029226317, -10.1457834297, 3.1318, 3.38142)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60867.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.06 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 452 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 452 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t452 features in original data used to generate 452 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.75 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.866779\n",
      "[2000]\tvalid_set's rmse: 0.859861\n",
      "[3000]\tvalid_set's rmse: 0.859072\n",
      "[4000]\tvalid_set's rmse: 0.858918\n",
      "[5000]\tvalid_set's rmse: 0.858862\n",
      "[6000]\tvalid_set's rmse: 0.858882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8589\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.53s of the 163.53s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.781304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7794\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 157.5s of the 157.49s of remaining time.\n",
      "\t-0.7546\t = Validation score   (-root_mean_squared_error)\n",
      "\t128.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 28.53s of the 28.53s of remaining time.\n",
      "\t-0.8393\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.72s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the -0.85s of remaining time.\n",
      "\t-0.7501\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels142\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24014605424664087\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24014605424664087,\n",
      "    \"mean_squared_error\": -0.05767012737023078,\n",
      "    \"mean_absolute_error\": -0.08383318451239061,\n",
      "    \"r2\": 0.9949557896129961,\n",
      "    \"pearsonr\": 0.9974876484105157,\n",
      "    \"median_absolute_error\": -0.02488099236561281\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels143\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   57.07 GB / 2000.36 GB (2.9%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 768\n",
      "Label Column: 768\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.8600365508, -22.0439076657, -1.34668, 3.46666)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60859.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.14 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 453 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 453 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t453 features in original data used to generate 453 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.83 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.944649\n",
      "[2000]\tvalid_set's rmse: 0.937313\n",
      "[3000]\tvalid_set's rmse: 0.936081\n",
      "[4000]\tvalid_set's rmse: 0.935595\n",
      "[5000]\tvalid_set's rmse: 0.935582\n",
      "[6000]\tvalid_set's rmse: 0.935542\n",
      "[7000]\tvalid_set's rmse: 0.935539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9355\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.72s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.7s of the 160.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.900114\n",
      "[2000]\tvalid_set's rmse: 0.898752\n",
      "[3000]\tvalid_set's rmse: 0.898641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8985\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 149.82s of the 149.82s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8474.\n",
      "\t-0.8615\t = Validation score   (-root_mean_squared_error)\n",
      "\t149.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the -0.91s of remaining time.\n",
      "\t-0.8596\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels143\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.27201163165860504\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.27201163165860504,\n",
      "    \"mean_squared_error\": -0.07399032775757697,\n",
      "    \"mean_absolute_error\": -0.06948619539426372,\n",
      "    \"r2\": 0.9938426475784728,\n",
      "    \"pearsonr\": 0.9969187480688346,\n",
      "    \"median_absolute_error\": -0.005642000474523845\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels144\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.99 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 769\n",
      "Label Column: 769\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.7207058778, -12.813404418, 2.94313, 3.41504)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60861.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.23 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 454 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 454 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t454 features in original data used to generate 454 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.92 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.44s of the 179.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.638837\n",
      "[2000]\tvalid_set's rmse: 0.635419\n",
      "[3000]\tvalid_set's rmse: 0.635006\n",
      "[4000]\tvalid_set's rmse: 0.634857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6348\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.36s of the 168.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.600186\n",
      "[2000]\tvalid_set's rmse: 0.598294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5982\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 158.95s of the 158.95s of remaining time.\n",
      "\t-0.5496\t = Validation score   (-root_mean_squared_error)\n",
      "\t79.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 79.32s of the 79.32s of remaining time.\n",
      "\t-0.6524\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 54.46s of the 54.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.663275\n",
      "[2000]\tvalid_set's rmse: 0.663173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2631. Best iteration is:\n",
      "\t[2113]\tvalid_set's rmse: 0.663172\n",
      "\t-0.6632\t = Validation score   (-root_mean_squared_error)\n",
      "\t54.8s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.44s of the -1.34s of remaining time.\n",
      "\t-0.5478\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.01s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels144\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18300277371962756\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18300277371962756,\n",
      "    \"mean_squared_error\": -0.03349001518907731,\n",
      "    \"mean_absolute_error\": -0.08418706510536662,\n",
      "    \"r2\": 0.9971281190515506,\n",
      "    \"pearsonr\": 0.9985684763837936,\n",
      "    \"median_absolute_error\": -0.040049317343139545\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels145\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.90 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 770\n",
      "Label Column: 770\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.9014349603, -21.1406652567, -0.88726, 4.00317)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60867.2 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.31 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 455 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 455 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t455 features in original data used to generate 455 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.0 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.958992\n",
      "[2000]\tvalid_set's rmse: 0.953531\n",
      "[3000]\tvalid_set's rmse: 0.953126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.953\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.88s of the 169.88s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.889073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8888\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.84s of the 164.84s of remaining time.\n",
      "\t-0.8732\t = Validation score   (-root_mean_squared_error)\n",
      "\t56.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 108.02s of the 108.02s of remaining time.\n",
      "\t-0.9519\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.21s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 90.68s of the 90.68s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.953461\n",
      "[2000]\tvalid_set's rmse: 0.953409\n",
      "[3000]\tvalid_set's rmse: 0.953409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9534\t = Validation score   (-root_mean_squared_error)\n",
      "\t63.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the 26.71s of remaining time.\n",
      "\t-0.8629\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 153.92s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels145\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.307248617706428\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.307248617706428,\n",
      "    \"mean_squared_error\": -0.09440171308251025,\n",
      "    \"mean_absolute_error\": -0.16289002450618467,\n",
      "    \"r2\": 0.9941086721000372,\n",
      "    \"pearsonr\": 0.9970780131816517,\n",
      "    \"median_absolute_error\": -0.09772380258776553\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels146\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.81 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 771\n",
      "Label Column: 771\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.0358038169, -14.4954413446, 2.59982, 3.49197)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60867.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.39 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 456 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 456 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t456 features in original data used to generate 456 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.09 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.43s of the 179.43s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.824771\n",
      "[2000]\tvalid_set's rmse: 0.818576\n",
      "[3000]\tvalid_set's rmse: 0.817439\n",
      "[4000]\tvalid_set's rmse: 0.817417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8174\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.94s of the 168.94s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.767846\n",
      "[2000]\tvalid_set's rmse: 0.765839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7657\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.13s of the 160.13s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8912.\n",
      "\t-0.7202\t = Validation score   (-root_mean_squared_error)\n",
      "\t160.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.43s of the -0.69s of remaining time.\n",
      "\t-0.7193\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.31s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels146\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22773251847266993\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22773251847266993,\n",
      "    \"mean_squared_error\": -0.05186209996990505,\n",
      "    \"mean_absolute_error\": -0.06152012412335243,\n",
      "    \"r2\": 0.9957464494136024,\n",
      "    \"pearsonr\": 0.9978758295602064,\n",
      "    \"median_absolute_error\": -0.006587763086004839\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels147\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.75 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 772\n",
      "Label Column: 772\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.7670631214, -28.927627314, -0.18167, 4.61344)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60906.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.48 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 457 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 457 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t457 features in original data used to generate 457 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.17 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.43s of the 179.43s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.5211\n",
      "[2000]\tvalid_set's rmse: 1.51712\n",
      "[3000]\tvalid_set's rmse: 1.51604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.5159\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 170.03s of the 170.02s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.37927\n",
      "[2000]\tvalid_set's rmse: 1.37769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.377\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.7s of the 162.7s of remaining time.\n",
      "\t-1.358\t = Validation score   (-root_mean_squared_error)\n",
      "\t67.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 94.35s of the 94.35s of remaining time.\n",
      "\t-1.4372\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 71.32s of the 71.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.48132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.4812\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.43s of the 38.36s of remaining time.\n",
      "\t-1.3496\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 142.3s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels147\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.4545795640795291\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.4545795640795291,\n",
      "    \"mean_squared_error\": -0.20664258007873418,\n",
      "    \"mean_absolute_error\": -0.20937918520909185,\n",
      "    \"r2\": 0.99029017647817,\n",
      "    \"pearsonr\": 0.9951791183117098,\n",
      "    \"median_absolute_error\": -0.10746012698693541\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels148\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.68 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 773\n",
      "Label Column: 773\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.5026184386, -26.4529766465, 0.13723, 4.85173)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60951.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.56 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 458 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 458 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t458 features in original data used to generate 458 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.25 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.722505\n",
      "[2000]\tvalid_set's rmse: 0.71923\n",
      "[3000]\tvalid_set's rmse: 0.718743\n",
      "[4000]\tvalid_set's rmse: 0.718653\n",
      "[5000]\tvalid_set's rmse: 0.718625\n",
      "[6000]\tvalid_set's rmse: 0.718626\n",
      "[7000]\tvalid_set's rmse: 0.718635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7186\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.12s of the 161.12s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.639083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6385\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.06s of the 156.06s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8670.\n",
      "\t-0.5898\t = Validation score   (-root_mean_squared_error)\n",
      "\t156.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the -0.89s of remaining time.\n",
      "\t-0.5876\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.58s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels148\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1863305422963969\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1863305422963969,\n",
      "    \"mean_squared_error\": -0.03471907099246916,\n",
      "    \"mean_absolute_error\": -0.05053543592378366,\n",
      "    \"r2\": 0.9985249172843451,\n",
      "    \"pearsonr\": 0.9992624593495526,\n",
      "    \"median_absolute_error\": -0.009745563326031548\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels149\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.61 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 774\n",
      "Label Column: 774\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.7273786878, -14.7041862568, 2.09177, 3.57949)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60913.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.64 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 459 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 459 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t459 features in original data used to generate 459 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.34 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.767209\n",
      "[2000]\tvalid_set's rmse: 0.759114\n",
      "[3000]\tvalid_set's rmse: 0.758595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7586\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.04s of the 171.03s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.693444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6924\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.9s of the 164.9s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 9092.\n",
      "\t-0.6293\t = Validation score   (-root_mean_squared_error)\n",
      "\t165.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the -0.77s of remaining time.\n",
      "\t-0.6293\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.37s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels149\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1991527070780382\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1991527070780382,\n",
      "    \"mean_squared_error\": -0.039661800736510974,\n",
      "    \"mean_absolute_error\": -0.05291814837826627,\n",
      "    \"r2\": 0.9969042089685182,\n",
      "    \"pearsonr\": 0.9984543765776703,\n",
      "    \"median_absolute_error\": -0.004594197330587746\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels150\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.55 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 775\n",
      "Label Column: 775\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (31.2451697654, -30.7699464579, 1.96959, 5.03454)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60913.4 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.73 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 460 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 460 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t460 features in original data used to generate 460 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.42 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.81144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.8095\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 175.5s of the 175.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.74558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.7449\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 171.0s of the 171.0s of remaining time.\n",
      "\t-1.7192\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 150.45s of the 150.45s of remaining time.\n",
      "\t-1.7813\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 134.08s of the 134.08s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.77755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.7773\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the 97.06s of remaining time.\n",
      "\t-1.7105\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 83.57s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels150\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.9037621408594142\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.9037621408594142,\n",
      "    \"mean_squared_error\": -0.8167860072507899,\n",
      "    \"mean_absolute_error\": -0.6435067375589749,\n",
      "    \"r2\": 0.9677722057010337,\n",
      "    \"pearsonr\": 0.9843285071925303,\n",
      "    \"median_absolute_error\": -0.46894300597881455\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels151\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.48 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 776\n",
      "Label Column: 776\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.0889027932, -21.8658254386, 2.47688, 4.10669)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60894.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.81 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 461 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 461 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t461 features in original data used to generate 461 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.5 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.43s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.918434\n",
      "[2000]\tvalid_set's rmse: 0.916231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9155\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.86s of the 173.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.897369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8969\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 169.5s of the 169.49s of remaining time.\n",
      "\t-0.8609\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 141.01s of the 141.01s of remaining time.\n",
      "\t-0.927\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.52s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 125.07s of the 125.07s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.939118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.939\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.43s of the 83.34s of remaining time.\n",
      "\t-0.8546\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 97.32s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels151\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.4062644675212085\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.4062644675212085,\n",
      "    \"mean_squared_error\": -0.16505081757029175,\n",
      "    \"mean_absolute_error\": -0.28805614204026964,\n",
      "    \"r2\": 0.9902124150832268,\n",
      "    \"pearsonr\": 0.995171508640739,\n",
      "    \"median_absolute_error\": -0.21441833302757113\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels152\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.41 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 777\n",
      "Label Column: 777\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.337133339, -15.9866891704, 2.26711, 3.2238)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60947.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.9 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 462 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 462 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t462 features in original data used to generate 462 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.59 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.820259\n",
      "[2000]\tvalid_set's rmse: 0.818976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8185\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.77s of the 173.77s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.802676\n",
      "[2000]\tvalid_set's rmse: 0.801593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8016\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.46s of the 164.46s of remaining time.\n",
      "\t-0.767\t = Validation score   (-root_mean_squared_error)\n",
      "\t92.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 71.42s of the 71.42s of remaining time.\n",
      "\t-0.8391\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.19s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 58.13s of the 58.13s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.868686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8686\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the 26.39s of remaining time.\n",
      "\t-0.7648\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 154.34s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels152\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24954918130780765\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24954918130780765,\n",
      "    \"mean_squared_error\": -0.062274793891396854,\n",
      "    \"mean_absolute_error\": -0.10150392789837921,\n",
      "    \"r2\": 0.9940073725775176,\n",
      "    \"pearsonr\": 0.9970096978273018,\n",
      "    \"median_absolute_error\": -0.041016049541259925\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels153\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.34 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 778\n",
      "Label Column: 778\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.2177270507, -9.7622583143, 1.47822, 2.83024)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60845.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 64.98 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 463 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 463 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t463 features in original data used to generate 463 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.67 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.727723\n",
      "[2000]\tvalid_set's rmse: 0.725419\n",
      "[3000]\tvalid_set's rmse: 0.724588\n",
      "[4000]\tvalid_set's rmse: 0.724392\n",
      "[5000]\tvalid_set's rmse: 0.724367\n",
      "[6000]\tvalid_set's rmse: 0.724379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7244\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.13s of the 163.12s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.730899\n",
      "[2000]\tvalid_set's rmse: 0.729903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7298\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.56s of the 155.55s of remaining time.\n",
      "\t-0.6727\t = Validation score   (-root_mean_squared_error)\n",
      "\t52.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 102.57s of the 102.56s of remaining time.\n",
      "\t-0.7767\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 85.4s of the 85.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.791792\n",
      "[2000]\tvalid_set's rmse: 0.791697\n",
      "[3000]\tvalid_set's rmse: 0.791696\n",
      "[4000]\tvalid_set's rmse: 0.791696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 4069. Best iteration is:\n",
      "\t[3455]\tvalid_set's rmse: 0.791696\n",
      "\t-0.7917\t = Validation score   (-root_mean_squared_error)\n",
      "\t85.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the -1.76s of remaining time.\n",
      "\t-0.6727\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.43s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels153\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2666720793939434\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2666720793939434,\n",
      "    \"mean_squared_error\": -0.07111399792828965,\n",
      "    \"mean_absolute_error\": -0.16821075334987823,\n",
      "    \"r2\": 0.9911212694638105,\n",
      "    \"pearsonr\": 0.9956220888033432,\n",
      "    \"median_absolute_error\": -0.11448602677396846\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels154\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.23 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 779\n",
      "Label Column: 779\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.1959039825, -15.2445683739, 0.9892, 3.1725)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60916.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.06 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 464 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 464 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t464 features in original data used to generate 464 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.75 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.748094\n",
      "[2000]\tvalid_set's rmse: 0.74332\n",
      "[3000]\tvalid_set's rmse: 0.74279\n",
      "[4000]\tvalid_set's rmse: 0.742756\n",
      "[5000]\tvalid_set's rmse: 0.742715\n",
      "[6000]\tvalid_set's rmse: 0.742729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7427\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.55s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.92s of the 162.92s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.728771\n",
      "[2000]\tvalid_set's rmse: 0.727514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7273\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 152.63s of the 152.63s of remaining time.\n",
      "\t-0.6813\t = Validation score   (-root_mean_squared_error)\n",
      "\t84.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 68.25s of the 68.25s of remaining time.\n",
      "\t-0.7632\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.94s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 34.86s of the 34.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.773865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1612. Best iteration is:\n",
      "\t[1612]\tvalid_set's rmse: 0.773787\n",
      "\t-0.7738\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the -1.05s of remaining time.\n",
      "\t-0.6803\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.67s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels154\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22552491813316156\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22552491813316156,\n",
      "    \"mean_squared_error\": -0.05086148869896901,\n",
      "    \"mean_absolute_error\": -0.09979256414046916,\n",
      "    \"r2\": 0.9949460932862256,\n",
      "    \"pearsonr\": 0.997479990201901,\n",
      "    \"median_absolute_error\": -0.04595540528845826\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels155\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.14 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 780\n",
      "Label Column: 780\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.3737515831, -13.5923269585, 1.10927, 3.47673)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60908.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.15 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 465 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 465 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t465 features in original data used to generate 465 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.84 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.888198\n",
      "[2000]\tvalid_set's rmse: 0.881113\n",
      "[3000]\tvalid_set's rmse: 0.880536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8804\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.3s of the 171.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.825169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8247\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 166.97s of the 166.97s of remaining time.\n",
      "\t-0.7859\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 31.12s of the 31.12s of remaining time.\n",
      "\t-0.8667\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.38s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 13.65s of the 13.64s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 591. Best iteration is:\n",
      "\t[591]\tvalid_set's rmse: 0.864436\n",
      "\t-0.8644\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the -0.39s of remaining time.\n",
      "\t-0.785\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.03s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels155\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.25126059173677023\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.25126059173677023,\n",
      "    \"mean_squared_error\": -0.06313188495991233,\n",
      "    \"mean_absolute_error\": -0.08316658420001777,\n",
      "    \"r2\": 0.9947766510557691,\n",
      "    \"pearsonr\": 0.9973957399329934,\n",
      "    \"median_absolute_error\": -0.026214807719125377\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels156\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   56.07 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 781\n",
      "Label Column: 781\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.6070393655, -16.055867415, 1.28079, 3.68512)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60916.47 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.23 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 466 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 466 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t466 features in original data used to generate 466 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 38.92 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.961281\n",
      "[2000]\tvalid_set's rmse: 0.955159\n",
      "[3000]\tvalid_set's rmse: 0.954348\n",
      "[4000]\tvalid_set's rmse: 0.95402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.954\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.89s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.05s of the 167.05s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.864656\n",
      "[2000]\tvalid_set's rmse: 0.863108\n",
      "[3000]\tvalid_set's rmse: 0.862938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8629\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.13s of the 155.13s of remaining time.\n",
      "\t-0.8394\t = Validation score   (-root_mean_squared_error)\n",
      "\t71.78s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 82.93s of the 82.93s of remaining time.\n",
      "\t-0.9065\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.79s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 62.03s of the 62.03s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.911774\n",
      "[2000]\tvalid_set's rmse: 0.911705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2896. Best iteration is:\n",
      "\t[2499]\tvalid_set's rmse: 0.911704\n",
      "\t-0.9117\t = Validation score   (-root_mean_squared_error)\n",
      "\t62.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -1.48s of remaining time.\n",
      "\t-0.8352\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels156\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2772902076015181\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2772902076015181,\n",
      "    \"mean_squared_error\": -0.07688985923169306,\n",
      "    \"mean_absolute_error\": -0.11699740447464411,\n",
      "    \"r2\": 0.99433751824442,\n",
      "    \"pearsonr\": 0.9971908326928817,\n",
      "    \"median_absolute_error\": -0.057115164247784456\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels157\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.97 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 782\n",
      "Label Column: 782\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.2538558303, -16.240522288, 1.58108, 3.80637)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60939.1 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.31 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 467 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 467 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t467 features in original data used to generate 467 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.0 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.43s of the 179.43s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.920756\n",
      "[2000]\tvalid_set's rmse: 0.915146\n",
      "[3000]\tvalid_set's rmse: 0.914897\n",
      "[4000]\tvalid_set's rmse: 0.914693\n",
      "[5000]\tvalid_set's rmse: 0.914673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9146\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.92s of the 166.92s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.804649\n",
      "[2000]\tvalid_set's rmse: 0.803414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8031\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.11s of the 159.1s of remaining time.\n",
      "\t-0.7858\t = Validation score   (-root_mean_squared_error)\n",
      "\t84.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 74.32s of the 74.32s of remaining time.\n",
      "\t-0.8466\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 51.82s of the 51.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.865497\n",
      "[2000]\tvalid_set's rmse: 0.865453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2419. Best iteration is:\n",
      "\t[2297]\tvalid_set's rmse: 0.865453\n",
      "\t-0.8655\t = Validation score   (-root_mean_squared_error)\n",
      "\t52.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.43s of the -1.29s of remaining time.\n",
      "\t-0.7774\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.04s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels157\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2532297471604794\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2532297471604794,\n",
      "    \"mean_squared_error\": -0.06412530484696048,\n",
      "    \"mean_absolute_error\": -0.09684305516734458,\n",
      "    \"r2\": 0.9955736272584547,\n",
      "    \"pearsonr\": 0.9978018223237383,\n",
      "    \"median_absolute_error\": -0.0402988905724366\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels158\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.88 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 783\n",
      "Label Column: 783\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.7590777937, -11.6872367524, 0.42496, 2.93042)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60839.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.4 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 468 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 468 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t468 features in original data used to generate 468 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.09 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.668852\n",
      "[2000]\tvalid_set's rmse: 0.666092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6655\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.62s of the 173.62s of remaining time.\n",
      "\t-0.5976\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 170.46s of the 170.46s of remaining time.\n",
      "\t-0.5811\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 136.79s of the 136.79s of remaining time.\n",
      "\t-0.6107\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.38s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 121.32s of the 121.31s of remaining time.\n",
      "\t-0.6183\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the 102.11s of remaining time.\n",
      "\t-0.5791\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 78.58s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels158\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2927036691625148\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2927036691625148,\n",
      "    \"mean_squared_error\": -0.08567543794119911,\n",
      "    \"mean_absolute_error\": -0.21751832983923244,\n",
      "    \"r2\": 0.9900221225348653,\n",
      "    \"pearsonr\": 0.9950725953391821,\n",
      "    \"median_absolute_error\": -0.17109841162915046\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels159\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.82 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 784\n",
      "Label Column: 784\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.7785073162, -17.4876594877, -0.13087, 2.71444)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60947.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.48 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 469 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 469 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t469 features in original data used to generate 469 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.17 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.50153\n",
      "[2000]\tvalid_set's rmse: 0.497551\n",
      "[3000]\tvalid_set's rmse: 0.49711\n",
      "[4000]\tvalid_set's rmse: 0.496966\n",
      "[5000]\tvalid_set's rmse: 0.496927\n",
      "[6000]\tvalid_set's rmse: 0.496922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4969\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.33s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.2s of the 164.2s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.465838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4657\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.45s of the 159.45s of remaining time.\n",
      "\t-0.4421\t = Validation score   (-root_mean_squared_error)\n",
      "\t124.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 34.44s of the 34.43s of remaining time.\n",
      "\t-0.4747\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.44s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 12.56s of the 12.56s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 545. Best iteration is:\n",
      "\t[540]\tvalid_set's rmse: 0.472302\n",
      "\t-0.4723\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.72s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.39s of remaining time.\n",
      "\t-0.4375\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.04s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels159\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14043539086937065\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14043539086937065,\n",
      "    \"mean_squared_error\": -0.01972209900863289,\n",
      "    \"mean_absolute_error\": -0.050617403307321444,\n",
      "    \"r2\": 0.9973230785374997,\n",
      "    \"pearsonr\": 0.9986646564766395,\n",
      "    \"median_absolute_error\": -0.016378066525378432\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels160\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.75 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 785\n",
      "Label Column: 785\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.8613491429, -23.2830675914, -0.70134, 2.71716)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60842.6 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.56 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 470 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 470 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t470 features in original data used to generate 470 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.25 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.465875\n",
      "[2000]\tvalid_set's rmse: 0.463158\n",
      "[3000]\tvalid_set's rmse: 0.462898\n",
      "[4000]\tvalid_set's rmse: 0.46286\n",
      "[5000]\tvalid_set's rmse: 0.462882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4628\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.58s of the 166.58s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.424482\n",
      "[2000]\tvalid_set's rmse: 0.424188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.424\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.47s of the 159.46s of remaining time.\n",
      "\t-0.3884\t = Validation score   (-root_mean_squared_error)\n",
      "\t56.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 103.07s of the 103.07s of remaining time.\n",
      "\t-0.4515\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 72.02s of the 72.01s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.448351\n",
      "[2000]\tvalid_set's rmse: 0.448324\n",
      "[3000]\tvalid_set's rmse: 0.448323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4483\t = Validation score   (-root_mean_squared_error)\n",
      "\t71.1s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the -0.31s of remaining time.\n",
      "\t-0.387\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.02s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels160\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14653942086061655\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14653942086061655,\n",
      "    \"mean_squared_error\": -0.021473801866164936,\n",
      "    \"mean_absolute_error\": -0.0895040668754679,\n",
      "    \"r2\": 0.9970911491154739,\n",
      "    \"pearsonr\": 0.9985547430068276,\n",
      "    \"median_absolute_error\": -0.058288849378063945\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels161\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.65 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 786\n",
      "Label Column: 786\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.9696477959, -20.2268766619, -0.98712, 2.86979)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60915.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.65 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 471 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 471 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t471 features in original data used to generate 471 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.34 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.484364\n",
      "[2000]\tvalid_set's rmse: 0.480237\n",
      "[3000]\tvalid_set's rmse: 0.479784\n",
      "[4000]\tvalid_set's rmse: 0.479599\n",
      "[5000]\tvalid_set's rmse: 0.479559\n",
      "[6000]\tvalid_set's rmse: 0.479563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4796\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.81s of the 162.81s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.413338\n",
      "[2000]\tvalid_set's rmse: 0.412562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4125\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 153.05s of the 153.04s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8296.\n",
      "\t-0.3795\t = Validation score   (-root_mean_squared_error)\n",
      "\t153.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the -0.92s of remaining time.\n",
      "\t-0.3773\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.57s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels161\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11942952519201842\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11942952519201842,\n",
      "    \"mean_squared_error\": -0.014263411487591011,\n",
      "    \"mean_absolute_error\": -0.031901499003045175,\n",
      "    \"r2\": 0.9982679370032972,\n",
      "    \"pearsonr\": 0.9991341557087189,\n",
      "    \"median_absolute_error\": -0.003213204700299066\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels162\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.58 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 787\n",
      "Label Column: 787\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.0554509772, -14.0516247041, -0.98739, 2.88706)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60918.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.73 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 472 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 472 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t472 features in original data used to generate 472 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.42 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.453552\n",
      "[2000]\tvalid_set's rmse: 0.449676\n",
      "[3000]\tvalid_set's rmse: 0.449215\n",
      "[4000]\tvalid_set's rmse: 0.449071\n",
      "[5000]\tvalid_set's rmse: 0.449023\n",
      "[6000]\tvalid_set's rmse: 0.449019\n",
      "[7000]\tvalid_set's rmse: 0.44901\n",
      "[8000]\tvalid_set's rmse: 0.44901\n",
      "[9000]\tvalid_set's rmse: 0.449009\n",
      "[10000]\tvalid_set's rmse: 0.449009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.449\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.33s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 155.07s of the 155.07s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.409153\n",
      "[2000]\tvalid_set's rmse: 0.408338\n",
      "[3000]\tvalid_set's rmse: 0.408275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4083\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.58s of the 144.58s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7885.\n",
      "\t-0.3771\t = Validation score   (-root_mean_squared_error)\n",
      "\t144.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.43s of the -0.87s of remaining time.\n",
      "\t-0.3754\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.55s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels162\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11899254960707067\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11899254960707067,\n",
      "    \"mean_squared_error\": -0.014159226861991142,\n",
      "    \"mean_absolute_error\": -0.033628347704227624,\n",
      "    \"r2\": 0.9983010948746937,\n",
      "    \"pearsonr\": 0.999150203662633,\n",
      "    \"median_absolute_error\": -0.004973308935424914\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels163\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.50 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 788\n",
      "Label Column: 788\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.0481151054, -13.8783336725, -0.8284, 2.83562)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60896.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.81 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 473 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 473 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t473 features in original data used to generate 473 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.51 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.508006\n",
      "[2000]\tvalid_set's rmse: 0.504853\n",
      "[3000]\tvalid_set's rmse: 0.504698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5046\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.56s of the 169.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.45206\n",
      "[2000]\tvalid_set's rmse: 0.450616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4506\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.4s of the 160.39s of remaining time.\n",
      "\t-0.4129\t = Validation score   (-root_mean_squared_error)\n",
      "\t121.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 38.32s of the 38.32s of remaining time.\n",
      "\t-0.48\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.52s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 21.39s of the 21.39s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 951. Best iteration is:\n",
      "\t[950]\tvalid_set's rmse: 0.486735\n",
      "\t-0.4867\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.82s of remaining time.\n",
      "\t-0.4121\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.47s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels163\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13215861784582994\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13215861784582994,\n",
      "    \"mean_squared_error\": -0.017465900270920074,\n",
      "    \"mean_absolute_error\": -0.0462029432799549,\n",
      "    \"r2\": 0.9978276206080812,\n",
      "    \"pearsonr\": 0.9989133967196575,\n",
      "    \"median_absolute_error\": -0.014243517079711943\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels164\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.42 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 789\n",
      "Label Column: 789\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.2407037729, -15.9066966167, -0.50365, 2.98299)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60811.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.9 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 474 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 474 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t474 features in original data used to generate 474 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.59 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.517309\n",
      "[2000]\tvalid_set's rmse: 0.51108\n",
      "[3000]\tvalid_set's rmse: 0.510449\n",
      "[4000]\tvalid_set's rmse: 0.510347\n",
      "[5000]\tvalid_set's rmse: 0.510329\n",
      "[6000]\tvalid_set's rmse: 0.510315\n",
      "[7000]\tvalid_set's rmse: 0.510319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5103\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.93s of the 160.93s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.485214\n",
      "[2000]\tvalid_set's rmse: 0.484126\n",
      "[3000]\tvalid_set's rmse: 0.484\n",
      "[4000]\tvalid_set's rmse: 0.48397\n",
      "[5000]\tvalid_set's rmse: 0.483967\n",
      "[6000]\tvalid_set's rmse: 0.483966\n",
      "[7000]\tvalid_set's rmse: 0.483966\n",
      "[8000]\tvalid_set's rmse: 0.483966\n",
      "[9000]\tvalid_set's rmse: 0.483966\n",
      "[10000]\tvalid_set's rmse: 0.483966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.484\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.48s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 127.2s of the 127.2s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7026.\n",
      "\t-0.4211\t = Validation score   (-root_mean_squared_error)\n",
      "\t127.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.85s of remaining time.\n",
      "\t-0.4211\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.5s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels164\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13373524536777587\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13373524536777587,\n",
      "    \"mean_squared_error\": -0.017885115853579228,\n",
      "    \"mean_absolute_error\": -0.04043580678764589,\n",
      "    \"r2\": 0.997989849941026,\n",
      "    \"pearsonr\": 0.9989945973475811,\n",
      "    \"median_absolute_error\": -0.007722503343511877\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels165\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.33 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 790\n",
      "Label Column: 790\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.1823118186, -15.1739794622, 0.15295, 3.18444)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60886.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 65.98 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 475 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 475 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t475 features in original data used to generate 475 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.67 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.656556\n",
      "[2000]\tvalid_set's rmse: 0.652446\n",
      "[3000]\tvalid_set's rmse: 0.652262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6521\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 170.28s of the 170.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.575654\n",
      "[2000]\tvalid_set's rmse: 0.573987\n",
      "[3000]\tvalid_set's rmse: 0.573895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5739\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.37s of the 159.37s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8531.\n",
      "\t-0.5498\t = Validation score   (-root_mean_squared_error)\n",
      "\t159.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the -0.65s of remaining time.\n",
      "\t-0.5458\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.32s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels165\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17271672500780832\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17271672500780832,\n",
      "    \"mean_squared_error\": -0.029831067097422843,\n",
      "    \"mean_absolute_error\": -0.045397116772658314,\n",
      "    \"r2\": 0.9970579875917686,\n",
      "    \"pearsonr\": 0.9985288452938256,\n",
      "    \"median_absolute_error\": -0.003946948496423408\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels166\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.27 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 791\n",
      "Label Column: 791\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.8767449033, -11.8349618585, 0.5685, 3.38017)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60910.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.06 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 476 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 476 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t476 features in original data used to generate 476 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.76 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.779088\n",
      "[2000]\tvalid_set's rmse: 0.775299\n",
      "[3000]\tvalid_set's rmse: 0.774366\n",
      "[4000]\tvalid_set's rmse: 0.774051\n",
      "[5000]\tvalid_set's rmse: 0.774041\n",
      "[6000]\tvalid_set's rmse: 0.774004\n",
      "[7000]\tvalid_set's rmse: 0.774005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.774\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.67s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.03s of the 161.03s of remaining time.\n",
      "\t-0.7055\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 157.48s of the 157.48s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8458.\n",
      "\t-0.6857\t = Validation score   (-root_mean_squared_error)\n",
      "\t157.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.9s of remaining time.\n",
      "\t-0.6813\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels166\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22720019009175163\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22720019009175163,\n",
      "    \"mean_squared_error\": -0.05161992637772809,\n",
      "    \"mean_absolute_error\": -0.10629535552412393,\n",
      "    \"r2\": 0.9954816342876756,\n",
      "    \"pearsonr\": 0.9977585260620943,\n",
      "    \"median_absolute_error\": -0.05481949425078092\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels167\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.20 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 792\n",
      "Label Column: 792\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.6257779674, -10.2750623728, 0.743, 3.50439)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60933.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.15 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 477 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 477 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t477 features in original data used to generate 477 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.84 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.327954\n",
      "[2000]\tvalid_set's rmse: 0.325275\n",
      "[3000]\tvalid_set's rmse: 0.324979\n",
      "[4000]\tvalid_set's rmse: 0.324911\n",
      "[5000]\tvalid_set's rmse: 0.324873\n",
      "[6000]\tvalid_set's rmse: 0.324874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3249\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.2s of the 163.2s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.32023\n",
      "[2000]\tvalid_set's rmse: 0.319801\n",
      "[3000]\tvalid_set's rmse: 0.319778\n",
      "[4000]\tvalid_set's rmse: 0.319758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3198\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 147.34s of the 147.34s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7997.\n",
      "\t-0.2699\t = Validation score   (-root_mean_squared_error)\n",
      "\t147.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.66s of remaining time.\n",
      "\t-0.2678\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.37s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels167\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.08474662499604901\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.08474662499604901,\n",
      "    \"mean_squared_error\": -0.007181990448220963,\n",
      "    \"mean_absolute_error\": -0.022351363457612107,\n",
      "    \"r2\": 0.9994151275656107,\n",
      "    \"pearsonr\": 0.9997075264787123,\n",
      "    \"median_absolute_error\": -0.0018712387928467855\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels168\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.13 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 793\n",
      "Label Column: 793\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.9681062096, -18.0684207425, 1.69493, 3.39493)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60891.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.23 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 478 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 478 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t478 features in original data used to generate 478 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 39.92 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.42s of the 179.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.662495\n",
      "[2000]\tvalid_set's rmse: 0.656733\n",
      "[3000]\tvalid_set's rmse: 0.656454\n",
      "[4000]\tvalid_set's rmse: 0.656373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6563\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.42s of the 167.42s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.596515\n",
      "[2000]\tvalid_set's rmse: 0.595272\n",
      "[3000]\tvalid_set's rmse: 0.595184\n",
      "[4000]\tvalid_set's rmse: 0.595151\n",
      "[5000]\tvalid_set's rmse: 0.59514\n",
      "[6000]\tvalid_set's rmse: 0.595139\n",
      "[7000]\tvalid_set's rmse: 0.595139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5951\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 143.25s of the 143.25s of remaining time.\n",
      "\t-0.5458\t = Validation score   (-root_mean_squared_error)\n",
      "\t121.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 21.34s of the 21.33s of remaining time.\n",
      "\t-0.6397\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.5s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.42s of the -0.31s of remaining time.\n",
      "\t-0.545\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.05s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels168\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17459702504626848\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17459702504626848,\n",
      "    \"mean_squared_error\": -0.03048412115500728,\n",
      "    \"mean_absolute_error\": -0.05960869774621013,\n",
      "    \"r2\": 0.9973548306564759,\n",
      "    \"pearsonr\": 0.9986816278890212,\n",
      "    \"median_absolute_error\": -0.018032475713574136\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels169\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   55.05 GB / 2000.36 GB (2.8%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 794\n",
      "Label Column: 794\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (25.2352714213, -16.5549621566, 2.61008, 3.49456)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60832.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.32 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 479 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 479 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t479 features in original data used to generate 479 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.01 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.992228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9895\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.91s of the 173.91s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.893755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8931\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.57s of the 167.57s of remaining time.\n",
      "\t-0.8716\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 127.96s of the 127.95s of remaining time.\n",
      "\t-0.9299\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 115.86s of the 115.86s of remaining time.\n",
      "\t-0.9337\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the 93.75s of remaining time.\n",
      "\t-0.8666\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 86.94s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels169\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.364520816214157\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.364520816214157,\n",
      "    \"mean_squared_error\": -0.1328754254534356,\n",
      "    \"mean_absolute_error\": -0.246337508629839,\n",
      "    \"r2\": 0.9891181654573675,\n",
      "    \"pearsonr\": 0.9946945104567597,\n",
      "    \"median_absolute_error\": -0.1784623647310668\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels170\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.99 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 795\n",
      "Label Column: 795\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.6035432925, -13.2563285217, -0.04926, 3.27852)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60936.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.4 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 480 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 480 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t480 features in original data used to generate 480 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.09 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.800816\n",
      "[2000]\tvalid_set's rmse: 0.79613\n",
      "[3000]\tvalid_set's rmse: 0.795556\n",
      "[4000]\tvalid_set's rmse: 0.795345\n",
      "[5000]\tvalid_set's rmse: 0.795243\n",
      "[6000]\tvalid_set's rmse: 0.795241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7952\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.48s of the 161.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.733376\n",
      "[2000]\tvalid_set's rmse: 0.732859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7327\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 152.69s of the 152.69s of remaining time.\n",
      "\t-0.6944\t = Validation score   (-root_mean_squared_error)\n",
      "\t119.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 33.03s of the 33.03s of remaining time.\n",
      "\t-0.7572\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 14.86s of the 14.86s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 632. Best iteration is:\n",
      "\t[630]\tvalid_set's rmse: 0.765892\n",
      "\t-0.7659\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.42s of remaining time.\n",
      "\t-0.6933\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.12s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels170\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2225291281919149\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2225291281919149,\n",
      "    \"mean_squared_error\": -0.04951921289385382,\n",
      "    \"mean_absolute_error\": -0.07901672358536277,\n",
      "    \"r2\": 0.9953925701029498,\n",
      "    \"pearsonr\": 0.997704059662898,\n",
      "    \"median_absolute_error\": -0.025120600056329323\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels171\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.91 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 796\n",
      "Label Column: 796\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.8312507293, -18.0495880953, 3.19418, 3.69963)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60812.2 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.48 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 481 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 481 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t481 features in original data used to generate 481 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.17 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.589594\n",
      "[2000]\tvalid_set's rmse: 0.586688\n",
      "[3000]\tvalid_set's rmse: 0.586578\n",
      "[4000]\tvalid_set's rmse: 0.586505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5865\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.64s of the 167.63s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.514778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5144\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.99s of the 162.99s of remaining time.\n",
      "\t-0.4809\t = Validation score   (-root_mean_squared_error)\n",
      "\t61.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 101.09s of the 101.09s of remaining time.\n",
      "\t-0.5636\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.72s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 79.97s of the 79.97s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.55483\n",
      "[2000]\tvalid_set's rmse: 0.55481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5548\t = Validation score   (-root_mean_squared_error)\n",
      "\t52.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the 26.85s of remaining time.\n",
      "\t-0.4783\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 153.8s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels171\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17270684944886133\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17270684944886133,\n",
      "    \"mean_squared_error\": -0.02982765584655165,\n",
      "    \"mean_absolute_error\": -0.09452533435191071,\n",
      "    \"r2\": 0.9978205628114956,\n",
      "    \"pearsonr\": 0.9989151065557678,\n",
      "    \"median_absolute_error\": -0.05960571111352542\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels172\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.83 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 797\n",
      "Label Column: 797\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.764066902, -13.6624182719, -0.67302, 3.28165)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60811.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.57 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 482 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 482 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t482 features in original data used to generate 482 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.26 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.6s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.4s of the 179.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.488915\n",
      "[2000]\tvalid_set's rmse: 0.486846\n",
      "[3000]\tvalid_set's rmse: 0.486343\n",
      "[4000]\tvalid_set's rmse: 0.486237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4862\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.77s of the 166.77s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.392719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3926\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.55s of the 162.54s of remaining time.\n",
      "\t-0.3601\t = Validation score   (-root_mean_squared_error)\n",
      "\t104.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 57.6s of the 57.6s of remaining time.\n",
      "\t-0.4277\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.12s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 35.04s of the 35.04s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.410114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4101\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.4s of the 3.54s of remaining time.\n",
      "\t-0.3583\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 177.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels172\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11772723695012506\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11772723695012506,\n",
      "    \"mean_squared_error\": -0.013859702319910875,\n",
      "    \"mean_absolute_error\": -0.04812660423681155,\n",
      "    \"r2\": 0.9987129040807114,\n",
      "    \"pearsonr\": 0.999360480515398,\n",
      "    \"median_absolute_error\": -0.022475132333130032\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels173\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.75 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 798\n",
      "Label Column: 798\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.8373124113, -13.546363125, 3.98145, 4.11443)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60808.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.65 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 483 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 483 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t483 features in original data used to generate 483 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.34 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.6s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.4s of the 179.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.19802\n",
      "[2000]\tvalid_set's rmse: 1.19278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1924\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.5s of the 171.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.10298\n",
      "[2000]\tvalid_set's rmse: 1.10162\n",
      "[3000]\tvalid_set's rmse: 1.10167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1015\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 160.42s of the 160.42s of remaining time.\n",
      "\t-1.1\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 118.31s of the 118.3s of remaining time.\n",
      "\t-1.1414\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 90.88s of the 90.88s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.16087\n",
      "[2000]\tvalid_set's rmse: 1.16077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1608\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.4s of the 44.84s of remaining time.\n",
      "\t-1.0855\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 135.85s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels173\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.38839820664424546\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.38839820664424546,\n",
      "    \"mean_squared_error\": -0.15085316692446632,\n",
      "    \"mean_absolute_error\": -0.2137052947695532,\n",
      "    \"r2\": 0.991087966384112,\n",
      "    \"pearsonr\": 0.9956028961476495,\n",
      "    \"median_absolute_error\": -0.13407858648309312\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels174\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.67 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 799\n",
      "Label Column: 799\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.6522731435, -15.7090003197, -1.92325, 3.12693)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60877.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.73 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 484 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 484 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t484 features in original data used to generate 484 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.42 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.934193\n",
      "[2000]\tvalid_set's rmse: 0.929403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.929\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.71s of the 172.71s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.865215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8646\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.42s of the 168.42s of remaining time.\n",
      "\t-0.8355\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 141.93s of the 141.93s of remaining time.\n",
      "\t-0.8954\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 125.5s of the 125.5s of remaining time.\n",
      "\t-0.8789\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the 104.27s of remaining time.\n",
      "\t-0.8345\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 76.5s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels174\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.4450021947248595\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.4450021947248595,\n",
      "    \"mean_squared_error\": -0.1980269533099422,\n",
      "    \"mean_absolute_error\": -0.3310197004293131,\n",
      "    \"r2\": 0.9797450992596519,\n",
      "    \"pearsonr\": 0.9900796987167562,\n",
      "    \"median_absolute_error\": -0.26030384872280266\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels175\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.61 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 800\n",
      "Label Column: 800\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (24.8445915183, -14.4198667876, 4.19554, 4.30704)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60914.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.82 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 485 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 485 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t485 features in original data used to generate 485 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.51 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.6s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.4s of the 179.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.566541\n",
      "[2000]\tvalid_set's rmse: 0.563555\n",
      "[3000]\tvalid_set's rmse: 0.562894\n",
      "[4000]\tvalid_set's rmse: 0.562721\n",
      "[5000]\tvalid_set's rmse: 0.562731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5627\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 165.6s of the 165.6s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.494745\n",
      "[2000]\tvalid_set's rmse: 0.493722\n",
      "[3000]\tvalid_set's rmse: 0.493588\n",
      "[4000]\tvalid_set's rmse: 0.493577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4936\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 151.28s of the 151.28s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8038.\n",
      "\t-0.4321\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.4s of the -0.62s of remaining time.\n",
      "\t-0.4317\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.28s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels175\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1370741826354277\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1370741826354277,\n",
      "    \"mean_squared_error\": -0.018789331545170665,\n",
      "    \"mean_absolute_error\": -0.039291726648488054,\n",
      "    \"r2\": 0.9989870326335214,\n",
      "    \"pearsonr\": 0.9994941680851699,\n",
      "    \"median_absolute_error\": -0.007518958003393195\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels176\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.54 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 801\n",
      "Label Column: 801\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.55066888, -15.6204000396, -2.78845, 3.08275)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60888.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.9 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 486 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 486 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t486 features in original data used to generate 486 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.59 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.602174\n",
      "[2000]\tvalid_set's rmse: 0.59686\n",
      "[3000]\tvalid_set's rmse: 0.596347\n",
      "[4000]\tvalid_set's rmse: 0.596238\n",
      "[5000]\tvalid_set's rmse: 0.596179\n",
      "[6000]\tvalid_set's rmse: 0.596171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5962\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.63s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.2s of the 162.2s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.565517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5644\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 154.73s of the 154.73s of remaining time.\n",
      "\t-0.5451\t = Validation score   (-root_mean_squared_error)\n",
      "\t136.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 17.69s of the 17.68s of remaining time.\n",
      "\t-0.5875\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.84s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.58s of remaining time.\n",
      "\t-0.5389\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.23s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels176\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1723308043601603\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1723308043601603,\n",
      "    \"mean_squared_error\": -0.02969790613141987,\n",
      "    \"mean_absolute_error\": -0.058906141717729184,\n",
      "    \"r2\": 0.9968747131874768,\n",
      "    \"pearsonr\": 0.9984392633422241,\n",
      "    \"median_absolute_error\": -0.01717574786134668\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels177\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.47 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 802\n",
      "Label Column: 802\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (25.2740575197, -12.4334265022, 4.24505, 4.04201)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60928.99 MB\n",
      "\tTrain Data (Original)  Memory Usage: 66.98 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 487 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 487 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t487 features in original data used to generate 487 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.67 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.31979\n",
      "[2000]\tvalid_set's rmse: 1.31343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.3132\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.74s of the 171.74s of remaining time.\n",
      "\t-1.2368\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 167.85s of the 167.85s of remaining time.\n",
      "\t-1.2261\t = Validation score   (-root_mean_squared_error)\n",
      "\t52.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 114.95s of the 114.95s of remaining time.\n",
      "\t-1.2679\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 91.38s of the 91.38s of remaining time.\n",
      "\t-1.2577\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the 70.53s of remaining time.\n",
      "\t-1.2163\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 110.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels177\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.4735142207910094\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.4735142207910094,\n",
      "    \"mean_squared_error\": -0.22421571729131742,\n",
      "    \"mean_absolute_error\": -0.29068774064482017,\n",
      "    \"r2\": 0.9862749743701912,\n",
      "    \"pearsonr\": 0.9932851270946017,\n",
      "    \"median_absolute_error\": -0.19822060673225117\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels178\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.41 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 803\n",
      "Label Column: 803\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (9.8068718688, -13.3429830958, -2.84493, 2.80213)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60911.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.07 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 488 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 488 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t488 features in original data used to generate 488 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.76 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.544917\n",
      "[2000]\tvalid_set's rmse: 0.540645\n",
      "[3000]\tvalid_set's rmse: 0.539992\n",
      "[4000]\tvalid_set's rmse: 0.539893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5399\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.28s of the 167.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.50341\n",
      "[2000]\tvalid_set's rmse: 0.50247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5023\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 158.14s of the 158.14s of remaining time.\n",
      "\t-0.4782\t = Validation score   (-root_mean_squared_error)\n",
      "\t97.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 60.13s of the 60.13s of remaining time.\n",
      "\t-0.5385\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.4s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 31.55s of the 31.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.556053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1372. Best iteration is:\n",
      "\t[1366]\tvalid_set's rmse: 0.556002\n",
      "\t-0.556\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.94s of remaining time.\n",
      "\t-0.4745\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels178\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15429905835373497\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15429905835373497,\n",
      "    \"mean_squared_error\": -0.023808199408849293,\n",
      "    \"mean_absolute_error\": -0.06184873742174986,\n",
      "    \"r2\": 0.9969675675615077,\n",
      "    \"pearsonr\": 0.9984873470693141,\n",
      "    \"median_absolute_error\": -0.024120538146534642\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels179\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.32 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 804\n",
      "Label Column: 804\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.3082171593, -9.0989422366, 3.59229, 3.49343)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60814.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.15 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 489 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 489 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t489 features in original data used to generate 489 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.84 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.783012\n",
      "[2000]\tvalid_set's rmse: 0.777425\n",
      "[3000]\tvalid_set's rmse: 0.776619\n",
      "[4000]\tvalid_set's rmse: 0.776252\n",
      "[5000]\tvalid_set's rmse: 0.776176\n",
      "[6000]\tvalid_set's rmse: 0.776176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7762\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.43s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.0s of the 160.99s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.736506\n",
      "[2000]\tvalid_set's rmse: 0.735776\n",
      "[3000]\tvalid_set's rmse: 0.735611\n",
      "[4000]\tvalid_set's rmse: 0.735603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7356\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.64s of the 144.64s of remaining time.\n",
      "\t-0.6924\t = Validation score   (-root_mean_squared_error)\n",
      "\t66.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 77.74s of the 77.73s of remaining time.\n",
      "\t-0.7722\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 42.1s of the 42.1s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.785271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7852\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the 4.55s of remaining time.\n",
      "\t-0.6912\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 176.1s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels179\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24641473980303122\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24641473980303122,\n",
      "    \"mean_squared_error\": -0.06072022399219539,\n",
      "    \"mean_absolute_error\": -0.13380418490983575,\n",
      "    \"r2\": 0.9950241136248206,\n",
      "    \"pearsonr\": 0.9975334515657313,\n",
      "    \"median_absolute_error\": -0.07866243878376444\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels180\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.23 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 805\n",
      "Label Column: 805\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.4338493795, -12.5930613585, -2.2332, 2.56045)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60909.68 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.23 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 490 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 490 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t490 features in original data used to generate 490 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 40.92 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.544514\n",
      "[2000]\tvalid_set's rmse: 0.539678\n",
      "[3000]\tvalid_set's rmse: 0.538965\n",
      "[4000]\tvalid_set's rmse: 0.538887\n",
      "[5000]\tvalid_set's rmse: 0.538814\n",
      "[6000]\tvalid_set's rmse: 0.538809\n",
      "[7000]\tvalid_set's rmse: 0.538807\n",
      "[8000]\tvalid_set's rmse: 0.538806\n",
      "[9000]\tvalid_set's rmse: 0.538806\n",
      "[10000]\tvalid_set's rmse: 0.538806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5388\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.74s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 153.53s of the 153.53s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.518322\n",
      "[2000]\tvalid_set's rmse: 0.516758\n",
      "[3000]\tvalid_set's rmse: 0.516571\n",
      "[4000]\tvalid_set's rmse: 0.516542\n",
      "[5000]\tvalid_set's rmse: 0.516545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5165\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 135.44s of the 135.44s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7214.\n",
      "\t-0.4745\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.66s of remaining time.\n",
      "\t-0.4742\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.33s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels180\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1502180583565783\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1502180583565783,\n",
      "    \"mean_squared_error\": -0.02256546505642034,\n",
      "    \"mean_absolute_error\": -0.042209823050373935,\n",
      "    \"r2\": 0.9965576518349378,\n",
      "    \"pearsonr\": 0.9982794363831422,\n",
      "    \"median_absolute_error\": -0.00517507649222404\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels181\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.14 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 806\n",
      "Label Column: 806\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.8848902968, -10.741942454, 2.91032, 3.05739)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60917.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.32 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 491 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 491 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t491 features in original data used to generate 491 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.01 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.6s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.4s of the 179.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.645919\n",
      "[2000]\tvalid_set's rmse: 0.642379\n",
      "[3000]\tvalid_set's rmse: 0.64206\n",
      "[4000]\tvalid_set's rmse: 0.64194\n",
      "[5000]\tvalid_set's rmse: 0.641907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6419\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.57s of the 164.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.642465\n",
      "[2000]\tvalid_set's rmse: 0.64152\n",
      "[3000]\tvalid_set's rmse: 0.641341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6413\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 151.06s of the 151.06s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7914.\n",
      "\t-0.5848\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.4s of the -0.88s of remaining time.\n",
      "\t-0.5846\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.55s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels181\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18528089557451619\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18528089557451619,\n",
      "    \"mean_squared_error\": -0.034329010264894884,\n",
      "    \"mean_absolute_error\": -0.05302154351897647,\n",
      "    \"r2\": 0.9963271699648806,\n",
      "    \"pearsonr\": 0.9981627123725976,\n",
      "    \"median_absolute_error\": -0.00729375966844556\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels182\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   54.07 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 807\n",
      "Label Column: 807\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.1583051952, -12.9733061899, -1.72465, 2.69366)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60890.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.4 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 492 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 492 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t492 features in original data used to generate 492 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.09 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.6s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.4s of the 179.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.632529\n",
      "[2000]\tvalid_set's rmse: 0.626698\n",
      "[3000]\tvalid_set's rmse: 0.626335\n",
      "[4000]\tvalid_set's rmse: 0.626234\n",
      "[5000]\tvalid_set's rmse: 0.626197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6262\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.57s of the 163.56s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.625673\n",
      "[2000]\tvalid_set's rmse: 0.624144\n",
      "[3000]\tvalid_set's rmse: 0.623991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6239\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.58s of the 150.58s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7898.\n",
      "\t-0.5585\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.4s of the -0.73s of remaining time.\n",
      "\t-0.5585\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.42s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels182\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17674837373223024\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17674837373223024,\n",
      "    \"mean_squared_error\": -0.031239987616988204,\n",
      "    \"mean_absolute_error\": -0.046553014608399446,\n",
      "    \"r2\": 0.9956940578719559,\n",
      "    \"pearsonr\": 0.997851267257462,\n",
      "    \"median_absolute_error\": -0.004282383316029287\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels183\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.99 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 808\n",
      "Label Column: 808\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.9931117331, -9.6070444741, 2.70327, 3.13676)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60900.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.48 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 493 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 493 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t493 features in original data used to generate 493 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.18 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.39s of the 179.39s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.712455\n",
      "[2000]\tvalid_set's rmse: 0.709915\n",
      "[3000]\tvalid_set's rmse: 0.709278\n",
      "[4000]\tvalid_set's rmse: 0.709072\n",
      "[5000]\tvalid_set's rmse: 0.709058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.709\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.41s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.05s of the 164.04s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.685823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.685\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 157.37s of the 157.37s of remaining time.\n",
      "\t-0.6494\t = Validation score   (-root_mean_squared_error)\n",
      "\t63.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 94.12s of the 94.12s of remaining time.\n",
      "\t-0.72\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.35s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 66.34s of the 66.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.728089\n",
      "[2000]\tvalid_set's rmse: 0.727987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.728\t = Validation score   (-root_mean_squared_error)\n",
      "\t54.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.39s of the 10.83s of remaining time.\n",
      "\t-0.6475\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 169.83s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels183\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24120994708097185\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24120994708097185,\n",
      "    \"mean_squared_error\": -0.058182238570805005,\n",
      "    \"mean_absolute_error\": -0.14424533150143518,\n",
      "    \"r2\": 0.9940861471163064,\n",
      "    \"pearsonr\": 0.9970723252911068,\n",
      "    \"median_absolute_error\": -0.08965232678637713\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels184\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.90 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 809\n",
      "Label Column: 809\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.3607480004, -15.1007769516, -1.33856, 2.98704)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60920.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.57 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 494 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 494 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t494 features in original data used to generate 494 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.26 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.6s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.4s of the 179.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.672091\n",
      "[2000]\tvalid_set's rmse: 0.665262\n",
      "[3000]\tvalid_set's rmse: 0.664426\n",
      "[4000]\tvalid_set's rmse: 0.664493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6644\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.7s of the 167.7s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.63399\n",
      "[2000]\tvalid_set's rmse: 0.632332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6322\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 158.22s of the 158.21s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8183.\n",
      "\t-0.5843\t = Validation score   (-root_mean_squared_error)\n",
      "\t158.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.4s of the -0.67s of remaining time.\n",
      "\t-0.584\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.33s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels184\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18484257234732301\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18484257234732301,\n",
      "    \"mean_squared_error\": -0.03416677655197525,\n",
      "    \"mean_absolute_error\": -0.04785498955317259,\n",
      "    \"r2\": 0.9961703011684981,\n",
      "    \"pearsonr\": 0.9980856795657247,\n",
      "    \"median_absolute_error\": -0.004744887069628945\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels185\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.84 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 810\n",
      "Label Column: 810\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.7843053051, -11.2067367287, 2.93156, 3.45904)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60908.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.65 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 495 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 495 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t495 features in original data used to generate 495 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.34 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.39s of the 179.39s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.81865\n",
      "[2000]\tvalid_set's rmse: 0.811342\n",
      "[3000]\tvalid_set's rmse: 0.810594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8105\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.12s of the 168.12s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.749161\n",
      "[2000]\tvalid_set's rmse: 0.746341\n",
      "[3000]\tvalid_set's rmse: 0.746182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7461\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.46s of the 155.46s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8074.\n",
      "\t-0.7102\t = Validation score   (-root_mean_squared_error)\n",
      "\t155.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.39s of the -0.88s of remaining time.\n",
      "\t-0.7079\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.55s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels185\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22416033899758725\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22416033899758725,\n",
      "    \"mean_squared_error\": -0.05024785757951315,\n",
      "    \"mean_absolute_error\": -0.06204774926959018,\n",
      "    \"r2\": 0.9958000017757881,\n",
      "    \"pearsonr\": 0.9979053537979327,\n",
      "    \"median_absolute_error\": -0.00700397303486322\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels186\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.77 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 811\n",
      "Label Column: 811\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.7820830214, -19.6252283552, -1.06904, 3.35976)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60887.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.73 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 496 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 496 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t496 features in original data used to generate 496 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.750782\n",
      "[2000]\tvalid_set's rmse: 0.745431\n",
      "[3000]\tvalid_set's rmse: 0.745324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7451\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 170.36s of the 170.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.692173\n",
      "[2000]\tvalid_set's rmse: 0.691275\n",
      "[3000]\tvalid_set's rmse: 0.691068\n",
      "[4000]\tvalid_set's rmse: 0.691047\n",
      "[5000]\tvalid_set's rmse: 0.691043\n",
      "[6000]\tvalid_set's rmse: 0.691042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.691\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.96s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 147.49s of the 147.48s of remaining time.\n",
      "\t-0.6322\t = Validation score   (-root_mean_squared_error)\n",
      "\t136.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 10.17s of the 10.17s of remaining time.\n",
      "\t-0.7264\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.27s of remaining time.\n",
      "\t-0.6322\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.97s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels186\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.20192439051946864\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.20192439051946864,\n",
      "    \"mean_squared_error\": -0.04077345948665888,\n",
      "    \"mean_absolute_error\": -0.06653950121705142,\n",
      "    \"r2\": 0.9963875447856014,\n",
      "    \"pearsonr\": 0.9981947128266877,\n",
      "    \"median_absolute_error\": -0.0180786160459534\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels187\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.70 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 812\n",
      "Label Column: 812\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.7346396783, -10.7814882353, 2.94814, 3.53261)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60765.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.82 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 497 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 497 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t497 features in original data used to generate 497 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.51 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.39s of the 179.39s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.647975\n",
      "[2000]\tvalid_set's rmse: 0.6409\n",
      "[3000]\tvalid_set's rmse: 0.640212\n",
      "[4000]\tvalid_set's rmse: 0.639931\n",
      "[5000]\tvalid_set's rmse: 0.639885\n",
      "[6000]\tvalid_set's rmse: 0.639894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6399\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.36s of the 162.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.619785\n",
      "[2000]\tvalid_set's rmse: 0.618317\n",
      "[3000]\tvalid_set's rmse: 0.618251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6182\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 151.07s of the 151.07s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7834.\n",
      "\t-0.5571\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.39s of the -0.73s of remaining time.\n",
      "\t-0.5567\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.4s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels187\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1763050835082083\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1763050835082083,\n",
      "    \"mean_squared_error\": -0.0310834824708364,\n",
      "    \"mean_absolute_error\": -0.048582364245031304,\n",
      "    \"r2\": 0.9975089706519197,\n",
      "    \"pearsonr\": 0.9987576498299201,\n",
      "    \"median_absolute_error\": -0.0058957525207399275\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels188\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.63 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 813\n",
      "Label Column: 813\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (29.7277702491, -22.1807133873, -0.68675, 3.93839)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60789.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.9 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 498 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 498 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t498 features in original data used to generate 498 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.59 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.41s of the 179.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.767694\n",
      "[2000]\tvalid_set's rmse: 0.764337\n",
      "[3000]\tvalid_set's rmse: 0.763591\n",
      "[4000]\tvalid_set's rmse: 0.763419\n",
      "[5000]\tvalid_set's rmse: 0.763325\n",
      "[6000]\tvalid_set's rmse: 0.763306\n",
      "[7000]\tvalid_set's rmse: 0.763304\n",
      "[8000]\tvalid_set's rmse: 0.763301\n",
      "[9000]\tvalid_set's rmse: 0.7633\n",
      "[10000]\tvalid_set's rmse: 0.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7633\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.16s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 153.27s of the 153.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.752713\n",
      "[2000]\tvalid_set's rmse: 0.7516\n",
      "[3000]\tvalid_set's rmse: 0.75151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7515\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 139.95s of the 139.94s of remaining time.\n",
      "\t-0.6814\t = Validation score   (-root_mean_squared_error)\n",
      "\t107.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 32.67s of the 32.67s of remaining time.\n",
      "\t-0.7982\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.88s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.41s of the -0.68s of remaining time.\n",
      "\t-0.6806\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.37s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels188\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22030420317771857\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22030420317771857,\n",
      "    \"mean_squared_error\": -0.0485339419377696,\n",
      "    \"mean_absolute_error\": -0.08149506193179305,\n",
      "    \"r2\": 0.9968706828279201,\n",
      "    \"pearsonr\": 0.9984396414813783,\n",
      "    \"median_absolute_error\": -0.031167169963647545\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels189\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.54 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 814\n",
      "Label Column: 814\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.3358965897, -11.111955118, 2.62505, 3.57873)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60895.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 67.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 499 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 499 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t499 features in original data used to generate 499 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.39s of the 179.39s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.842531\n",
      "[2000]\tvalid_set's rmse: 0.837461\n",
      "[3000]\tvalid_set's rmse: 0.836594\n",
      "[4000]\tvalid_set's rmse: 0.83647\n",
      "[5000]\tvalid_set's rmse: 0.836398\n",
      "[6000]\tvalid_set's rmse: 0.836364\n",
      "[7000]\tvalid_set's rmse: 0.836353\n",
      "[8000]\tvalid_set's rmse: 0.836354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8364\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 156.09s of the 156.08s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.788353\n",
      "[2000]\tvalid_set's rmse: 0.786357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7862\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 145.2s of the 145.19s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7524.\n",
      "\t-0.7319\t = Validation score   (-root_mean_squared_error)\n",
      "\t145.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.39s of the -0.88s of remaining time.\n",
      "\t-0.7316\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels189\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2316199772558603\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2316199772558603,\n",
      "    \"mean_squared_error\": -0.05364781386400551,\n",
      "    \"mean_absolute_error\": -0.06130881347206491,\n",
      "    \"r2\": 0.9958107462647618,\n",
      "    \"pearsonr\": 0.9979090830684032,\n",
      "    \"median_absolute_error\": -0.0067040278745484905\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels190\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.46 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 815\n",
      "Label Column: 815\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (24.3838855211, -28.5980312285, -0.04548, 4.33885)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60850.1 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.07 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 500 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 500 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t500 features in original data used to generate 500 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.76 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.38s of the 179.38s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.01666\n",
      "[2000]\tvalid_set's rmse: 1.01149\n",
      "[3000]\tvalid_set's rmse: 1.00988\n",
      "[4000]\tvalid_set's rmse: 1.00963\n",
      "[5000]\tvalid_set's rmse: 1.00956\n",
      "[6000]\tvalid_set's rmse: 1.00956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0096\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.5s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.28s of the 162.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.926375\n",
      "[2000]\tvalid_set's rmse: 0.923605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9234\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 154.34s of the 154.34s of remaining time.\n",
      "\t-0.8592\t = Validation score   (-root_mean_squared_error)\n",
      "\t111.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 42.41s of the 42.41s of remaining time.\n",
      "\t-0.9577\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 19.0s of the 18.99s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 790. Best iteration is:\n",
      "\t[789]\tvalid_set's rmse: 0.979943\n",
      "\t-0.9799\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.38s of the -0.76s of remaining time.\n",
      "\t-0.8589\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.49s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels190\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2784860533725931\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2784860533725931,\n",
      "    \"mean_squared_error\": -0.07755448192304287,\n",
      "    \"mean_absolute_error\": -0.10473786575646289,\n",
      "    \"r2\": 0.9958799854575341,\n",
      "    \"pearsonr\": 0.9979485327852184,\n",
      "    \"median_absolute_error\": -0.04007666014100958\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels191\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.38 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 816\n",
      "Label Column: 816\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.6605422955, -13.9763786586, 2.35504, 3.55851)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60895.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.15 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 501 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 501 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t501 features in original data used to generate 501 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.84 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.38s of the 179.38s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.69022\n",
      "[2000]\tvalid_set's rmse: 0.685914\n",
      "[3000]\tvalid_set's rmse: 0.685158\n",
      "[4000]\tvalid_set's rmse: 0.685062\n",
      "[5000]\tvalid_set's rmse: 0.685012\n",
      "[6000]\tvalid_set's rmse: 0.684995\n",
      "[7000]\tvalid_set's rmse: 0.684998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.685\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 158.97s of the 158.97s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.644952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6446\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 151.87s of the 151.86s of remaining time.\n",
      "\t-0.585\t = Validation score   (-root_mean_squared_error)\n",
      "\t111.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 39.46s of the 39.45s of remaining time.\n",
      "\t-0.6718\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 10.03s of the 10.03s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 387. Best iteration is:\n",
      "\t[387]\tvalid_set's rmse: 0.693028\n",
      "\t-0.693\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.38s of the -0.33s of remaining time.\n",
      "\t-0.5848\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.01s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels191\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1899674629154214\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1899674629154214,\n",
      "    \"mean_squared_error\": -0.036087636966521945,\n",
      "    \"mean_absolute_error\": -0.07535213314167935,\n",
      "    \"r2\": 0.9971498841907475,\n",
      "    \"pearsonr\": 0.9985851498373779,\n",
      "    \"median_absolute_error\": -0.028683273004736165\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels192\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.30 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 817\n",
      "Label Column: 817\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.0757671637, -33.8827645597, 0.75725, 4.97112)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60781.77 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.24 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 502 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 502 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t502 features in original data used to generate 502 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.93 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.37s of the 179.37s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.41705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.4139\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 174.44s of the 174.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.34731\n",
      "[2000]\tvalid_set's rmse: 1.34496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.3448\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.27s of the 164.27s of remaining time.\n",
      "\t-1.2601\t = Validation score   (-root_mean_squared_error)\n",
      "\t117.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 46.47s of the 46.47s of remaining time.\n",
      "\t-1.3905\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.72s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 27.64s of the 27.63s of remaining time.\n",
      "\t-1.4563\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.38s of the 2.79s of remaining time.\n",
      "\t-1.26\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 177.99s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels192\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.40631726337858143\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.40631726337858143,\n",
      "    \"mean_squared_error\": -0.16509371851945961,\n",
      "    \"mean_absolute_error\": -0.1488508853545619,\n",
      "    \"r2\": 0.9933186714102221,\n",
      "    \"pearsonr\": 0.9966638487412347,\n",
      "    \"median_absolute_error\": -0.053156719487658766\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels193\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.24 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 818\n",
      "Label Column: 818\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (31.8638100887, -32.629584922, 1.07988, 5.36621)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60787.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.32 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 503 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 503 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t503 features in original data used to generate 503 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.01 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.39s of the 179.38s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.703407\n",
      "[2000]\tvalid_set's rmse: 0.69914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.699\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.72s of the 171.71s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.583181\n",
      "[2000]\tvalid_set's rmse: 0.582911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5828\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.17s of the 162.17s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 8228.\n",
      "\t-0.542\t = Validation score   (-root_mean_squared_error)\n",
      "\t162.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.39s of the -0.93s of remaining time.\n",
      "\t-0.5346\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.65s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels193\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16927060096418167\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16927060096418167,\n",
      "    \"mean_squared_error\": -0.028652536350775352,\n",
      "    \"mean_absolute_error\": -0.04302724525793935,\n",
      "    \"r2\": 0.9990048947113261,\n",
      "    \"pearsonr\": 0.9995026327095888,\n",
      "    \"median_absolute_error\": -0.005219881662480053\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels194\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.17 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 819\n",
      "Label Column: 819\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.1587127779, -14.7441789104, 1.95606, 3.74686)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60782.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.4 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 504 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 504 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t504 features in original data used to generate 504 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.09 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.39s of the 179.39s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.799267\n",
      "[2000]\tvalid_set's rmse: 0.792358\n",
      "[3000]\tvalid_set's rmse: 0.79147\n",
      "[4000]\tvalid_set's rmse: 0.791109\n",
      "[5000]\tvalid_set's rmse: 0.791128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7911\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.58s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.24s of the 163.24s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.716492\n",
      "[2000]\tvalid_set's rmse: 0.71537\n",
      "[3000]\tvalid_set's rmse: 0.715192\n",
      "[4000]\tvalid_set's rmse: 0.715171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7152\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 145.37s of the 145.37s of remaining time.\n",
      "\t-0.6615\t = Validation score   (-root_mean_squared_error)\n",
      "\t134.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 10.72s of the 10.72s of remaining time.\n",
      "\t-0.7697\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.39s of the -0.21s of remaining time.\n",
      "\t-0.6611\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.95s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels194\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21115734362284935\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21115734362284935,\n",
      "    \"mean_squared_error\": -0.0445874237658583,\n",
      "    \"mean_absolute_error\": -0.06844702230235988,\n",
      "    \"r2\": 0.9968237132602247,\n",
      "    \"pearsonr\": 0.9984192828078697,\n",
      "    \"median_absolute_error\": -0.01896808859618962\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels195\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.10 GB / 2000.36 GB (2.7%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 820\n",
      "Label Column: 820\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.7124157931, -16.0818162013, 2.30549, 3.69204)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60736.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.49 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 505 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 505 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t505 features in original data used to generate 505 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.18 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.36s of the 179.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.907957\n",
      "[2000]\tvalid_set's rmse: 0.903087\n",
      "[3000]\tvalid_set's rmse: 0.901736\n",
      "[4000]\tvalid_set's rmse: 0.901397\n",
      "[5000]\tvalid_set's rmse: 0.901342\n",
      "[6000]\tvalid_set's rmse: 0.90132\n",
      "[7000]\tvalid_set's rmse: 0.901331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9013\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 157.9s of the 157.9s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.842737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8416\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 153.28s of the 153.28s of remaining time.\n",
      "\t-0.7964\t = Validation score   (-root_mean_squared_error)\n",
      "\t87.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 65.55s of the 65.55s of remaining time.\n",
      "\t-0.8937\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.64s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 45.79s of the 45.79s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.893481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1961. Best iteration is:\n",
      "\t[1959]\tvalid_set's rmse: 0.893373\n",
      "\t-0.8934\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.36s of the -0.88s of remaining time.\n",
      "\t-0.796\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels195\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2727974996857541\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2727974996857541,\n",
      "    \"mean_squared_error\": -0.07441847583479864,\n",
      "    \"mean_absolute_error\": -0.13523079307426886,\n",
      "    \"r2\": 0.9945400449999573,\n",
      "    \"pearsonr\": 0.9972961702303089,\n",
      "    \"median_absolute_error\": -0.07234925079709578\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels196\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   53.00 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 821\n",
      "Label Column: 821\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (36.7930311575, -25.2846520161, 2.53567, 5.37459)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60740.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.57 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 506 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 506 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t506 features in original data used to generate 506 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.26 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.35s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.60384\n",
      "[2000]\tvalid_set's rmse: 1.59966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.5991\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.09s of the 171.08s of remaining time.\n",
      "\t-1.5474\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.28s of the 168.28s of remaining time.\n",
      "\t-1.5088\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 122.8s of the 122.79s of remaining time.\n",
      "\t-1.5678\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 105.15s of the 105.15s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.57849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.5784\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.35s of the 80.14s of remaining time.\n",
      "\t-1.4965\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 100.56s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels196\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.6464858050118095\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.6464858050118095,\n",
      "    \"mean_squared_error\": -0.417943896081768,\n",
      "    \"mean_absolute_error\": -0.4319943010400553,\n",
      "    \"r2\": 0.985529995702741,\n",
      "    \"pearsonr\": 0.99293186304057,\n",
      "    \"median_absolute_error\": -0.30218524526426394\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels197\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.94 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 822\n",
      "Label Column: 822\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (24.2439420315, -18.6167342387, 2.45811, 4.35107)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60732.9 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.65 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 507 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 507 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t507 features in original data used to generate 507 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.34 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.36s of the 179.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.863722\n",
      "[2000]\tvalid_set's rmse: 0.861115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8605\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.44s of the 172.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.819055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.818\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.9s of the 164.9s of remaining time.\n",
      "\t-0.8014\t = Validation score   (-root_mean_squared_error)\n",
      "\t65.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 98.58s of the 98.58s of remaining time.\n",
      "\t-0.8685\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.51s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 60.49s of the 60.49s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.874854\n",
      "[2000]\tvalid_set's rmse: 0.874736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2635. Best iteration is:\n",
      "\t[2449]\tvalid_set's rmse: 0.874735\n",
      "\t-0.8747\t = Validation score   (-root_mean_squared_error)\n",
      "\t60.87s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.36s of the -1.46s of remaining time.\n",
      "\t-0.7907\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 182.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels197\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2786463473577374\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2786463473577374,\n",
      "    \"mean_squared_error\": -0.07764378689580877,\n",
      "    \"mean_absolute_error\": -0.14878031422696636,\n",
      "    \"r2\": 0.9958983786391584,\n",
      "    \"pearsonr\": 0.9979579945875875,\n",
      "    \"median_absolute_error\": -0.08604857743358152\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels198\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.84 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 823\n",
      "Label Column: 823\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.6366261741, -13.1918520982, 2.20895, 3.33455)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60848.64 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.74 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 508 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 508 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t508 features in original data used to generate 508 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.35s of the 179.35s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.802568\n",
      "[2000]\tvalid_set's rmse: 0.796048\n",
      "[3000]\tvalid_set's rmse: 0.794534\n",
      "[4000]\tvalid_set's rmse: 0.794262\n",
      "[5000]\tvalid_set's rmse: 0.794139\n",
      "[6000]\tvalid_set's rmse: 0.794134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7941\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.68s of the 160.68s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.80311\n",
      "[2000]\tvalid_set's rmse: 0.801504\n",
      "[3000]\tvalid_set's rmse: 0.801303\n",
      "[4000]\tvalid_set's rmse: 0.801304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8013\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.13s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.11s of the 144.11s of remaining time.\n",
      "\t-0.7426\t = Validation score   (-root_mean_squared_error)\n",
      "\t73.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 70.27s of the 70.27s of remaining time.\n",
      "\t-0.8258\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.27s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 44.57s of the 44.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.830366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8303\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.35s of the 6.38s of remaining time.\n",
      "\t-0.7417\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 174.4s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels198\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.26099613033806857\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.26099613033806857,\n",
      "    \"mean_squared_error\": -0.06811898005144591,\n",
      "    \"mean_absolute_error\": -0.13833484759234627,\n",
      "    \"r2\": 0.9938731938370043,\n",
      "    \"pearsonr\": 0.9969627238051076,\n",
      "    \"median_absolute_error\": -0.08067931001333051\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels199\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.75 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 824\n",
      "Label Column: 824\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.6915866336, -13.0374347777, 2.0059, 3.03958)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60754.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.82 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 509 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 509 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t509 features in original data used to generate 509 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.51 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.36s of the 179.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.779256\n",
      "[2000]\tvalid_set's rmse: 0.775148\n",
      "[3000]\tvalid_set's rmse: 0.774321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7743\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.87s of the 167.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.766864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7666\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.99s of the 162.98s of remaining time.\n",
      "\t-0.714\t = Validation score   (-root_mean_squared_error)\n",
      "\t76.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 85.65s of the 85.65s of remaining time.\n",
      "\t-0.7985\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.36s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 56.86s of the 56.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.809244\n",
      "[2000]\tvalid_set's rmse: 0.809187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8092\t = Validation score   (-root_mean_squared_error)\n",
      "\t47.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.36s of the 9.02s of remaining time.\n",
      "\t-0.7137\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 171.66s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels199\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.25238848687430115\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.25238848687430115,\n",
      "    \"mean_squared_error\": -0.06369994830669906,\n",
      "    \"mean_absolute_error\": -0.13400304258436777,\n",
      "    \"r2\": 0.9931047003448071,\n",
      "    \"pearsonr\": 0.9965927546328003,\n",
      "    \"median_absolute_error\": -0.07856073335773939\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels200\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.67 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 825\n",
      "Label Column: 825\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.1085932727, -16.9849704436, 1.94851, 3.37265)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60748.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.9 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 510 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 510 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t510 features in original data used to generate 510 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.6 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.36s of the 179.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.8075\n",
      "[2000]\tvalid_set's rmse: 0.804382\n",
      "[3000]\tvalid_set's rmse: 0.803817\n",
      "[4000]\tvalid_set's rmse: 0.803682\n",
      "[5000]\tvalid_set's rmse: 0.803653\n",
      "[6000]\tvalid_set's rmse: 0.803649\n",
      "[7000]\tvalid_set's rmse: 0.80364\n",
      "[8000]\tvalid_set's rmse: 0.803638\n",
      "[9000]\tvalid_set's rmse: 0.803639\n",
      "[10000]\tvalid_set's rmse: 0.803639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8036\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.11s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 152.0s of the 152.0s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.796981\n",
      "[2000]\tvalid_set's rmse: 0.795913\n",
      "[3000]\tvalid_set's rmse: 0.795764\n",
      "[4000]\tvalid_set's rmse: 0.79572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7957\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 135.33s of the 135.33s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6903.\n",
      "\t-0.7348\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.36s of the -1.12s of remaining time.\n",
      "\t-0.7348\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.8s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels200\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.23295035863989538\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.23295035863989538,\n",
      "    \"mean_squared_error\": -0.054265869590455616,\n",
      "    \"mean_absolute_error\": -0.0625732394051761,\n",
      "    \"r2\": 0.9952288090566128,\n",
      "    \"pearsonr\": 0.9976143103822415,\n",
      "    \"median_absolute_error\": -0.010152732098092698\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels201\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.59 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 826\n",
      "Label Column: 826\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.978569947, -15.021577898, 1.76753, 3.72832)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60826.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 68.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 511 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 511 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t511 features in original data used to generate 511 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.34s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.948759\n",
      "[2000]\tvalid_set's rmse: 0.943591\n",
      "[3000]\tvalid_set's rmse: 0.943067\n",
      "[4000]\tvalid_set's rmse: 0.942845\n",
      "[5000]\tvalid_set's rmse: 0.94281\n",
      "[6000]\tvalid_set's rmse: 0.942832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9428\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.56s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.15s of the 161.15s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.879327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8784\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.19s of the 156.19s of remaining time.\n",
      "\t-0.8336\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 119.44s of the 119.44s of remaining time.\n",
      "\t-0.933\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 104.61s of the 104.6s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.931558\n",
      "[2000]\tvalid_set's rmse: 0.931498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9315\t = Validation score   (-root_mean_squared_error)\n",
      "\t65.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.34s of the 38.23s of remaining time.\n",
      "\t-0.8324\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 142.54s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels201\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.38157284224013027\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.38157284224013027,\n",
      "    \"mean_squared_error\": -0.1455978339352117,\n",
      "    \"mean_absolute_error\": -0.2616914815792392,\n",
      "    \"r2\": 0.9895245861153196,\n",
      "    \"pearsonr\": 0.9948415774948505,\n",
      "    \"median_absolute_error\": -0.19517264630632347\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels202\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.49 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 827\n",
      "Label Column: 827\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (30.1176699373, -18.7046485095, 1.52237, 3.95331)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60755.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.07 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 512 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 512 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t512 features in original data used to generate 512 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.76 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.34s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.02996\n",
      "[2000]\tvalid_set's rmse: 1.02528\n",
      "[3000]\tvalid_set's rmse: 1.02454\n",
      "[4000]\tvalid_set's rmse: 1.02447\n",
      "[5000]\tvalid_set's rmse: 1.02437\n",
      "[6000]\tvalid_set's rmse: 1.02438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0244\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.25s of the 161.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.928709\n",
      "[2000]\tvalid_set's rmse: 0.926737\n",
      "[3000]\tvalid_set's rmse: 0.9266\n",
      "[4000]\tvalid_set's rmse: 0.926598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9266\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.91s of the 144.91s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7334.\n",
      "\t-0.8916\t = Validation score   (-root_mean_squared_error)\n",
      "\t145.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.34s of the -0.65s of remaining time.\n",
      "\t-0.8896\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.36s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels202\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.28221141357883833\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.28221141357883833,\n",
      "    \"mean_squared_error\": -0.07964328195416642,\n",
      "    \"mean_absolute_error\": -0.06929874422423228,\n",
      "    \"r2\": 0.9949035364127,\n",
      "    \"pearsonr\": 0.9974614112922169,\n",
      "    \"median_absolute_error\": -0.013953890147644032\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels203\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.42 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 828\n",
      "Label Column: 828\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (27.757273642, -21.9374675195, 1.6194, 4.02641)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60758.11 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.15 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 513 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 513 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t513 features in original data used to generate 513 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.85 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.36s of the 179.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.0019\n",
      "[2000]\tvalid_set's rmse: 0.997925\n",
      "[3000]\tvalid_set's rmse: 0.997836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9978\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.91s of the 167.91s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.910532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9101\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 161.86s of the 161.85s of remaining time.\n",
      "\t-0.8685\t = Validation score   (-root_mean_squared_error)\n",
      "\t87.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 73.41s of the 73.41s of remaining time.\n",
      "\t-0.9816\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.71s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 51.27s of the 51.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.981086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.981\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.36s of the 6.78s of remaining time.\n",
      "\t-0.8667\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 173.95s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels203\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.28674065084034145\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.28674065084034145,\n",
      "    \"mean_squared_error\": -0.08222020084434296,\n",
      "    \"mean_absolute_error\": -0.11576764469068494,\n",
      "    \"r2\": 0.9949279548859468,\n",
      "    \"pearsonr\": 0.9974778572767204,\n",
      "    \"median_absolute_error\": -0.05871599310549325\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels204\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.34 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 829\n",
      "Label Column: 829\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.9181404909, -9.4301766526, -0.10203, 2.304)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60854.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.24 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 514 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 514 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t514 features in original data used to generate 514 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 42.93 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.34s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.627899\n",
      "[2000]\tvalid_set's rmse: 0.626117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6256\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 173.03s of the 173.03s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.572885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5726\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 168.19s of the 168.18s of remaining time.\n",
      "\t-0.5705\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 124.59s of the 124.59s of remaining time.\n",
      "\t-0.5881\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.22s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 108.97s of the 108.97s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.579751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5797\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.34s of the 61.7s of remaining time.\n",
      "\t-0.5615\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 119.04s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels204\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2242436219266691\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2242436219266691,\n",
      "    \"mean_squared_error\": -0.050285201974790666,\n",
      "    \"mean_absolute_error\": -0.14579409964663342,\n",
      "    \"r2\": 0.9905263212962016,\n",
      "    \"pearsonr\": 0.9953300178962268,\n",
      "    \"median_absolute_error\": -0.10048168377443092\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels205\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.26 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 830\n",
      "Label Column: 830\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (9.2837806896, -14.0893371013, -0.53537, 2.17084)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60756.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.32 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 515 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 515 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t515 features in original data used to generate 515 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.01 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.35s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.444806\n",
      "[2000]\tvalid_set's rmse: 0.441315\n",
      "[3000]\tvalid_set's rmse: 0.440926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4409\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.88s of the 167.88s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.436149\n",
      "[2000]\tvalid_set's rmse: 0.435338\n",
      "[3000]\tvalid_set's rmse: 0.435244\n",
      "[4000]\tvalid_set's rmse: 0.435229\n",
      "[5000]\tvalid_set's rmse: 0.435227\n",
      "[6000]\tvalid_set's rmse: 0.435226\n",
      "[7000]\tvalid_set's rmse: 0.435226\n",
      "[8000]\tvalid_set's rmse: 0.435226\n",
      "[9000]\tvalid_set's rmse: 0.435226\n",
      "[10000]\tvalid_set's rmse: 0.435226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4352\t = Validation score   (-root_mean_squared_error)\n",
      "\t38.22s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 128.16s of the 128.16s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6475.\n",
      "\t-0.4062\t = Validation score   (-root_mean_squared_error)\n",
      "\t128.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.35s of the -0.65s of remaining time.\n",
      "\t-0.4059\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.34s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels205\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.12945003103947908\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.12945003103947908,\n",
      "    \"mean_squared_error\": -0.016757310536122085,\n",
      "    \"mean_absolute_error\": -0.04320785797858277,\n",
      "    \"r2\": 0.996443770592152,\n",
      "    \"pearsonr\": 0.9982242894996345,\n",
      "    \"median_absolute_error\": -0.010599287073401076\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels206\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.18 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 831\n",
      "Label Column: 831\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.8262626627, -16.8858203544, -0.89074, 2.19601)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60823.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.41 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 516 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 516 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t516 features in original data used to generate 516 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.1 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.35s of the 179.35s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.382505\n",
      "[2000]\tvalid_set's rmse: 0.380801\n",
      "[3000]\tvalid_set's rmse: 0.380627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3806\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.7s of the 168.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.360119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3598\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.59s of the 163.59s of remaining time.\n",
      "\t-0.3277\t = Validation score   (-root_mean_squared_error)\n",
      "\t91.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 71.39s of the 71.39s of remaining time.\n",
      "\t-0.3782\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 49.5s of the 49.5s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.396507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3965\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.35s of the 15.41s of remaining time.\n",
      "\t-0.3266\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 165.35s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels206\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11260828800782582\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11260828800782582,\n",
      "    \"mean_squared_error\": -0.01268062652805342,\n",
      "    \"mean_absolute_error\": -0.05766992998863064,\n",
      "    \"r2\": 0.9973702449500704,\n",
      "    \"pearsonr\": 0.9986915798510559,\n",
      "    \"median_absolute_error\": -0.03081157571365367\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels207\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.10 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 832\n",
      "Label Column: 832\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (8.1454580979, -14.0435689254, -0.97444, 2.29362)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60754.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.49 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 517 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 517 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t517 features in original data used to generate 517 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.18 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.385208\n",
      "[2000]\tvalid_set's rmse: 0.381023\n",
      "[3000]\tvalid_set's rmse: 0.380608\n",
      "[4000]\tvalid_set's rmse: 0.380437\n",
      "[5000]\tvalid_set's rmse: 0.380392\n",
      "[6000]\tvalid_set's rmse: 0.380382\n",
      "[7000]\tvalid_set's rmse: 0.380378\n",
      "[8000]\tvalid_set's rmse: 0.380376\n",
      "[9000]\tvalid_set's rmse: 0.380376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3804\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.3s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 152.84s of the 152.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.348096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3479\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 146.52s of the 146.52s of remaining time.\n",
      "\t-0.3205\t = Validation score   (-root_mean_squared_error)\n",
      "\t98.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 47.6s of the 47.59s of remaining time.\n",
      "\t-0.3728\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.13s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 12.99s of the 12.99s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 500. Best iteration is:\n",
      "\t[500]\tvalid_set's rmse: 0.381393\n",
      "\t-0.3814\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.66s of remaining time.\n",
      "\t-0.3195\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.43s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels207\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10774066938957674\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10774066938957674,\n",
      "    \"mean_squared_error\": -0.011608051840514095,\n",
      "    \"mean_absolute_error\": -0.0511240634336071,\n",
      "    \"r2\": 0.9977932318626631,\n",
      "    \"pearsonr\": 0.9989009608843431,\n",
      "    \"median_absolute_error\": -0.025149550965789746\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels208\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   52.01 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 833\n",
      "Label Column: 833\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.2377219666, -12.9317639765, -0.76381, 2.34937)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60710.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.57 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 518 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 518 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t518 features in original data used to generate 518 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.26 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.35s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.41741\n",
      "[2000]\tvalid_set's rmse: 0.414451\n",
      "[3000]\tvalid_set's rmse: 0.413906\n",
      "[4000]\tvalid_set's rmse: 0.413774\n",
      "[5000]\tvalid_set's rmse: 0.41372\n",
      "[6000]\tvalid_set's rmse: 0.413707\n",
      "[7000]\tvalid_set's rmse: 0.413706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4137\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.82s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 157.54s of the 157.54s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.388003\n",
      "[2000]\tvalid_set's rmse: 0.386455\n",
      "[3000]\tvalid_set's rmse: 0.38635\n",
      "[4000]\tvalid_set's rmse: 0.386316\n",
      "[5000]\tvalid_set's rmse: 0.386312\n",
      "[6000]\tvalid_set's rmse: 0.386311\n",
      "[7000]\tvalid_set's rmse: 0.386311\n",
      "[8000]\tvalid_set's rmse: 0.386311\n",
      "[9000]\tvalid_set's rmse: 0.386311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3863\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.71s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 120.67s of the 120.67s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6139.\n",
      "\t-0.3598\t = Validation score   (-root_mean_squared_error)\n",
      "\t120.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.35s of the -0.69s of remaining time.\n",
      "\t-0.3592\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.45s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels208\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11516033137287798\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11516033137287798,\n",
      "    \"mean_squared_error\": -0.013261901921911082,\n",
      "    \"mean_absolute_error\": -0.041512765635328845,\n",
      "    \"r2\": 0.9975970605394212,\n",
      "    \"pearsonr\": 0.9987993130672868,\n",
      "    \"median_absolute_error\": -0.012540416524817577\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels209\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.92 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 834\n",
      "Label Column: 834\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.3211964207, -11.6428816591, -0.46072, 2.41922)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60733.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.66 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 519 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 519 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t519 features in original data used to generate 519 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.35 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.36s of the 179.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.409735\n",
      "[2000]\tvalid_set's rmse: 0.405199\n",
      "[3000]\tvalid_set's rmse: 0.404446\n",
      "[4000]\tvalid_set's rmse: 0.404357\n",
      "[5000]\tvalid_set's rmse: 0.404321\n",
      "[6000]\tvalid_set's rmse: 0.404306\n",
      "[7000]\tvalid_set's rmse: 0.404304\n",
      "[8000]\tvalid_set's rmse: 0.404302\n",
      "[9000]\tvalid_set's rmse: 0.404301\n",
      "[10000]\tvalid_set's rmse: 0.404302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4043\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.59s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 151.36s of the 151.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.392268\n",
      "[2000]\tvalid_set's rmse: 0.391383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3913\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 142.46s of the 142.46s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7080.\n",
      "\t-0.3454\t = Validation score   (-root_mean_squared_error)\n",
      "\t142.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.36s of the -0.66s of remaining time.\n",
      "\t-0.3454\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.37s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels209\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10943134860974632\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10943134860974632,\n",
      "    \"mean_squared_error\": -0.011975220058547786,\n",
      "    \"mean_absolute_error\": -0.030982156526112702,\n",
      "    \"r2\": 0.9979536694368429,\n",
      "    \"pearsonr\": 0.998977158024536,\n",
      "    \"median_absolute_error\": -0.0041215418793388164\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels210\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.84 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 835\n",
      "Label Column: 835\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.1576086365, -14.4028059286, -0.2062, 2.59347)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60724.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.74 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 520 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 520 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t520 features in original data used to generate 520 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.36s of the 179.35s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.435124\n",
      "[2000]\tvalid_set's rmse: 0.431264\n",
      "[3000]\tvalid_set's rmse: 0.430531\n",
      "[4000]\tvalid_set's rmse: 0.430391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4304\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.88s of the 164.88s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.405095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4044\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.49s of the 156.49s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7704.\n",
      "\t-0.3707\t = Validation score   (-root_mean_squared_error)\n",
      "\t156.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.36s of the -0.87s of remaining time.\n",
      "\t-0.3698\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.64s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels210\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11783128625659262\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11783128625659262,\n",
      "    \"mean_squared_error\": -0.013884212020883064,\n",
      "    \"mean_absolute_error\": -0.03819595127825157,\n",
      "    \"r2\": 0.9979355764672078,\n",
      "    \"pearsonr\": 0.9989675239578663,\n",
      "    \"median_absolute_error\": -0.009241420569210734\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels211\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.77 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 836\n",
      "Label Column: 836\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.1538076463, -15.3214932098, 0.13666, 2.84128)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60836.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.82 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 521 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 521 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t521 features in original data used to generate 521 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.51 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.34s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.645444\n",
      "[2000]\tvalid_set's rmse: 0.63978\n",
      "[3000]\tvalid_set's rmse: 0.639231\n",
      "[4000]\tvalid_set's rmse: 0.639119\n",
      "[5000]\tvalid_set's rmse: 0.639096\n",
      "[6000]\tvalid_set's rmse: 0.639084\n",
      "[7000]\tvalid_set's rmse: 0.63908\n",
      "[8000]\tvalid_set's rmse: 0.63908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6391\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 156.28s of the 156.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.571489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5711\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.94s of the 150.94s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7458.\n",
      "\t-0.5436\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.34s of the -0.65s of remaining time.\n",
      "\t-0.5426\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.4s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels211\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1738903756112399\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1738903756112399,\n",
      "    \"mean_squared_error\": -0.030237862730218227,\n",
      "    \"mean_absolute_error\": -0.06167238672555917,\n",
      "    \"r2\": 0.9962540306590648,\n",
      "    \"pearsonr\": 0.9981285762373882,\n",
      "    \"median_absolute_error\": -0.019738271325372314\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels212\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.70 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 837\n",
      "Label Column: 837\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.3446192138, -9.1707703131, 0.41787, 2.98245)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60840.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.91 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 522 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 522 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t522 features in original data used to generate 522 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.6 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.35s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.661336\n",
      "[2000]\tvalid_set's rmse: 0.657029\n",
      "[3000]\tvalid_set's rmse: 0.656631\n",
      "[4000]\tvalid_set's rmse: 0.656653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6566\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.55s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.07s of the 166.06s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.594922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5946\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.64s of the 159.63s of remaining time.\n",
      "\t-0.575\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 112.93s of the 112.92s of remaining time.\n",
      "\t-0.6273\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.19s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 87.28s of the 87.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.650124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6501\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.35s of the 55.68s of remaining time.\n",
      "\t-0.5714\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 125.06s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels212\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.23790569418635082\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.23790569418635082,\n",
      "    \"mean_squared_error\": -0.0565991193262894,\n",
      "    \"mean_absolute_error\": -0.15835343007488983,\n",
      "    \"r2\": 0.9936363639884783,\n",
      "    \"pearsonr\": 0.9968495002021126,\n",
      "    \"median_absolute_error\": -0.11297740382624202\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels213\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.63 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 838\n",
      "Label Column: 838\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.4128747275, -9.2662276672, 0.65381, 3.04184)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60883.29 MB\n",
      "\tTrain Data (Original)  Memory Usage: 69.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 523 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 523 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t523 features in original data used to generate 523 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.307131\n",
      "[2000]\tvalid_set's rmse: 0.305978\n",
      "[3000]\tvalid_set's rmse: 0.305604\n",
      "[4000]\tvalid_set's rmse: 0.305593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3056\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.28s of the 167.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.269435\n",
      "[2000]\tvalid_set's rmse: 0.269183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2692\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.6s of the 155.6s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7636.\n",
      "\t-0.2465\t = Validation score   (-root_mean_squared_error)\n",
      "\t155.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.88s of remaining time.\n",
      "\t-0.2423\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.59s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels213\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.07673068150188472\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.07673068150188472,\n",
      "    \"mean_squared_error\": -0.005887597483743689,\n",
      "    \"mean_absolute_error\": -0.021297024363412305,\n",
      "    \"r2\": 0.9993636345354872,\n",
      "    \"pearsonr\": 0.9996818372376861,\n",
      "    \"median_absolute_error\": -0.0025627747617491536\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels214\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.56 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 839\n",
      "Label Column: 839\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.6747722384, -15.8211556, 1.09237, 3.25513)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60883.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.07 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 524 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 524 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t524 features in original data used to generate 524 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.76 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.34s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.585056\n",
      "[2000]\tvalid_set's rmse: 0.582411\n",
      "[3000]\tvalid_set's rmse: 0.581811\n",
      "[4000]\tvalid_set's rmse: 0.581707\n",
      "[5000]\tvalid_set's rmse: 0.581669\n",
      "[6000]\tvalid_set's rmse: 0.58166\n",
      "[7000]\tvalid_set's rmse: 0.581654\n",
      "[8000]\tvalid_set's rmse: 0.581655\n",
      "[9000]\tvalid_set's rmse: 0.581654\n",
      "[10000]\tvalid_set's rmse: 0.581654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5817\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.22s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 150.62s of the 150.62s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.56752\n",
      "[2000]\tvalid_set's rmse: 0.565981\n",
      "[3000]\tvalid_set's rmse: 0.565748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5657\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 134.33s of the 134.33s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6661.\n",
      "\t-0.5187\t = Validation score   (-root_mean_squared_error)\n",
      "\t134.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.34s of the -0.87s of remaining time.\n",
      "\t-0.5174\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.6s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels214\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16398226519710477\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16398226519710477,\n",
      "    \"mean_squared_error\": -0.026890183299173538,\n",
      "    \"mean_absolute_error\": -0.045971563225195314,\n",
      "    \"r2\": 0.9974619541901527,\n",
      "    \"pearsonr\": 0.9987324710837686,\n",
      "    \"median_absolute_error\": -0.006551147568908844\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels215\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.47 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 840\n",
      "Label Column: 840\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.7310582606, -13.3725744951, 1.91333, 3.37432)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60876.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.16 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 525 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 525 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t525 features in original data used to generate 525 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.85 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.34s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.696324\n",
      "[2000]\tvalid_set's rmse: 0.689568\n",
      "[3000]\tvalid_set's rmse: 0.689205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6892\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.59s of the 169.59s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.619891\n",
      "[2000]\tvalid_set's rmse: 0.617821\n",
      "[3000]\tvalid_set's rmse: 0.61773\n",
      "[4000]\tvalid_set's rmse: 0.617718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6177\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.46s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.36s of the 150.35s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7385.\n",
      "\t-0.5796\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.34s of the -1.13s of remaining time.\n",
      "\t-0.5784\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.89s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels215\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18321746113797932\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18321746113797932,\n",
      "    \"mean_squared_error\": -0.03356863806584708,\n",
      "    \"mean_absolute_error\": -0.04882291814749506,\n",
      "    \"r2\": 0.9970514964557461,\n",
      "    \"pearsonr\": 0.9985275479785943,\n",
      "    \"median_absolute_error\": -0.006450351563830781\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels216\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.40 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 841\n",
      "Label Column: 841\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.2092413084, -12.1830318857, 0.05481, 3.07991)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60839.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.24 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 526 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 526 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t526 features in original data used to generate 526 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 43.93 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.35s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.547752\n",
      "[2000]\tvalid_set's rmse: 0.542851\n",
      "[3000]\tvalid_set's rmse: 0.5422\n",
      "[4000]\tvalid_set's rmse: 0.542203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5421\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.85s of the 166.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.503055\n",
      "[2000]\tvalid_set's rmse: 0.502428\n",
      "[3000]\tvalid_set's rmse: 0.502323\n",
      "[4000]\tvalid_set's rmse: 0.502296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5023\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 147.66s of the 147.66s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7273.\n",
      "\t-0.4574\t = Validation score   (-root_mean_squared_error)\n",
      "\t147.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.35s of the -0.65s of remaining time.\n",
      "\t-0.4574\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.41s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels216\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1449548894888988\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1449548894888988,\n",
      "    \"mean_squared_error\": -0.021011919986738913,\n",
      "    \"mean_absolute_error\": -0.040864931713047156,\n",
      "    \"r2\": 0.9977846999707258,\n",
      "    \"pearsonr\": 0.9988943236045863,\n",
      "    \"median_absolute_error\": -0.005688538918661147\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels217\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.32 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 842\n",
      "Label Column: 842\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (32.5991806982, -18.9699505437, 3.02133, 3.62866)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60872.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.32 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 527 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 527 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t527 features in original data used to generate 527 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.02 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.35s of the 179.35s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.04595\n",
      "[2000]\tvalid_set's rmse: 1.04045\n",
      "[3000]\tvalid_set's rmse: 1.03924\n",
      "[4000]\tvalid_set's rmse: 1.03913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0391\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 165.69s of the 165.69s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.952167\n",
      "[2000]\tvalid_set's rmse: 0.949717\n",
      "[3000]\tvalid_set's rmse: 0.949451\n",
      "[4000]\tvalid_set's rmse: 0.949378\n",
      "[5000]\tvalid_set's rmse: 0.949376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9494\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.1s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.77s of the 144.77s of remaining time.\n",
      "\t-0.92\t = Validation score   (-root_mean_squared_error)\n",
      "\t80.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 63.15s of the 63.15s of remaining time.\n",
      "\t-0.9766\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 42.26s of the 42.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.999792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1716. Best iteration is:\n",
      "\t[1703]\tvalid_set's rmse: 0.999705\n",
      "\t-0.9997\t = Validation score   (-root_mean_squared_error)\n",
      "\t42.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.35s of the -0.81s of remaining time.\n",
      "\t-0.9159\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.59s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels217\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.30797173637752956\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.30797173637752956,\n",
      "    \"mean_squared_error\": -0.09484659040739063,\n",
      "    \"mean_absolute_error\": -0.14679771899741023,\n",
      "    \"r2\": 0.992796042252125,\n",
      "    \"pearsonr\": 0.9964440751982562,\n",
      "    \"median_absolute_error\": -0.07281561754328056\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels218\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.23 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 843\n",
      "Label Column: 843\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.8785907657, -13.8872203921, -0.46565, 2.9506)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60766.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.41 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 528 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 528 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t528 features in original data used to generate 528 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.1 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.34s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.767172\n",
      "[2000]\tvalid_set's rmse: 0.760428\n",
      "[3000]\tvalid_set's rmse: 0.759659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7595\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.34s of the 168.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.696265\n",
      "[2000]\tvalid_set's rmse: 0.695045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6949\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.8s of the 155.79s of remaining time.\n",
      "\t-0.6683\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 11.53s of the 11.53s of remaining time.\n",
      "\t-0.7493\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.71s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.34s of the -0.25s of remaining time.\n",
      "\t-0.6663\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.99s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels218\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21225072413217935\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21225072413217935,\n",
      "    \"mean_squared_error\": -0.04505036989463441,\n",
      "    \"mean_absolute_error\": -0.06843439321031472,\n",
      "    \"r2\": 0.9948248937980594,\n",
      "    \"pearsonr\": 0.9974163884404157,\n",
      "    \"median_absolute_error\": -0.016238039400244153\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels219\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.16 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 844\n",
      "Label Column: 844\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.895390187, -19.3020865547, 3.59547, 3.81524)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60774.5 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.49 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 529 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 529 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t529 features in original data used to generate 529 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.18 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.581694\n",
      "[2000]\tvalid_set's rmse: 0.576959\n",
      "[3000]\tvalid_set's rmse: 0.576443\n",
      "[4000]\tvalid_set's rmse: 0.576278\n",
      "[5000]\tvalid_set's rmse: 0.576251\n",
      "[6000]\tvalid_set's rmse: 0.576246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5762\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.32s of the 159.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.430087\n",
      "[2000]\tvalid_set's rmse: 0.429223\n",
      "[3000]\tvalid_set's rmse: 0.429132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4291\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 142.87s of the 142.87s of remaining time.\n",
      "\t-0.4147\t = Validation score   (-root_mean_squared_error)\n",
      "\t102.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 40.09s of the 40.09s of remaining time.\n",
      "\t-0.4677\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.66s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 15.28s of the 15.28s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 576. Best iteration is:\n",
      "\t[545]\tvalid_set's rmse: 0.486327\n",
      "\t-0.4863\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.71s of remaining time.\n",
      "\t-0.4031\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels219\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.12963912158387147\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.12963912158387147,\n",
      "    \"mean_squared_error\": -0.01680630184503781,\n",
      "    \"mean_absolute_error\": -0.04647448422393268,\n",
      "    \"r2\": 0.9988453000312817,\n",
      "    \"pearsonr\": 0.9994244322946917,\n",
      "    \"median_absolute_error\": -0.01629244606777336\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels220\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   51.08 GB / 2000.36 GB (2.6%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 845\n",
      "Label Column: 845\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.7644089393, -14.3156899891, -1.01707, 3.03138)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60838.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.57 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 530 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 530 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t530 features in original data used to generate 530 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.27 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.425765\n",
      "[2000]\tvalid_set's rmse: 0.422604\n",
      "[3000]\tvalid_set's rmse: 0.422488\n",
      "[4000]\tvalid_set's rmse: 0.422306\n",
      "[5000]\tvalid_set's rmse: 0.422284\n",
      "[6000]\tvalid_set's rmse: 0.422274\n",
      "[7000]\tvalid_set's rmse: 0.422269\n",
      "[8000]\tvalid_set's rmse: 0.422268\n",
      "[9000]\tvalid_set's rmse: 0.422268\n",
      "[10000]\tvalid_set's rmse: 0.422268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4223\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.3s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 150.72s of the 150.71s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.352395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3517\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 142.44s of the 142.44s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6938.\n",
      "\t-0.3182\t = Validation score   (-root_mean_squared_error)\n",
      "\t142.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.73s of remaining time.\n",
      "\t-0.3178\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.47s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels220\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10076060689456456\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10076060689456456,\n",
      "    \"mean_squared_error\": -0.010152699901760936,\n",
      "    \"mean_absolute_error\": -0.02713128175996606,\n",
      "    \"r2\": 0.9988950520691969,\n",
      "    \"pearsonr\": 0.9994487921376382,\n",
      "    \"median_absolute_error\": -0.004708862510894751\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels221\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.99 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 846\n",
      "Label Column: 846\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (25.258793812, -15.0089676454, 3.93436, 4.09189)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60817.7 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.66 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 531 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 531 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t531 features in original data used to generate 531 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.35 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.24024\n",
      "[2000]\tvalid_set's rmse: 1.23356\n",
      "[3000]\tvalid_set's rmse: 1.23288\n",
      "[4000]\tvalid_set's rmse: 1.23266\n",
      "[5000]\tvalid_set's rmse: 1.23262\n",
      "[6000]\tvalid_set's rmse: 1.23261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.2326\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.12s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.29s of the 160.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.15524\n",
      "[2000]\tvalid_set's rmse: 1.1529\n",
      "[3000]\tvalid_set's rmse: 1.1525\n",
      "[4000]\tvalid_set's rmse: 1.15248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1525\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.72s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 140.11s of the 140.11s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6832.\n",
      "\t-1.1114\t = Validation score   (-root_mean_squared_error)\n",
      "\t140.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.62s of remaining time.\n",
      "\t-1.1095\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.37s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels221\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.3531177036069516\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.3531177036069516,\n",
      "    \"mean_squared_error\": -0.12469211260064728,\n",
      "    \"mean_absolute_error\": -0.11059483224404394,\n",
      "    \"r2\": 0.9925521097928173,\n",
      "    \"pearsonr\": 0.9962831533696469,\n",
      "    \"median_absolute_error\": -0.024842669886718793\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels222\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.92 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 847\n",
      "Label Column: 847\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.6816738765, -16.8893156203, -2.19895, 3.04322)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60829.73 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.74 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 532 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 532 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t532 features in original data used to generate 532 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.34s of the 179.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.834584\n",
      "[2000]\tvalid_set's rmse: 0.828999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8288\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 170.84s of the 170.84s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.786305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7859\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.22s of the 162.22s of remaining time.\n",
      "\t-0.7743\t = Validation score   (-root_mean_squared_error)\n",
      "\t58.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 103.57s of the 103.56s of remaining time.\n",
      "\t-0.8396\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 81.37s of the 81.37s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.81891\n",
      "[2000]\tvalid_set's rmse: 0.81882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8188\t = Validation score   (-root_mean_squared_error)\n",
      "\t59.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.34s of the 20.51s of remaining time.\n",
      "\t-0.7674\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 160.27s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels222\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.27896126259158194\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.27896126259158194,\n",
      "    \"mean_squared_error\": -0.07781938602668946,\n",
      "    \"mean_absolute_error\": -0.16004741107829368,\n",
      "    \"r2\": 0.9915964534660967,\n",
      "    \"pearsonr\": 0.9958588634717693,\n",
      "    \"median_absolute_error\": -0.09924724012951658\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels223\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.83 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 848\n",
      "Label Column: 848\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (26.0657751006, -14.6635607354, 4.24899, 4.24489)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60866.31 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.83 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 533 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 533 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t533 features in original data used to generate 533 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.52 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.647516\n",
      "[2000]\tvalid_set's rmse: 0.643928\n",
      "[3000]\tvalid_set's rmse: 0.643476\n",
      "[4000]\tvalid_set's rmse: 0.64324\n",
      "[5000]\tvalid_set's rmse: 0.643214\n",
      "[6000]\tvalid_set's rmse: 0.643201\n",
      "[7000]\tvalid_set's rmse: 0.643196\n",
      "[8000]\tvalid_set's rmse: 0.643193\n",
      "[9000]\tvalid_set's rmse: 0.643193\n",
      "[10000]\tvalid_set's rmse: 0.643192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6432\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.66s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 150.14s of the 150.14s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.544281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.544\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 143.28s of the 143.28s of remaining time.\n",
      "\t-0.5281\t = Validation score   (-root_mean_squared_error)\n",
      "\t86.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 56.81s of the 56.81s of remaining time.\n",
      "\t-0.5749\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 21.05s of the 21.04s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 820. Best iteration is:\n",
      "\t[818]\tvalid_set's rmse: 0.582159\n",
      "\t-0.5822\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.51s of remaining time.\n",
      "\t-0.5216\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.26s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels223\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17415272287116626\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17415272287116626,\n",
      "    \"mean_squared_error\": -0.030329170883441104,\n",
      "    \"mean_absolute_error\": -0.07378850654596227,\n",
      "    \"r2\": 0.9983166708688963,\n",
      "    \"pearsonr\": 0.9991613088459259,\n",
      "    \"median_absolute_error\": -0.038793814849264474\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels224\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.73 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 849\n",
      "Label Column: 849\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.471774343, -17.6621619056, -2.92046, 3.07819)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60859.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.91 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 534 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 534 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t534 features in original data used to generate 534 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.6 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.577465\n",
      "[2000]\tvalid_set's rmse: 0.574098\n",
      "[3000]\tvalid_set's rmse: 0.573707\n",
      "[4000]\tvalid_set's rmse: 0.573692\n",
      "[5000]\tvalid_set's rmse: 0.573692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5737\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.89s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.92s of the 162.91s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.548777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.548\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.11s of the 156.11s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7490.\n",
      "\t-0.5206\t = Validation score   (-root_mean_squared_error)\n",
      "\t156.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.69s of remaining time.\n",
      "\t-0.5184\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.47s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels224\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.165078919662386\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.165078919662386,\n",
      "    \"mean_squared_error\": -0.02725104971690032,\n",
      "    \"mean_absolute_error\": -0.05387919177361791,\n",
      "    \"r2\": 0.9971237029947773,\n",
      "    \"pearsonr\": 0.9985617568834918,\n",
      "    \"median_absolute_error\": -0.013712572153802612\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels225\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.66 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 850\n",
      "Label Column: 850\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (27.404688997, -14.1577242612, 3.5661, 3.92896)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60922.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 70.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 535 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 535 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t535 features in original data used to generate 535 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.52835\n",
      "[2000]\tvalid_set's rmse: 1.51809\n",
      "[3000]\tvalid_set's rmse: 1.51699\n",
      "[4000]\tvalid_set's rmse: 1.51663\n",
      "[5000]\tvalid_set's rmse: 1.51655\n",
      "[6000]\tvalid_set's rmse: 1.51652\n",
      "[7000]\tvalid_set's rmse: 1.51652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.5165\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.78s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 157.83s of the 157.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.42206\n",
      "[2000]\tvalid_set's rmse: 1.42036\n",
      "[3000]\tvalid_set's rmse: 1.42024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.4202\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.09s of the 144.09s of remaining time.\n",
      "\t-1.3863\t = Validation score   (-root_mean_squared_error)\n",
      "\t84.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 59.3s of the 59.3s of remaining time.\n",
      "\t-1.4905\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.49s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 28.36s of the 28.36s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.49503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1138. Best iteration is:\n",
      "\t[1033]\tvalid_set's rmse: 1.49498\n",
      "\t-1.495\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -1.11s of remaining time.\n",
      "\t-1.3827\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.84s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels225\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.4643901451030359\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.4643901451030359,\n",
      "    \"mean_squared_error\": -0.2156582068688185,\n",
      "    \"mean_absolute_error\": -0.21142630096391532,\n",
      "    \"r2\": 0.9860282045704352,\n",
      "    \"pearsonr\": 0.9931268112169981,\n",
      "    \"median_absolute_error\": -0.10561852754966888\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels226\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.57 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 851\n",
      "Label Column: 851\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.8398472595, -15.0671057291, -2.6837, 2.86445)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60754.6 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.08 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 536 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 536 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t536 features in original data used to generate 536 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.77 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.553967\n",
      "[2000]\tvalid_set's rmse: 0.547921\n",
      "[3000]\tvalid_set's rmse: 0.547426\n",
      "[4000]\tvalid_set's rmse: 0.547386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5474\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.1s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.76s of the 164.76s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.50876\n",
      "[2000]\tvalid_set's rmse: 0.507613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5075\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 152.57s of the 152.57s of remaining time.\n",
      "\t-0.4625\t = Validation score   (-root_mean_squared_error)\n",
      "\t69.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 82.49s of the 82.48s of remaining time.\n",
      "\t-0.5416\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.7s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 59.26s of the 59.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.567261\n",
      "[2000]\tvalid_set's rmse: 0.567202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5672\t = Validation score   (-root_mean_squared_error)\n",
      "\t59.27s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.93s of remaining time.\n",
      "\t-0.462\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.7s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels226\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17044627391701034\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17044627391701034,\n",
      "    \"mean_squared_error\": -0.02905193229219244,\n",
      "    \"mean_absolute_error\": -0.09966068500821042,\n",
      "    \"r2\": 0.9964589353663841,\n",
      "    \"pearsonr\": 0.9982421129064993,\n",
      "    \"median_absolute_error\": -0.0628093961019836\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels227\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.48 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 852\n",
      "Label Column: 852\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.3926724772, -9.0224075081, 2.47043, 3.19262)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60862.77 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.16 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 537 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 537 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t537 features in original data used to generate 537 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.85 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.832613\n",
      "[2000]\tvalid_set's rmse: 0.828994\n",
      "[3000]\tvalid_set's rmse: 0.8284\n",
      "[4000]\tvalid_set's rmse: 0.828336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8283\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.3s of the 166.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.793019\n",
      "[2000]\tvalid_set's rmse: 0.791272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7912\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.07s of the 155.06s of remaining time.\n",
      "\t-0.7487\t = Validation score   (-root_mean_squared_error)\n",
      "\t75.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 79.21s of the 79.2s of remaining time.\n",
      "\t-0.8161\t = Validation score   (-root_mean_squared_error)\n",
      "\t38.22s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 40.51s of the 40.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.838601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1629. Best iteration is:\n",
      "\t[1614]\tvalid_set's rmse: 0.838556\n",
      "\t-0.8386\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -1.07s of remaining time.\n",
      "\t-0.7474\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.87s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels227\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.26455650389393437\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.26455650389393437,\n",
      "    \"mean_squared_error\": -0.06999014375258135,\n",
      "    \"mean_absolute_error\": -0.14236443248294173,\n",
      "    \"r2\": 0.9931327130418062,\n",
      "    \"pearsonr\": 0.9966044782126493,\n",
      "    \"median_absolute_error\": -0.08170640192654108\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels228\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.39 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 853\n",
      "Label Column: 853\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.1069356553, -11.6663654014, -2.07123, 2.6329)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60688.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.24 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 538 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 538 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t538 features in original data used to generate 538 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 44.93 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.523154\n",
      "[2000]\tvalid_set's rmse: 0.517505\n",
      "[3000]\tvalid_set's rmse: 0.516945\n",
      "[4000]\tvalid_set's rmse: 0.516815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5168\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.42s of the 164.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.495547\n",
      "[2000]\tvalid_set's rmse: 0.493576\n",
      "[3000]\tvalid_set's rmse: 0.493544\n",
      "[4000]\tvalid_set's rmse: 0.493536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4935\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 146.07s of the 146.07s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7075.\n",
      "\t-0.4576\t = Validation score   (-root_mean_squared_error)\n",
      "\t146.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.64s of remaining time.\n",
      "\t-0.4566\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.41s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels228\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14471181075595013\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14471181075595013,\n",
      "    \"mean_squared_error\": -0.020941508172265928,\n",
      "    \"mean_absolute_error\": -0.04079358601387113,\n",
      "    \"r2\": 0.9969787981384071,\n",
      "    \"pearsonr\": 0.9984917119293337,\n",
      "    \"median_absolute_error\": -0.005848452843255525\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels229\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.31 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 854\n",
      "Label Column: 854\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.9857277911, -8.9103144276, 1.76303, 2.62009)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60759.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.33 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 539 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 539 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t539 features in original data used to generate 539 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.02 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.34s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.657876\n",
      "[2000]\tvalid_set's rmse: 0.652915\n",
      "[3000]\tvalid_set's rmse: 0.652328\n",
      "[4000]\tvalid_set's rmse: 0.652108\n",
      "[5000]\tvalid_set's rmse: 0.652005\n",
      "[6000]\tvalid_set's rmse: 0.651995\n",
      "[7000]\tvalid_set's rmse: 0.65199\n",
      "[8000]\tvalid_set's rmse: 0.651988\n",
      "[9000]\tvalid_set's rmse: 0.651989\n",
      "[10000]\tvalid_set's rmse: 0.651988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.652\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.15s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 149.92s of the 149.91s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.643725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6435\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 143.63s of the 143.62s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6864.\n",
      "\t-0.6012\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.34s of the -0.64s of remaining time.\n",
      "\t-0.6006\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.42s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels229\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19149935228606701\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19149935228606701,\n",
      "    \"mean_squared_error\": -0.036672001925983334,\n",
      "    \"mean_absolute_error\": -0.06310166593799751,\n",
      "    \"r2\": 0.9946575233815167,\n",
      "    \"pearsonr\": 0.9973319231646283,\n",
      "    \"median_absolute_error\": -0.016487905215930132\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels230\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.23 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 855\n",
      "Label Column: 855\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.7233282516, -12.8487012894, -1.61098, 2.6453)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60698.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.41 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 540 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 540 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t540 features in original data used to generate 540 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.1 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.532411\n",
      "[2000]\tvalid_set's rmse: 0.527381\n",
      "[3000]\tvalid_set's rmse: 0.526567\n",
      "[4000]\tvalid_set's rmse: 0.526434\n",
      "[5000]\tvalid_set's rmse: 0.526378\n",
      "[6000]\tvalid_set's rmse: 0.526367\n",
      "[7000]\tvalid_set's rmse: 0.52636\n",
      "[8000]\tvalid_set's rmse: 0.52636\n",
      "[9000]\tvalid_set's rmse: 0.52636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5264\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.62s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 151.79s of the 151.79s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.534152\n",
      "[2000]\tvalid_set's rmse: 0.532741\n",
      "[3000]\tvalid_set's rmse: 0.532588\n",
      "[4000]\tvalid_set's rmse: 0.532564\n",
      "[5000]\tvalid_set's rmse: 0.532563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5326\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 129.98s of the 129.98s of remaining time.\n",
      "\t-0.4777\t = Validation score   (-root_mean_squared_error)\n",
      "\t120.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 9.18s of the 9.18s of remaining time.\n",
      "\t-0.5898\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.21s of remaining time.\n",
      "\t-0.4777\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.04s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels230\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15448747633295434\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15448747633295434,\n",
      "    \"mean_squared_error\": -0.023866380343725277,\n",
      "    \"mean_absolute_error\": -0.0590296004539868,\n",
      "    \"r2\": 0.996589027544057,\n",
      "    \"pearsonr\": 0.9983029696839687,\n",
      "    \"median_absolute_error\": -0.021807080230322295\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels231\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.15 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 856\n",
      "Label Column: 856\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.6268978041, -11.7069003405, 2.03691, 3.0274)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60579.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.49 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 541 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 541 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t541 features in original data used to generate 541 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.18 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.747976\n",
      "[2000]\tvalid_set's rmse: 0.742632\n",
      "[3000]\tvalid_set's rmse: 0.741285\n",
      "[4000]\tvalid_set's rmse: 0.741004\n",
      "[5000]\tvalid_set's rmse: 0.740986\n",
      "[6000]\tvalid_set's rmse: 0.740956\n",
      "[7000]\tvalid_set's rmse: 0.740953\n",
      "[8000]\tvalid_set's rmse: 0.740951\n",
      "[9000]\tvalid_set's rmse: 0.74095\n",
      "[10000]\tvalid_set's rmse: 0.740949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7409\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.94s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 148.88s of the 148.88s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.747795\n",
      "[2000]\tvalid_set's rmse: 0.74522\n",
      "[3000]\tvalid_set's rmse: 0.745168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7451\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 135.09s of the 135.08s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6499.\n",
      "\t-0.6676\t = Validation score   (-root_mean_squared_error)\n",
      "\t135.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -0.39s of remaining time.\n",
      "\t-0.6676\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels231\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2120553870423907\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2120553870423907,\n",
      "    \"mean_squared_error\": -0.04496748717369816,\n",
      "    \"mean_absolute_error\": -0.06462510716090249,\n",
      "    \"r2\": 0.9950931812182988,\n",
      "    \"pearsonr\": 0.9975479505655628,\n",
      "    \"median_absolute_error\": -0.012112026999900882\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels232\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   50.06 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 857\n",
      "Label Column: 857\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.5820164467, -18.6382553662, -1.3883, 2.90359)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60612.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.58 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 542 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 542 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t542 features in original data used to generate 542 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.27 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.654018\n",
      "[2000]\tvalid_set's rmse: 0.648861\n",
      "[3000]\tvalid_set's rmse: 0.648289\n",
      "[4000]\tvalid_set's rmse: 0.648142\n",
      "[5000]\tvalid_set's rmse: 0.648078\n",
      "[6000]\tvalid_set's rmse: 0.648059\n",
      "[7000]\tvalid_set's rmse: 0.64806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6481\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.87s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 155.41s of the 155.41s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.60846\n",
      "[2000]\tvalid_set's rmse: 0.606923\n",
      "[3000]\tvalid_set's rmse: 0.606737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6067\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 139.39s of the 139.39s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6677.\n",
      "\t-0.5667\t = Validation score   (-root_mean_squared_error)\n",
      "\t139.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.86s of remaining time.\n",
      "\t-0.5657\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.66s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels232\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17959473873252244\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17959473873252244,\n",
      "    \"mean_squared_error\": -0.032254270180403055,\n",
      "    \"mean_absolute_error\": -0.0512406186363428,\n",
      "    \"r2\": 0.9961738863581008,\n",
      "    \"pearsonr\": 0.9980881029520249,\n",
      "    \"median_absolute_error\": -0.010005343892150798\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels233\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.98 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 858\n",
      "Label Column: 858\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.2516478794, -15.6283748981, 2.87056, 3.66176)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60634.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.66 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 543 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 543 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t543 features in original data used to generate 543 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.35 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.694436\n",
      "[2000]\tvalid_set's rmse: 0.687319\n",
      "[3000]\tvalid_set's rmse: 0.686271\n",
      "[4000]\tvalid_set's rmse: 0.686212\n",
      "[5000]\tvalid_set's rmse: 0.686135\n",
      "[6000]\tvalid_set's rmse: 0.686109\n",
      "[7000]\tvalid_set's rmse: 0.686106\n",
      "[8000]\tvalid_set's rmse: 0.686104\n",
      "[9000]\tvalid_set's rmse: 0.686104\n",
      "[10000]\tvalid_set's rmse: 0.686104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6861\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.67s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 149.67s of the 149.67s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.625007\n",
      "[2000]\tvalid_set's rmse: 0.623519\n",
      "[3000]\tvalid_set's rmse: 0.623329\n",
      "[4000]\tvalid_set's rmse: 0.62328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6233\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 130.62s of the 130.62s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6271.\n",
      "\t-0.5562\t = Validation score   (-root_mean_squared_error)\n",
      "\t130.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.63s of remaining time.\n",
      "\t-0.5561\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.37s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels233\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.176976003962101\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.176976003962101,\n",
      "    \"mean_squared_error\": -0.03132050597839359,\n",
      "    \"mean_absolute_error\": -0.05591165161020785,\n",
      "    \"r2\": 0.9976639104894199,\n",
      "    \"pearsonr\": 0.9988336414406253,\n",
      "    \"median_absolute_error\": -0.012396505378808653\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels234\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.89 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 859\n",
      "Label Column: 859\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (21.4038875232, -17.089718514, -1.07235, 3.29943)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60633.11 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.74 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 544 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 544 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t544 features in original data used to generate 544 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.44 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.812109\n",
      "[2000]\tvalid_set's rmse: 0.808003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8075\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.66s of the 171.66s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.764146\n",
      "[2000]\tvalid_set's rmse: 0.762131\n",
      "[3000]\tvalid_set's rmse: 0.761873\n",
      "[4000]\tvalid_set's rmse: 0.76184\n",
      "[5000]\tvalid_set's rmse: 0.761836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7618\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.78s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 148.06s of the 148.06s of remaining time.\n",
      "\t-0.7509\t = Validation score   (-root_mean_squared_error)\n",
      "\t73.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 74.19s of the 74.19s of remaining time.\n",
      "\t-0.7989\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 47.94s of the 47.94s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.786884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1890. Best iteration is:\n",
      "\t[1680]\tvalid_set's rmse: 0.786756\n",
      "\t-0.7868\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -1.21s of remaining time.\n",
      "\t-0.7406\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.98s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels234\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24620574461801123\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24620574461801123,\n",
      "    \"mean_squared_error\": -0.06061726868290934,\n",
      "    \"mean_absolute_error\": -0.11021544655308818,\n",
      "    \"r2\": 0.9944312162955307,\n",
      "    \"pearsonr\": 0.9972287342116015,\n",
      "    \"median_absolute_error\": -0.052824519160595385\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels235\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.80 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 860\n",
      "Label Column: 860\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.3700811114, -10.8427326853, 2.9332, 3.70714)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60661.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.83 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 545 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 545 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t545 features in original data used to generate 545 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.52 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.616504\n",
      "[2000]\tvalid_set's rmse: 0.612595\n",
      "[3000]\tvalid_set's rmse: 0.612498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6124\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.09s of the 169.08s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.59947\n",
      "[2000]\tvalid_set's rmse: 0.597605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5976\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.45s of the 156.45s of remaining time.\n",
      "\t-0.5246\t = Validation score   (-root_mean_squared_error)\n",
      "\t146.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 9.49s of the 9.49s of remaining time.\n",
      "\t-0.6332\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.66s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -0.24s of remaining time.\n",
      "\t-0.5244\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.99s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels235\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.167782772788171\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.167782772788171,\n",
      "    \"mean_squared_error\": -0.028151058844486983,\n",
      "    \"mean_absolute_error\": -0.05788527829866193,\n",
      "    \"r2\": 0.997951396036405,\n",
      "    \"pearsonr\": 0.998982126138264,\n",
      "    \"median_absolute_error\": -0.016248163414160022\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels236\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.74 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 861\n",
      "Label Column: 861\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.0242766872, -20.3062146082, -0.4156, 3.79477)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60759.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.91 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 546 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 546 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t546 features in original data used to generate 546 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.6 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.649006\n",
      "[2000]\tvalid_set's rmse: 0.645544\n",
      "[3000]\tvalid_set's rmse: 0.645259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6452\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.68s of the 168.68s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.623205\n",
      "[2000]\tvalid_set's rmse: 0.621155\n",
      "[3000]\tvalid_set's rmse: 0.621023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.621\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 151.53s of the 151.53s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7139.\n",
      "\t-0.5532\t = Validation score   (-root_mean_squared_error)\n",
      "\t151.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.76s of remaining time.\n",
      "\t-0.5528\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.54s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels236\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17511096876999047\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17511096876999047,\n",
      "    \"mean_squared_error\": -0.030663851383564762,\n",
      "    \"mean_absolute_error\": -0.04699561131221995,\n",
      "    \"r2\": 0.997870404189802,\n",
      "    \"pearsonr\": 0.9989355685831399,\n",
      "    \"median_absolute_error\": -0.006241926988287294\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels237\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.66 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 862\n",
      "Label Column: 862\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (24.0373324441, -13.3918129506, 2.58715, 3.7958)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60738.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 71.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 547 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 547 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t547 features in original data used to generate 547 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.69 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.977872\n",
      "[2000]\tvalid_set's rmse: 0.971646\n",
      "[3000]\tvalid_set's rmse: 0.970223\n",
      "[4000]\tvalid_set's rmse: 0.969901\n",
      "[5000]\tvalid_set's rmse: 0.969819\n",
      "[6000]\tvalid_set's rmse: 0.969804\n",
      "[7000]\tvalid_set's rmse: 0.969798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9698\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 156.47s of the 156.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.886744\n",
      "[2000]\tvalid_set's rmse: 0.88412\n",
      "[3000]\tvalid_set's rmse: 0.883763\n",
      "[4000]\tvalid_set's rmse: 0.883732\n",
      "[5000]\tvalid_set's rmse: 0.883724\n",
      "[6000]\tvalid_set's rmse: 0.883721\n",
      "[7000]\tvalid_set's rmse: 0.88372\n",
      "[8000]\tvalid_set's rmse: 0.88372\n",
      "[9000]\tvalid_set's rmse: 0.88372\n",
      "[10000]\tvalid_set's rmse: 0.88372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8837\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.14s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 111.85s of the 111.85s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5404.\n",
      "\t-0.8592\t = Validation score   (-root_mean_squared_error)\n",
      "\t111.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -0.6s of remaining time.\n",
      "\t-0.8535\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.34s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels237\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2719391792140011\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2719391792140011,\n",
      "    \"mean_squared_error\": -0.07395091719158448,\n",
      "    \"mean_absolute_error\": -0.08606420704758015,\n",
      "    \"r2\": 0.9948669197432392,\n",
      "    \"pearsonr\": 0.9974441630676365,\n",
      "    \"median_absolute_error\": -0.021898992598168743\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels238\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.57 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 863\n",
      "Label Column: 863\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.4306816635, -28.628268781, 0.16253, 4.20007)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60747.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.08 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 548 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 548 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t548 features in original data used to generate 548 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.77 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.919544\n",
      "[2000]\tvalid_set's rmse: 0.915593\n",
      "[3000]\tvalid_set's rmse: 0.915248\n",
      "[4000]\tvalid_set's rmse: 0.91522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9152\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.79s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.05s of the 164.05s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.796077\n",
      "[2000]\tvalid_set's rmse: 0.794337\n",
      "[3000]\tvalid_set's rmse: 0.79428\n",
      "[4000]\tvalid_set's rmse: 0.794289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7943\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.8s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.8s of the 144.79s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6817.\n",
      "\t-0.7306\t = Validation score   (-root_mean_squared_error)\n",
      "\t144.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.89s of remaining time.\n",
      "\t-0.7305\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.68s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels238\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.23159717673570862\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.23159717673570862,\n",
      "    \"mean_squared_error\": -0.05363725227195121,\n",
      "    \"mean_absolute_error\": -0.06285448652574403,\n",
      "    \"r2\": 0.9969591444849842,\n",
      "    \"pearsonr\": 0.9984797562090366,\n",
      "    \"median_absolute_error\": -0.010314501390751546\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels239\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.49 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 864\n",
      "Label Column: 864\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.9785547269, -14.7850447766, 2.47116, 3.62684)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60723.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.16 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 549 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 549 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t549 features in original data used to generate 549 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.85 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.7084\n",
      "[2000]\tvalid_set's rmse: 0.704332\n",
      "[3000]\tvalid_set's rmse: 0.703917\n",
      "[4000]\tvalid_set's rmse: 0.703847\n",
      "[5000]\tvalid_set's rmse: 0.703793\n",
      "[6000]\tvalid_set's rmse: 0.703794\n",
      "[7000]\tvalid_set's rmse: 0.703787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7038\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.56s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 157.06s of the 157.06s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.666899\n",
      "[2000]\tvalid_set's rmse: 0.665787\n",
      "[3000]\tvalid_set's rmse: 0.66564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6656\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 140.59s of the 140.59s of remaining time.\n",
      "\t-0.61\t = Validation score   (-root_mean_squared_error)\n",
      "\t71.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 68.88s of the 68.88s of remaining time.\n",
      "\t-0.7178\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.14s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 42.62s of the 42.62s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.72258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1651. Best iteration is:\n",
      "\t[1503]\tvalid_set's rmse: 0.722555\n",
      "\t-0.7226\t = Validation score   (-root_mean_squared_error)\n",
      "\t42.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -1.08s of remaining time.\n",
      "\t-0.6093\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.84s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels239\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22222219607784924\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22222219607784924,\n",
      "    \"mean_squared_error\": -0.04938270442966214,\n",
      "    \"mean_absolute_error\": -0.12637302887096452,\n",
      "    \"r2\": 0.9962454304224423,\n",
      "    \"pearsonr\": 0.9981348994649455,\n",
      "    \"median_absolute_error\": -0.07810230152581166\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels240\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.39 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 865\n",
      "Label Column: 865\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (24.2443937348, -30.548846131, 0.65778, 4.52668)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60655.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.24 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 550 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 550 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t550 features in original data used to generate 550 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 45.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.02036\n",
      "[2000]\tvalid_set's rmse: 1.01447\n",
      "[3000]\tvalid_set's rmse: 1.01372\n",
      "[4000]\tvalid_set's rmse: 1.01357\n",
      "[5000]\tvalid_set's rmse: 1.01348\n",
      "[6000]\tvalid_set's rmse: 1.01347\n",
      "[7000]\tvalid_set's rmse: 1.01347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0135\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.57s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 156.02s of the 156.01s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.905143\n",
      "[2000]\tvalid_set's rmse: 0.903584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9035\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.18s of the 144.17s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6791.\n",
      "\t-0.8394\t = Validation score   (-root_mean_squared_error)\n",
      "\t144.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.87s of remaining time.\n",
      "\t-0.8392\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.66s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels240\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2662500035141081\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2662500035141081,\n",
      "    \"mean_squared_error\": -0.07088906437126294,\n",
      "    \"mean_absolute_error\": -0.07331184714396496,\n",
      "    \"r2\": 0.9965401165981278,\n",
      "    \"pearsonr\": 0.998274022310804,\n",
      "    \"median_absolute_error\": -0.01313588257096291\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels241\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.32 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 866\n",
      "Label Column: 866\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.0572754262, -17.8973157618, 2.50336, 3.85275)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60640.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.33 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 551 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 551 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t551 features in original data used to generate 551 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.02 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.776533\n",
      "[2000]\tvalid_set's rmse: 0.77003\n",
      "[3000]\tvalid_set's rmse: 0.768848\n",
      "[4000]\tvalid_set's rmse: 0.768629\n",
      "[5000]\tvalid_set's rmse: 0.768644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7686\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.29s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.53s of the 162.53s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.67034\n",
      "[2000]\tvalid_set's rmse: 0.668967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6689\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 149.47s of the 149.46s of remaining time.\n",
      "\t-0.6183\t = Validation score   (-root_mean_squared_error)\n",
      "\t124.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 24.59s of the 24.59s of remaining time.\n",
      "\t-0.7214\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.77s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.65s of remaining time.\n",
      "\t-0.6173\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.45s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels241\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19927558260947129\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19927558260947129,\n",
      "    \"mean_squared_error\": -0.03971075782434437,\n",
      "    \"mean_absolute_error\": -0.07345280620373161,\n",
      "    \"r2\": 0.9973244859153856,\n",
      "    \"pearsonr\": 0.998663935189197,\n",
      "    \"median_absolute_error\": -0.026711545020178223\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels242\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.24 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 867\n",
      "Label Column: 867\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (30.7123772305, -28.7263318836, 1.72881, 5.19647)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60656.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.41 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 552 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 552 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t552 features in original data used to generate 552 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.1 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.41273\n",
      "[2000]\tvalid_set's rmse: 1.40582\n",
      "[3000]\tvalid_set's rmse: 1.40445\n",
      "[4000]\tvalid_set's rmse: 1.40411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.4041\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.04s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.78s of the 163.78s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.30471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.3041\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.67s of the 156.67s of remaining time.\n",
      "\t-1.281\t = Validation score   (-root_mean_squared_error)\n",
      "\t111.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 45.18s of the 45.18s of remaining time.\n",
      "\t-1.339\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.27s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 11.44s of the 11.44s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 404. Best iteration is:\n",
      "\t[404]\tvalid_set's rmse: 1.38886\n",
      "\t-1.3889\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.38s of remaining time.\n",
      "\t-1.2683\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels242\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.4122935340884149\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.4122935340884149,\n",
      "    \"mean_squared_error\": -0.16998595825111415,\n",
      "    \"mean_absolute_error\": -0.15484320601655793,\n",
      "    \"r2\": 0.993704390790172,\n",
      "    \"pearsonr\": 0.9968659217414836,\n",
      "    \"median_absolute_error\": -0.0662103235245115\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels243\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.17 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 868\n",
      "Label Column: 868\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (34.9106360245, -29.2802409453, 2.16553, 5.62413)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60654.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.5 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 553 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 553 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t553 features in original data used to generate 553 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.19 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.675864\n",
      "[2000]\tvalid_set's rmse: 0.672581\n",
      "[3000]\tvalid_set's rmse: 0.672475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6724\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.25s of the 169.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.526886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5263\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 161.29s of the 161.29s of remaining time.\n",
      "\t-0.4807\t = Validation score   (-root_mean_squared_error)\n",
      "\t147.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 12.81s of the 12.81s of remaining time.\n",
      "\t-0.5615\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.56s of remaining time.\n",
      "\t-0.4739\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.35s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels243\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1520118651021122\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1520118651021122,\n",
      "    \"mean_squared_error\": -0.023107607131822742,\n",
      "    \"mean_absolute_error\": -0.05161414388296042,\n",
      "    \"r2\": 0.9992693888956552,\n",
      "    \"pearsonr\": 0.9996346563683742,\n",
      "    \"median_absolute_error\": -0.017159699732585243\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels244\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.10 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 869\n",
      "Label Column: 869\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.6098800238, -17.6228480147, 2.04341, 3.96793)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60643.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.58 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 554 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 554 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t554 features in original data used to generate 554 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.27 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.883004\n",
      "[2000]\tvalid_set's rmse: 0.877028\n",
      "[3000]\tvalid_set's rmse: 0.87614\n",
      "[4000]\tvalid_set's rmse: 0.876008\n",
      "[5000]\tvalid_set's rmse: 0.876046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.876\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.41s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.08s of the 162.08s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.800913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8005\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 154.71s of the 154.71s of remaining time.\n",
      "\t-0.7477\t = Validation score   (-root_mean_squared_error)\n",
      "\t123.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 31.08s of the 31.08s of remaining time.\n",
      "\t-0.8453\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.26s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.64s of remaining time.\n",
      "\t-0.7466\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.43s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels244\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.242575309423027\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.242575309423027,\n",
      "    \"mean_squared_error\": -0.058842780741677134,\n",
      "    \"mean_absolute_error\": -0.09254175575037558,\n",
      "    \"r2\": 0.9962622840194562,\n",
      "    \"pearsonr\": 0.9981418332197654,\n",
      "    \"median_absolute_error\": -0.037214581816229386\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels245\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   49.03 GB / 2000.36 GB (2.5%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 870\n",
      "Label Column: 870\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.6539139323, -15.8874267592, 2.46544, 4.03205)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60645.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.66 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 555 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 555 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t555 features in original data used to generate 555 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.35 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.734016\n",
      "[2000]\tvalid_set's rmse: 0.726181\n",
      "[3000]\tvalid_set's rmse: 0.724993\n",
      "[4000]\tvalid_set's rmse: 0.72493\n",
      "[5000]\tvalid_set's rmse: 0.724968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7249\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.13s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.71s of the 162.71s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.68512\n",
      "[2000]\tvalid_set's rmse: 0.684044\n",
      "[3000]\tvalid_set's rmse: 0.683926\n",
      "[4000]\tvalid_set's rmse: 0.683898\n",
      "[5000]\tvalid_set's rmse: 0.683892\n",
      "[6000]\tvalid_set's rmse: 0.683892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6839\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.09s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 133.67s of the 133.67s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6316.\n",
      "\t-0.6227\t = Validation score   (-root_mean_squared_error)\n",
      "\t133.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.67s of remaining time.\n",
      "\t-0.6214\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels245\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19703012800081648\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19703012800081648,\n",
      "    \"mean_squared_error\": -0.03882087134001814,\n",
      "    \"mean_absolute_error\": -0.05449632344088187,\n",
      "    \"r2\": 0.9976118836730884,\n",
      "    \"pearsonr\": 0.9988077998205931,\n",
      "    \"median_absolute_error\": -0.008894584861514454\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels246\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.95 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 871\n",
      "Label Column: 871\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (31.1720759534, -21.6557399096, 2.95036, 5.30728)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60645.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.75 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 556 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 556 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t556 features in original data used to generate 556 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.44 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.33s of the 179.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.49735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.4949\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 172.87s of the 172.87s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.41832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.4169\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 166.75s of the 166.75s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7664.\n",
      "\t-1.3699\t = Validation score   (-root_mean_squared_error)\n",
      "\t166.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.33s of the -0.92s of remaining time.\n",
      "\t-1.3646\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.68s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels246\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.4365008674862645\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.4365008674862645,\n",
      "    \"mean_squared_error\": -0.19053300731626155,\n",
      "    \"mean_absolute_error\": -0.1483881462527403,\n",
      "    \"r2\": 0.9932349994866425,\n",
      "    \"pearsonr\": 0.9966221936871686,\n",
      "    \"median_absolute_error\": -0.04609685674214106\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels247\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.88 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 872\n",
      "Label Column: 872\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (26.4533389753, -17.0736963311, 2.71889, 4.33412)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60745.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.83 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 557 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 557 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t557 features in original data used to generate 557 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.52 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.8672\n",
      "[2000]\tvalid_set's rmse: 0.863819\n",
      "[3000]\tvalid_set's rmse: 0.863738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8635\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.75s of the 168.75s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.837588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8373\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 161.55s of the 161.54s of remaining time.\n",
      "\t-0.7895\t = Validation score   (-root_mean_squared_error)\n",
      "\t113.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 47.29s of the 47.29s of remaining time.\n",
      "\t-0.8692\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.91s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 26.97s of the 26.96s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.894921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1019. Best iteration is:\n",
      "\t[1006]\tvalid_set's rmse: 0.894909\n",
      "\t-0.8949\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.93s of remaining time.\n",
      "\t-0.7878\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.71s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels247\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.25949251176794175\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.25949251176794175,\n",
      "    \"mean_squared_error\": -0.06733636366363535,\n",
      "    \"mean_absolute_error\": -0.11009907490668812,\n",
      "    \"r2\": 0.9964149973168875,\n",
      "    \"pearsonr\": 0.9982100442383165,\n",
      "    \"median_absolute_error\": -0.048436133119799885\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels248\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.81 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 873\n",
      "Label Column: 873\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.25170714, -11.1612777401, 2.51123, 3.36529)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60714.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.91 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 558 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 558 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t558 features in original data used to generate 558 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.6 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.745289\n",
      "[2000]\tvalid_set's rmse: 0.740756\n",
      "[3000]\tvalid_set's rmse: 0.739576\n",
      "[4000]\tvalid_set's rmse: 0.739378\n",
      "[5000]\tvalid_set's rmse: 0.739331\n",
      "[6000]\tvalid_set's rmse: 0.739318\n",
      "[7000]\tvalid_set's rmse: 0.739307\n",
      "[8000]\tvalid_set's rmse: 0.739306\n",
      "[9000]\tvalid_set's rmse: 0.739305\n",
      "[10000]\tvalid_set's rmse: 0.739305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7393\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.68s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 148.23s of the 148.22s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.710324\n",
      "[2000]\tvalid_set's rmse: 0.709448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7093\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 134.96s of the 134.95s of remaining time.\n",
      "\t-0.6991\t = Validation score   (-root_mean_squared_error)\n",
      "\t94.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 39.92s of the 39.92s of remaining time.\n",
      "\t-0.7435\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.14s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.44s of remaining time.\n",
      "\t-0.6878\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels248\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.22488188109042803\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.22488188109042803,\n",
      "    \"mean_squared_error\": -0.05057186044276948,\n",
      "    \"mean_absolute_error\": -0.09240560921970838,\n",
      "    \"r2\": 0.995534145735893,\n",
      "    \"pearsonr\": 0.997772530424168,\n",
      "    \"median_absolute_error\": -0.038304391449694775\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels249\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.71 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 874\n",
      "Label Column: 874\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.7834018985, -12.269705784, 2.53651, 3.12729)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60642.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.0 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 559 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 559 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t559 features in original data used to generate 559 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.69 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.687414\n",
      "[2000]\tvalid_set's rmse: 0.685434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6851\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.29s of the 171.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.676794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6762\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 163.41s of the 163.41s of remaining time.\n",
      "\t-0.6227\t = Validation score   (-root_mean_squared_error)\n",
      "\t87.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 75.52s of the 75.52s of remaining time.\n",
      "\t-0.7054\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 34.72s of the 34.72s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.725487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1317. Best iteration is:\n",
      "\t[1313]\tvalid_set's rmse: 0.725465\n",
      "\t-0.7255\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -1.0s of remaining time.\n",
      "\t-0.6221\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.78s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels249\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21666160244602323\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21666160244602323,\n",
      "    \"mean_squared_error\": -0.04694224997447871,\n",
      "    \"mean_absolute_error\": -0.11098351739848887,\n",
      "    \"r2\": 0.9951996822935808,\n",
      "    \"pearsonr\": 0.9976159799979396,\n",
      "    \"median_absolute_error\": -0.06236080224570273\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels250\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.63 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 875\n",
      "Label Column: 875\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (19.2219683936, -16.0314090081, 2.53118, 3.39632)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60833.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.08 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 560 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 560 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t560 features in original data used to generate 560 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.77 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.729525\n",
      "[2000]\tvalid_set's rmse: 0.727371\n",
      "[3000]\tvalid_set's rmse: 0.726911\n",
      "[4000]\tvalid_set's rmse: 0.726903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7269\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.77s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 165.12s of the 165.12s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.688028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6873\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 157.22s of the 157.22s of remaining time.\n",
      "\t-0.6306\t = Validation score   (-root_mean_squared_error)\n",
      "\t88.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 68.12s of the 68.12s of remaining time.\n",
      "\t-0.7315\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.96s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 41.7s of the 41.7s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.74469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7447\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.11s of remaining time.\n",
      "\t-0.6304\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 180.83s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels250\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21628837308020094\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21628837308020094,\n",
      "    \"mean_squared_error\": -0.046780660329680264,\n",
      "    \"mean_absolute_error\": -0.10581702964397026,\n",
      "    \"r2\": 0.995944071663865,\n",
      "    \"pearsonr\": 0.9979827150151575,\n",
      "    \"median_absolute_error\": -0.05901773162286372\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels251\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.55 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 876\n",
      "Label Column: 876\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (24.4911082977, -15.8194079671, 2.15292, 3.6927)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60715.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.16 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 561 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 561 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t561 features in original data used to generate 561 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.85 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.776858\n",
      "[2000]\tvalid_set's rmse: 0.772294\n",
      "[3000]\tvalid_set's rmse: 0.771598\n",
      "[4000]\tvalid_set's rmse: 0.7715\n",
      "[5000]\tvalid_set's rmse: 0.771476\n",
      "[6000]\tvalid_set's rmse: 0.771477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7715\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.89s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.83s of the 159.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.714327\n",
      "[2000]\tvalid_set's rmse: 0.713555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7134\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 149.3s of the 149.3s of remaining time.\n",
      "\t-0.6631\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 103.44s of the 103.44s of remaining time.\n",
      "\t-0.7647\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.12s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 72.19s of the 72.19s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.746673\n",
      "[2000]\tvalid_set's rmse: 0.746613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7466\t = Validation score   (-root_mean_squared_error)\n",
      "\t57.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the 13.99s of remaining time.\n",
      "\t-0.6628\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 166.75s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels251\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.28679326170104363\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.28679326170104363,\n",
      "    \"mean_squared_error\": -0.0822503749571232,\n",
      "    \"mean_absolute_error\": -0.19064085897951083,\n",
      "    \"r2\": 0.9939675830228563,\n",
      "    \"pearsonr\": 0.9970315155373765,\n",
      "    \"median_absolute_error\": -0.14168742150625924\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels252\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.45 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 877\n",
      "Label Column: 877\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (30.8863723879, -19.3273203454, 1.69281, 3.86932)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60827.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.25 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 562 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 562 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t562 features in original data used to generate 562 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 46.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.952027\n",
      "[2000]\tvalid_set's rmse: 0.950408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9501\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 171.31s of the 171.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.861874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8614\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 164.07s of the 164.07s of remaining time.\n",
      "\t-0.8397\t = Validation score   (-root_mean_squared_error)\n",
      "\t102.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 61.09s of the 61.09s of remaining time.\n",
      "\t-0.9224\t = Validation score   (-root_mean_squared_error)\n",
      "\t34.76s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 25.91s of the 25.91s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 959. Best iteration is:\n",
      "\t[951]\tvalid_set's rmse: 0.934489\n",
      "\t-0.9345\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.56s of remaining time.\n",
      "\t-0.8329\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.4s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels252\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2725148914348068\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2725148914348068,\n",
      "    \"mean_squared_error\": -0.0742643660537244,\n",
      "    \"mean_absolute_error\": -0.0992476660376112,\n",
      "    \"r2\": 0.9950391952191014,\n",
      "    \"pearsonr\": 0.9975361267523442,\n",
      "    \"median_absolute_error\": -0.04843759582629381\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels253\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.38 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 878\n",
      "Label Column: 878\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (30.9189892334, -20.6574079103, 1.69901, 4.03049)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60718.3 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.33 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 563 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 563 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t563 features in original data used to generate 563 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.02 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.870947\n",
      "[2000]\tvalid_set's rmse: 0.867111\n",
      "[3000]\tvalid_set's rmse: 0.86639\n",
      "[4000]\tvalid_set's rmse: 0.866414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8663\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 165.27s of the 165.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.800834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 157.22s of the 157.21s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7165.\n",
      "\t-0.7595\t = Validation score   (-root_mean_squared_error)\n",
      "\t157.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.67s of remaining time.\n",
      "\t-0.7579\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels253\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24009367908921778\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24009367908921778,\n",
      "    \"mean_squared_error\": -0.05764497473859642,\n",
      "    \"mean_absolute_error\": -0.05086606190618788,\n",
      "    \"r2\": 0.9964511462598016,\n",
      "    \"pearsonr\": 0.9982273862479033,\n",
      "    \"median_absolute_error\": -0.009959066279151968\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels254\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.31 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 879\n",
      "Label Column: 879\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (9.4943541548, -7.9528446712, -0.78492, 1.94649)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60719.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.41 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 564 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 564 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t564 features in original data used to generate 564 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.11 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.537038\n",
      "[2000]\tvalid_set's rmse: 0.534173\n",
      "[3000]\tvalid_set's rmse: 0.53396\n",
      "[4000]\tvalid_set's rmse: 0.533892\n",
      "[5000]\tvalid_set's rmse: 0.533885\n",
      "[6000]\tvalid_set's rmse: 0.53387\n",
      "[7000]\tvalid_set's rmse: 0.533872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5339\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.1s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 155.19s of the 155.18s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.491557\n",
      "[2000]\tvalid_set's rmse: 0.490427\n",
      "[3000]\tvalid_set's rmse: 0.49038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4904\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 140.52s of the 140.52s of remaining time.\n",
      "\t-0.4759\t = Validation score   (-root_mean_squared_error)\n",
      "\t84.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 55.61s of the 55.61s of remaining time.\n",
      "\t-0.4994\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.88s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 33.33s of the 33.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.505052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1257. Best iteration is:\n",
      "\t[1093]\tvalid_set's rmse: 0.505017\n",
      "\t-0.505\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.63s of remaining time.\n",
      "\t-0.4729\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels254\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16071993235078882\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16071993235078882,\n",
      "    \"mean_squared_error\": -0.025830896654842204,\n",
      "    \"mean_absolute_error\": -0.07930710244191258,\n",
      "    \"r2\": 0.993181709015773,\n",
      "    \"pearsonr\": 0.9966176021153497,\n",
      "    \"median_absolute_error\": -0.04182664775937506\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels255\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.21 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 880\n",
      "Label Column: 880\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (7.6840383128, -9.1337239971, -1.10851, 1.91219)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60711.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.5 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 565 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 565 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t565 features in original data used to generate 565 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.19 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.50383\n",
      "[2000]\tvalid_set's rmse: 0.501484\n",
      "[3000]\tvalid_set's rmse: 0.501243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5012\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.94s of the 166.94s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.498553\n",
      "[2000]\tvalid_set's rmse: 0.497129\n",
      "[3000]\tvalid_set's rmse: 0.497084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4971\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.0s of the 150.0s of remaining time.\n",
      "\t-0.4783\t = Validation score   (-root_mean_squared_error)\n",
      "\t93.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 56.23s of the 56.23s of remaining time.\n",
      "\t-0.5084\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.86s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 28.94s of the 28.94s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.508464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1082. Best iteration is:\n",
      "\t[1080]\tvalid_set's rmse: 0.508426\n",
      "\t-0.5084\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.91s of remaining time.\n",
      "\t-0.4771\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.64s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels255\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1607882147007632\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1607882147007632,\n",
      "    \"mean_squared_error\": -0.025852849986658766,\n",
      "    \"mean_absolute_error\": -0.07672237454441502,\n",
      "    \"r2\": 0.9929289109951812,\n",
      "    \"pearsonr\": 0.9964851337773235,\n",
      "    \"median_absolute_error\": -0.03811194107960983\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels256\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.13 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 881\n",
      "Label Column: 881\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (6.824545304, -9.8895698115, -1.27969, 2.02721)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60815.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.58 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 566 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 566 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t566 features in original data used to generate 566 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.27 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.40957\n",
      "[2000]\tvalid_set's rmse: 0.406896\n",
      "[3000]\tvalid_set's rmse: 0.406777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4067\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.74s of the 167.74s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.386576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3859\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 159.22s of the 159.22s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7215.\n",
      "\t-0.3706\t = Validation score   (-root_mean_squared_error)\n",
      "\t159.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -1.16s of remaining time.\n",
      "\t-0.3696\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.94s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels256\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11743582942002463\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11743582942002463,\n",
      "    \"mean_squared_error\": -0.01379117403156914,\n",
      "    \"mean_absolute_error\": -0.03650838500381341,\n",
      "    \"r2\": 0.9966438337128817,\n",
      "    \"pearsonr\": 0.9983226281133379,\n",
      "    \"median_absolute_error\": -0.007451237049118098\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels257\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   48.06 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 882\n",
      "Label Column: 882\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (6.0914024131, -9.7525901721, -1.17453, 2.09915)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60811.7 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.66 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 567 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 567 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t567 features in original data used to generate 567 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.36 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.430831\n",
      "[2000]\tvalid_set's rmse: 0.42652\n",
      "[3000]\tvalid_set's rmse: 0.425981\n",
      "[4000]\tvalid_set's rmse: 0.425848\n",
      "[5000]\tvalid_set's rmse: 0.42582\n",
      "[6000]\tvalid_set's rmse: 0.42581\n",
      "[7000]\tvalid_set's rmse: 0.425804\n",
      "[8000]\tvalid_set's rmse: 0.425804\n",
      "[9000]\tvalid_set's rmse: 0.425804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4258\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.07s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 150.06s of the 150.05s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.401371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4006\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 141.48s of the 141.48s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6473.\n",
      "\t-0.3764\t = Validation score   (-root_mean_squared_error)\n",
      "\t141.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.64s of remaining time.\n",
      "\t-0.3761\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.4s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels257\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.12004677818758522\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.12004677818758522,\n",
      "    \"mean_squared_error\": -0.01441122895321921,\n",
      "    \"mean_absolute_error\": -0.04033896705177855,\n",
      "    \"r2\": 0.9967291765034823,\n",
      "    \"pearsonr\": 0.9983659143355192,\n",
      "    \"median_absolute_error\": -0.010503471397268677\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels258\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.98 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 883\n",
      "Label Column: 883\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (8.8808878879, -10.2647867259, -0.76113, 2.25658)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60790.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.75 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 568 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 568 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t568 features in original data used to generate 568 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.44 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.546756\n",
      "[2000]\tvalid_set's rmse: 0.542711\n",
      "[3000]\tvalid_set's rmse: 0.542375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5423\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.39s of the 166.38s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.508088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5066\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 156.99s of the 156.99s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7098.\n",
      "\t-0.4807\t = Validation score   (-root_mean_squared_error)\n",
      "\t157.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.9s of remaining time.\n",
      "\t-0.48\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.65s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels258\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15234860659526178\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15234860659526178,\n",
      "    \"mean_squared_error\": -0.023210097931517945,\n",
      "    \"mean_absolute_error\": -0.04570160380961378,\n",
      "    \"r2\": 0.9954415622305485,\n",
      "    \"pearsonr\": 0.9977237342155638,\n",
      "    \"median_absolute_error\": -0.008286844515771219\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels259\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.91 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 884\n",
      "Label Column: 884\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (10.1474501426, -11.9339803016, -0.36749, 2.37516)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60813.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.83 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 569 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 569 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t569 features in original data used to generate 569 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.52 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.359115\n",
      "[2000]\tvalid_set's rmse: 0.355243\n",
      "[3000]\tvalid_set's rmse: 0.354614\n",
      "[4000]\tvalid_set's rmse: 0.35449\n",
      "[5000]\tvalid_set's rmse: 0.354475\n",
      "[6000]\tvalid_set's rmse: 0.354467\n",
      "[7000]\tvalid_set's rmse: 0.354465\n",
      "[8000]\tvalid_set's rmse: 0.354464\n",
      "[9000]\tvalid_set's rmse: 0.354464\n",
      "[10000]\tvalid_set's rmse: 0.354464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3545\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.93s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 147.14s of the 147.14s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.304267\n",
      "[2000]\tvalid_set's rmse: 0.303156\n",
      "[3000]\tvalid_set's rmse: 0.3031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3031\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 130.78s of the 130.78s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6006.\n",
      "\t-0.258\t = Validation score   (-root_mean_squared_error)\n",
      "\t130.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.84s of remaining time.\n",
      "\t-0.2579\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.65s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels259\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.08329575358294944\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.08329575358294944,\n",
      "    \"mean_squared_error\": -0.0069381825649514174,\n",
      "    \"mean_absolute_error\": -0.03136409062855791,\n",
      "    \"r2\": 0.9987700106782789,\n",
      "    \"pearsonr\": 0.9993871713217621,\n",
      "    \"median_absolute_error\": -0.011045599353759927\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels260\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.83 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 885\n",
      "Label Column: 885\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (9.7252355497, -11.1991918337, -0.05771, 2.38104)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60768.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 73.92 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 570 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 570 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t570 features in original data used to generate 570 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.61 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.382537\n",
      "[2000]\tvalid_set's rmse: 0.37875\n",
      "[3000]\tvalid_set's rmse: 0.37841\n",
      "[4000]\tvalid_set's rmse: 0.378407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3784\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.14s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.72s of the 163.72s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.35612\n",
      "[2000]\tvalid_set's rmse: 0.355673\n",
      "[3000]\tvalid_set's rmse: 0.355547\n",
      "[4000]\tvalid_set's rmse: 0.355529\n",
      "[5000]\tvalid_set's rmse: 0.355528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3555\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.21s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 135.95s of the 135.94s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6235.\n",
      "\t-0.3268\t = Validation score   (-root_mean_squared_error)\n",
      "\t136.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.64s of remaining time.\n",
      "\t-0.3266\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.47s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels260\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.10371174358736401\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.10371174358736401,\n",
      "    \"mean_squared_error\": -0.010756125757931133,\n",
      "    \"mean_absolute_error\": -0.030912809667305485,\n",
      "    \"r2\": 0.9981025747585826,\n",
      "    \"pearsonr\": 0.9990512239539289,\n",
      "    \"median_absolute_error\": -0.0059506865020751976\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels261\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.75 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 886\n",
      "Label Column: 886\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.1303203609, -15.3299016553, 0.12407, 2.59526)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60766.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.0 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 571 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 571 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t571 features in original data used to generate 571 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.69 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.512664\n",
      "[2000]\tvalid_set's rmse: 0.507354\n",
      "[3000]\tvalid_set's rmse: 0.506564\n",
      "[4000]\tvalid_set's rmse: 0.506323\n",
      "[5000]\tvalid_set's rmse: 0.506276\n",
      "[6000]\tvalid_set's rmse: 0.506256\n",
      "[7000]\tvalid_set's rmse: 0.506256\n",
      "[8000]\tvalid_set's rmse: 0.506256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5063\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.03s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 152.16s of the 152.16s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.463943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4638\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.74s of the 144.74s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6581.\n",
      "\t-0.4166\t = Validation score   (-root_mean_squared_error)\n",
      "\t144.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -0.64s of remaining time.\n",
      "\t-0.4166\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels261\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13270820253967172\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13270820253967172,\n",
      "    \"mean_squared_error\": -0.01761146702131061,\n",
      "    \"mean_absolute_error\": -0.04166124479322374,\n",
      "    \"r2\": 0.9973849770842422,\n",
      "    \"pearsonr\": 0.9986937506025689,\n",
      "    \"median_absolute_error\": -0.010151041123315774\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels262\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.68 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 887\n",
      "Label Column: 887\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.3523479292, -9.2978293879, 0.13263, 2.69776)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60774.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.08 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 572 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 572 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t572 features in original data used to generate 572 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.77 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.57227\n",
      "[2000]\tvalid_set's rmse: 0.568866\n",
      "[3000]\tvalid_set's rmse: 0.568413\n",
      "[4000]\tvalid_set's rmse: 0.568184\n",
      "[5000]\tvalid_set's rmse: 0.568177\n",
      "[6000]\tvalid_set's rmse: 0.568169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5682\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 157.34s of the 157.34s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.526266\n",
      "[2000]\tvalid_set's rmse: 0.525217\n",
      "[3000]\tvalid_set's rmse: 0.524948\n",
      "[4000]\tvalid_set's rmse: 0.524926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5249\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 136.19s of the 136.19s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6218.\n",
      "\t-0.492\t = Validation score   (-root_mean_squared_error)\n",
      "\t136.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.63s of remaining time.\n",
      "\t-0.4915\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.5s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels262\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.15691638047936154\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.15691638047936154,\n",
      "    \"mean_squared_error\": -0.0246227504627438,\n",
      "    \"mean_absolute_error\": -0.053180457100390266,\n",
      "    \"r2\": 0.9966164550471085,\n",
      "    \"pearsonr\": 0.9983076498622842,\n",
      "    \"median_absolute_error\": -0.013880139029840116\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels263\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.59 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 888\n",
      "Label Column: 888\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.8895344658, -8.662844409, 0.14719, 2.51561)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60780.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.17 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 573 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 573 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t573 features in original data used to generate 573 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.86 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.219233\n",
      "[2000]\tvalid_set's rmse: 0.217589\n",
      "[3000]\tvalid_set's rmse: 0.217399\n",
      "[4000]\tvalid_set's rmse: 0.217346\n",
      "[5000]\tvalid_set's rmse: 0.217335\n",
      "[6000]\tvalid_set's rmse: 0.217336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2173\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.03s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.68s of the 159.68s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.181249\n",
      "[2000]\tvalid_set's rmse: 0.180961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1809\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 145.46s of the 145.45s of remaining time.\n",
      "\t-0.1583\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1.34s of the 1.34s of remaining time.\n",
      "\t-0.2713\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -0.2s of remaining time.\n",
      "\t-0.1555\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.04s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels263\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.04970419215794452\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.04970419215794452,\n",
      "    \"mean_squared_error\": -0.0024705067180738854,\n",
      "    \"mean_absolute_error\": -0.017004939480382356,\n",
      "    \"r2\": 0.9996095729188439,\n",
      "    \"pearsonr\": 0.9998048039686334,\n",
      "    \"median_absolute_error\": -0.004768796745355297\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels264\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.52 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 889\n",
      "Label Column: 889\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.5816128135, -14.8965394106, 0.67927, 3.09861)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60718.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.25 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 574 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 574 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t574 features in original data used to generate 574 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.501614\n",
      "[2000]\tvalid_set's rmse: 0.496944\n",
      "[3000]\tvalid_set's rmse: 0.496653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4966\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.27s of the 167.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.457618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4571\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 158.23s of the 158.23s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 7116.\n",
      "\t-0.4187\t = Validation score   (-root_mean_squared_error)\n",
      "\t158.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.88s of remaining time.\n",
      "\t-0.4166\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.73s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels264\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13229588895553449\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13229588895553449,\n",
      "    \"mean_squared_error\": -0.01750220223453504,\n",
      "    \"mean_absolute_error\": -0.03958577239718412,\n",
      "    \"r2\": 0.9981769379025077,\n",
      "    \"pearsonr\": 0.9990929552229358,\n",
      "    \"median_absolute_error\": -0.007915265133982857\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels265\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.45 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 890\n",
      "Label Column: 890\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (18.796583423, -15.7287653599, 1.33451, 3.40808)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60719.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.33 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 575 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 575 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t575 features in original data used to generate 575 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.02 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.606631\n",
      "[2000]\tvalid_set's rmse: 0.602253\n",
      "[3000]\tvalid_set's rmse: 0.601891\n",
      "[4000]\tvalid_set's rmse: 0.601772\n",
      "[5000]\tvalid_set's rmse: 0.601754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6017\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.88s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.6s of the 160.59s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.494324\n",
      "[2000]\tvalid_set's rmse: 0.493187\n",
      "[3000]\tvalid_set's rmse: 0.493076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4931\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 143.52s of the 143.51s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6494.\n",
      "\t-0.4539\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.7s of remaining time.\n",
      "\t-0.4528\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.51s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels265\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14466256858603332\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14466256858603332,\n",
      "    \"mean_squared_error\": -0.02092725874990882,\n",
      "    \"mean_absolute_error\": -0.04843578864313258,\n",
      "    \"r2\": 0.998198082618771,\n",
      "    \"pearsonr\": 0.999106128611027,\n",
      "    \"median_absolute_error\": -0.013611838797253362\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels266\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.38 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 891\n",
      "Label Column: 891\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (15.1635093654, -10.3950826053, -0.09297, 2.88269)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60712.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.42 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 576 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 576 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t576 features in original data used to generate 576 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.11 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.32s of the 179.32s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.498018\n",
      "[2000]\tvalid_set's rmse: 0.491025\n",
      "[3000]\tvalid_set's rmse: 0.490459\n",
      "[4000]\tvalid_set's rmse: 0.490421\n",
      "[5000]\tvalid_set's rmse: 0.49038\n",
      "[6000]\tvalid_set's rmse: 0.490372\n",
      "[7000]\tvalid_set's rmse: 0.49037\n",
      "[8000]\tvalid_set's rmse: 0.490369\n",
      "[9000]\tvalid_set's rmse: 0.490369\n",
      "[10000]\tvalid_set's rmse: 0.490369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4904\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.03s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 147.24s of the 147.24s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.457013\n",
      "[2000]\tvalid_set's rmse: 0.455643\n",
      "[3000]\tvalid_set's rmse: 0.45554\n",
      "[4000]\tvalid_set's rmse: 0.455525\n",
      "[5000]\tvalid_set's rmse: 0.455522\n",
      "[6000]\tvalid_set's rmse: 0.455521\n",
      "[7000]\tvalid_set's rmse: 0.455521\n",
      "[8000]\tvalid_set's rmse: 0.455521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4555\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.31s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 108.91s of the 108.91s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5031.\n",
      "\t-0.4119\t = Validation score   (-root_mean_squared_error)\n",
      "\t109.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.32s of the -0.6s of remaining time.\n",
      "\t-0.4119\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.39s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels266\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13200887169708506\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13200887169708506,\n",
      "    \"mean_squared_error\": -0.017426342206737517,\n",
      "    \"mean_absolute_error\": -0.04568850207435525,\n",
      "    \"r2\": 0.9979027483496089,\n",
      "    \"pearsonr\": 0.9989528619005648,\n",
      "    \"median_absolute_error\": -0.014189784140924089\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels267\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.28 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 892\n",
      "Label Column: 892\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (35.8582443966, -18.6767383161, 2.40192, 3.61298)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60685.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.5 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 577 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 577 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t577 features in original data used to generate 577 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.19 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.656273\n",
      "[2000]\tvalid_set's rmse: 0.651486\n",
      "[3000]\tvalid_set's rmse: 0.651002\n",
      "[4000]\tvalid_set's rmse: 0.651012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6509\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.67s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.24s of the 164.23s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.559356\n",
      "[2000]\tvalid_set's rmse: 0.55806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5579\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 150.28s of the 150.28s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6744.\n",
      "\t-0.5348\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -0.65s of remaining time.\n",
      "\t-0.5241\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels267\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1662584091967664\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1662584091967664,\n",
      "    \"mean_squared_error\": -0.027641858628639486,\n",
      "    \"mean_absolute_error\": -0.04541406567069837,\n",
      "    \"r2\": 0.9978822379295468,\n",
      "    \"pearsonr\": 0.9989435215546669,\n",
      "    \"median_absolute_error\": -0.007254958606942724\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels268\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.21 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 893\n",
      "Label Column: 893\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.3573470202, -11.4323613355, -0.414, 2.88289)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60788.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.58 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 578 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 578 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t578 features in original data used to generate 578 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.27 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.31s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.454932\n",
      "[2000]\tvalid_set's rmse: 0.451652\n",
      "[3000]\tvalid_set's rmse: 0.451118\n",
      "[4000]\tvalid_set's rmse: 0.450949\n",
      "[5000]\tvalid_set's rmse: 0.450925\n",
      "[6000]\tvalid_set's rmse: 0.450917\n",
      "[7000]\tvalid_set's rmse: 0.450914\n",
      "[8000]\tvalid_set's rmse: 0.450913\n",
      "[9000]\tvalid_set's rmse: 0.450913\n",
      "[10000]\tvalid_set's rmse: 0.450912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4509\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.49s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 146.6s of the 146.6s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.390421\n",
      "[2000]\tvalid_set's rmse: 0.389268\n",
      "[3000]\tvalid_set's rmse: 0.389198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3892\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 131.67s of the 131.67s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5962.\n",
      "\t-0.3605\t = Validation score   (-root_mean_squared_error)\n",
      "\t131.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.31s of the -0.66s of remaining time.\n",
      "\t-0.3588\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.44s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels268\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11400882836857151\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11400882836857151,\n",
      "    \"mean_squared_error\": -0.012998012945974413,\n",
      "    \"mean_absolute_error\": -0.034240059291161355,\n",
      "    \"r2\": 0.9984359112086397,\n",
      "    \"pearsonr\": 0.9992194966018344,\n",
      "    \"median_absolute_error\": -0.007218805901297087\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels269\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.11 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 894\n",
      "Label Column: 894\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (28.9583431407, -19.8110801212, 3.0762, 3.88252)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60753.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.67 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 579 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 579 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t579 features in original data used to generate 579 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.36 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.978224\n",
      "[2000]\tvalid_set's rmse: 0.968365\n",
      "[3000]\tvalid_set's rmse: 0.967226\n",
      "[4000]\tvalid_set's rmse: 0.966927\n",
      "[5000]\tvalid_set's rmse: 0.966935\n",
      "[6000]\tvalid_set's rmse: 0.966926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9669\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 157.95s of the 157.95s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.883309\n",
      "[2000]\tvalid_set's rmse: 0.881474\n",
      "[3000]\tvalid_set's rmse: 0.881272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8812\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 140.03s of the 140.03s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6311.\n",
      "\t-0.8269\t = Validation score   (-root_mean_squared_error)\n",
      "\t140.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.85s of remaining time.\n",
      "\t-0.8261\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.7s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels269\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.26199679194206665\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.26199679194206665,\n",
      "    \"mean_squared_error\": -0.06864231898793458,\n",
      "    \"mean_absolute_error\": -0.07515504016296155,\n",
      "    \"r2\": 0.9954458614993319,\n",
      "    \"pearsonr\": 0.9977332423640597,\n",
      "    \"median_absolute_error\": -0.0122671482046387\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels270\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   47.04 GB / 2000.36 GB (2.4%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 895\n",
      "Label Column: 895\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.6380142143, -12.6163880916, -0.85439, 2.85416)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60765.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.75 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 580 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 580 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t580 features in original data used to generate 580 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.44 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.73446\n",
      "[2000]\tvalid_set's rmse: 0.729781\n",
      "[3000]\tvalid_set's rmse: 0.729754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7294\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.21s of the 168.21s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.652899\n",
      "[2000]\tvalid_set's rmse: 0.650912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6508\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.52s of the 155.51s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6965.\n",
      "\t-0.6308\t = Validation score   (-root_mean_squared_error)\n",
      "\t155.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.88s of remaining time.\n",
      "\t-0.6271\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.66s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels270\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19889569077922395\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19889569077922395,\n",
      "    \"mean_squared_error\": -0.03955949581054469,\n",
      "    \"mean_absolute_error\": -0.05880258450367743,\n",
      "    \"r2\": 0.9951433753913479,\n",
      "    \"pearsonr\": 0.9975720068584571,\n",
      "    \"median_absolute_error\": -0.009878236318008438\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels271\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.97 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 896\n",
      "Label Column: 896\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.814726117, -17.2332116189, 3.51969, 3.95009)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60742.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.83 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 581 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 581 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t581 features in original data used to generate 581 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.53 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.492629\n",
      "[2000]\tvalid_set's rmse: 0.489867\n",
      "[3000]\tvalid_set's rmse: 0.48932\n",
      "[4000]\tvalid_set's rmse: 0.489177\n",
      "[5000]\tvalid_set's rmse: 0.489195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4892\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.49s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.31s of the 161.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.407587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4072\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 153.76s of the 153.76s of remaining time.\n",
      "\t-0.3609\t = Validation score   (-root_mean_squared_error)\n",
      "\t131.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 21.67s of the 21.66s of remaining time.\n",
      "\t-0.4509\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.85s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.6s of remaining time.\n",
      "\t-0.3583\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.39s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels271\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11686750611068016\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11686750611068016,\n",
      "    \"mean_squared_error\": -0.013658013984529899,\n",
      "    \"mean_absolute_error\": -0.047172194354674714,\n",
      "    \"r2\": 0.9991245845161625,\n",
      "    \"pearsonr\": 0.9995627693545297,\n",
      "    \"median_absolute_error\": -0.0196295234129516\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels272\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.89 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 897\n",
      "Label Column: 897\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.8354558336, -15.3486364737, -1.41668, 2.90956)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60713.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 74.92 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 582 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 582 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t582 features in original data used to generate 582 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.61 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.324811\n",
      "[2000]\tvalid_set's rmse: 0.323142\n",
      "[3000]\tvalid_set's rmse: 0.322796\n",
      "[4000]\tvalid_set's rmse: 0.322726\n",
      "[5000]\tvalid_set's rmse: 0.32273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3227\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.83s of the 161.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.278069\n",
      "[2000]\tvalid_set's rmse: 0.277471\n",
      "[3000]\tvalid_set's rmse: 0.27742\n",
      "[4000]\tvalid_set's rmse: 0.277411\n",
      "[5000]\tvalid_set's rmse: 0.277409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2774\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.69s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 134.3s of the 134.3s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6062.\n",
      "\t-0.244\t = Validation score   (-root_mean_squared_error)\n",
      "\t134.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.63s of remaining time.\n",
      "\t-0.2425\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels272\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.07694401138443535\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.07694401138443535,\n",
      "    \"mean_squared_error\": -0.005920380887928117,\n",
      "    \"mean_absolute_error\": -0.022533299978486108,\n",
      "    \"r2\": 0.9993005846949274,\n",
      "    \"pearsonr\": 0.9996503678834272,\n",
      "    \"median_absolute_error\": -0.003936246145849531\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels273\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.81 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 898\n",
      "Label Column: 898\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.8824043201, -17.4646576429, 3.555, 4.26121)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60779.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.0 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 583 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 583 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t583 features in original data used to generate 583 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.69 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.995473\n",
      "[2000]\tvalid_set's rmse: 0.984297\n",
      "[3000]\tvalid_set's rmse: 0.982555\n",
      "[4000]\tvalid_set's rmse: 0.982437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9824\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.23s of the 164.22s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.883284\n",
      "[2000]\tvalid_set's rmse: 0.879849\n",
      "[3000]\tvalid_set's rmse: 0.879591\n",
      "[4000]\tvalid_set's rmse: 0.87954\n",
      "[5000]\tvalid_set's rmse: 0.879538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8795\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.22s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 136.16s of the 136.16s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6135.\n",
      "\t-0.7959\t = Validation score   (-root_mean_squared_error)\n",
      "\t136.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.85s of remaining time.\n",
      "\t-0.7959\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.62s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels273\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2526925815543813\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2526925815543813,\n",
      "    \"mean_squared_error\": -0.06385354077261773,\n",
      "    \"mean_absolute_error\": -0.07465763371315355,\n",
      "    \"r2\": 0.9964830863406732,\n",
      "    \"pearsonr\": 0.9982490512045273,\n",
      "    \"median_absolute_error\": -0.014194203217651413\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels274\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.73 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 899\n",
      "Label Column: 899\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.7400703414, -17.6180362873, -2.338, 3.1709)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60790.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.08 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 584 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 584 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t584 features in original data used to generate 584 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.78 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.687558\n",
      "[2000]\tvalid_set's rmse: 0.683223\n",
      "[3000]\tvalid_set's rmse: 0.682431\n",
      "[4000]\tvalid_set's rmse: 0.682396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6824\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.86s of the 164.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.611994\n",
      "[2000]\tvalid_set's rmse: 0.611029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6109\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 149.99s of the 149.99s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6667.\n",
      "\t-0.5604\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.66s of remaining time.\n",
      "\t-0.5604\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels274\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.17764926742540085\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.17764926742540085,\n",
      "    \"mean_squared_error\": -0.03155926221678165,\n",
      "    \"mean_absolute_error\": -0.05128517150026268,\n",
      "    \"r2\": 0.9968609084558251,\n",
      "    \"pearsonr\": 0.9984314631977462,\n",
      "    \"median_absolute_error\": -0.007812659266808797\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels275\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.66 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 900\n",
      "Label Column: 900\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (29.0677471371, -17.8455293033, 3.81117, 4.28179)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60780.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.17 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 585 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 585 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t585 features in original data used to generate 585 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.86 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.770223\n",
      "[2000]\tvalid_set's rmse: 0.766716\n",
      "[3000]\tvalid_set's rmse: 0.766289\n",
      "[4000]\tvalid_set's rmse: 0.766287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7662\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.68s of the 164.68s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.586971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5864\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.5s of the 155.49s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6886.\n",
      "\t-0.5808\t = Validation score   (-root_mean_squared_error)\n",
      "\t155.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.85s of remaining time.\n",
      "\t-0.5684\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.67s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels275\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1807688686175956\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1807688686175956,\n",
      "    \"mean_squared_error\": -0.03267738386128558,\n",
      "    \"mean_absolute_error\": -0.04583578948761232,\n",
      "    \"r2\": 0.9982174666149016,\n",
      "    \"pearsonr\": 0.9991123446472596,\n",
      "    \"median_absolute_error\": -0.013139823460083244\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels276\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.59 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 901\n",
      "Label Column: 901\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.2446531577, -17.5984451649, -2.74296, 3.14747)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60791.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.25 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 586 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 586 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t586 features in original data used to generate 586 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 48.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.449211\n",
      "[2000]\tvalid_set's rmse: 0.445521\n",
      "[3000]\tvalid_set's rmse: 0.444913\n",
      "[4000]\tvalid_set's rmse: 0.444881\n",
      "[5000]\tvalid_set's rmse: 0.444883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4449\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.58s of the 160.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.390615\n",
      "[2000]\tvalid_set's rmse: 0.389675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3896\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 149.85s of the 149.84s of remaining time.\n",
      "\t-0.346\t = Validation score   (-root_mean_squared_error)\n",
      "\t123.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 26.37s of the 26.36s of remaining time.\n",
      "\t-0.4154\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.56s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.33s of remaining time.\n",
      "\t-0.3457\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels276\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11289055933940002\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11289055933940002,\n",
      "    \"mean_squared_error\": -0.012744278387962677,\n",
      "    \"mean_absolute_error\": -0.04667918811883966,\n",
      "    \"r2\": 0.9987134282167888,\n",
      "    \"pearsonr\": 0.9993576495722432,\n",
      "    \"median_absolute_error\": -0.01924396825960084\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels277\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.52 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 902\n",
      "Label Column: 902\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (27.2233216547, -18.8846824815, 2.83417, 4.47697)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60662.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.34 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 587 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 587 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t587 features in original data used to generate 587 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.03 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.17786\n",
      "[2000]\tvalid_set's rmse: 1.17149\n",
      "[3000]\tvalid_set's rmse: 1.17065\n",
      "[4000]\tvalid_set's rmse: 1.17063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.1706\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.05s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 163.79s of the 163.78s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.01716\n",
      "[2000]\tvalid_set's rmse: 1.01549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0154\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 152.93s of the 152.92s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6758.\n",
      "\t-0.9943\t = Validation score   (-root_mean_squared_error)\n",
      "\t153.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.89s of remaining time.\n",
      "\t-0.982\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.69s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels277\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.3115275332687643\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.3115275332687643,\n",
      "    \"mean_squared_error\": -0.09704940398452136,\n",
      "    \"mean_absolute_error\": -0.08138139933439122,\n",
      "    \"r2\": 0.9951575462811951,\n",
      "    \"pearsonr\": 0.9975877440673611,\n",
      "    \"median_absolute_error\": -0.016641206063378933\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels278\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.45 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 903\n",
      "Label Column: 903\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.007203264, -15.4505463012, -2.37004, 3.05692)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60810.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.42 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 588 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 588 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t588 features in original data used to generate 588 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.11 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.398229\n",
      "[2000]\tvalid_set's rmse: 0.394377\n",
      "[3000]\tvalid_set's rmse: 0.393946\n",
      "[4000]\tvalid_set's rmse: 0.393901\n",
      "[5000]\tvalid_set's rmse: 0.393882\n",
      "[6000]\tvalid_set's rmse: 0.39387\n",
      "[7000]\tvalid_set's rmse: 0.393867\n",
      "[8000]\tvalid_set's rmse: 0.393866\n",
      "[9000]\tvalid_set's rmse: 0.393866\n",
      "[10000]\tvalid_set's rmse: 0.393866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3939\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 146.0s of the 145.99s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.355639\n",
      "[2000]\tvalid_set's rmse: 0.354075\n",
      "[3000]\tvalid_set's rmse: 0.35399\n",
      "[4000]\tvalid_set's rmse: 0.353991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.354\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 125.38s of the 125.38s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5611.\n",
      "\t-0.3032\t = Validation score   (-root_mean_squared_error)\n",
      "\t125.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.62s of remaining time.\n",
      "\t-0.3031\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.51s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels278\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.09712473857127016\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.09712473857127016,\n",
      "    \"mean_squared_error\": -0.009433214842537528,\n",
      "    \"mean_absolute_error\": -0.03338986803589436,\n",
      "    \"r2\": 0.99899043518545,\n",
      "    \"pearsonr\": 0.9994954193759744,\n",
      "    \"median_absolute_error\": -0.010243679478857626\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels279\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.36 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 904\n",
      "Label Column: 904\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (20.6379097857, -11.9425819891, 1.6763, 3.45088)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60777.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.5 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 589 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 589 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t589 features in original data used to generate 589 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.19 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.600537\n",
      "[2000]\tvalid_set's rmse: 0.597474\n",
      "[3000]\tvalid_set's rmse: 0.597031\n",
      "[4000]\tvalid_set's rmse: 0.596808\n",
      "[5000]\tvalid_set's rmse: 0.596762\n",
      "[6000]\tvalid_set's rmse: 0.596754\n",
      "[7000]\tvalid_set's rmse: 0.596752\n",
      "[8000]\tvalid_set's rmse: 0.596754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5967\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 151.98s of the 151.97s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.576963\n",
      "[2000]\tvalid_set's rmse: 0.575476\n",
      "[3000]\tvalid_set's rmse: 0.575379\n",
      "[4000]\tvalid_set's rmse: 0.575373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5754\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 131.31s of the 131.3s of remaining time.\n",
      "\t-0.5204\t = Validation score   (-root_mean_squared_error)\n",
      "\t107.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 23.54s of the 23.54s of remaining time.\n",
      "\t-0.6009\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.74s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.33s of remaining time.\n",
      "\t-0.5196\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels279\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1722463601580863\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1722463601580863,\n",
      "    \"mean_squared_error\": -0.029668808587709088,\n",
      "    \"mean_absolute_error\": -0.0750895603770082,\n",
      "    \"r2\": 0.9975083748771673,\n",
      "    \"pearsonr\": 0.9987560322846053,\n",
      "    \"median_absolute_error\": -0.03509888216021745\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels280\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.27 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 905\n",
      "Label Column: 905\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (12.2492354102, -13.0864168107, -1.83199, 2.90426)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60700.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.59 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 590 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 590 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t590 features in original data used to generate 590 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.28 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.404894\n",
      "[2000]\tvalid_set's rmse: 0.40139\n",
      "[3000]\tvalid_set's rmse: 0.401019\n",
      "[4000]\tvalid_set's rmse: 0.400965\n",
      "[5000]\tvalid_set's rmse: 0.400955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4009\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.43s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 158.99s of the 158.99s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.366569\n",
      "[2000]\tvalid_set's rmse: 0.365468\n",
      "[3000]\tvalid_set's rmse: 0.365394\n",
      "[4000]\tvalid_set's rmse: 0.365378\n",
      "[5000]\tvalid_set's rmse: 0.365378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.3654\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 129.52s of the 129.52s of remaining time.\n",
      "\t-0.326\t = Validation score   (-root_mean_squared_error)\n",
      "\t92.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 36.25s of the 36.25s of remaining time.\n",
      "\t-0.3809\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 6.91s of the 6.91s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 202. Best iteration is:\n",
      "\t[202]\tvalid_set's rmse: 0.397561\n",
      "\t-0.3976\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.31s of remaining time.\n",
      "\t-0.3249\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels280\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.11039922412625673\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.11039922412625673,\n",
      "    \"mean_squared_error\": -0.012187988687679455,\n",
      "    \"mean_absolute_error\": -0.05369624018939597,\n",
      "    \"r2\": 0.9985548880064343,\n",
      "    \"pearsonr\": 0.9992798362915485,\n",
      "    \"median_absolute_error\": -0.028684286950561466\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels281\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.18 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 906\n",
      "Label Column: 906\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (17.4900351574, -9.9731210387, 1.14939, 2.71753)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60802.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.67 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 591 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 591 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t591 features in original data used to generate 591 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.36 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.499004\n",
      "[2000]\tvalid_set's rmse: 0.494271\n",
      "[3000]\tvalid_set's rmse: 0.493567\n",
      "[4000]\tvalid_set's rmse: 0.493368\n",
      "[5000]\tvalid_set's rmse: 0.493346\n",
      "[6000]\tvalid_set's rmse: 0.493347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4933\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 155.88s of the 155.88s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.483256\n",
      "[2000]\tvalid_set's rmse: 0.481534\n",
      "[3000]\tvalid_set's rmse: 0.481345\n",
      "[4000]\tvalid_set's rmse: 0.481299\n",
      "[5000]\tvalid_set's rmse: 0.481292\n",
      "[6000]\tvalid_set's rmse: 0.48129\n",
      "[7000]\tvalid_set's rmse: 0.481289\n",
      "[8000]\tvalid_set's rmse: 0.481289\n",
      "[9000]\tvalid_set's rmse: 0.481289\n",
      "[10000]\tvalid_set's rmse: 0.481289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4813\t = Validation score   (-root_mean_squared_error)\n",
      "\t50.21s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 104.72s of the 104.71s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 4709.\n",
      "\t-0.4136\t = Validation score   (-root_mean_squared_error)\n",
      "\t104.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.35s of remaining time.\n",
      "\t-0.4136\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.26s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels281\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13358712176724313\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13358712176724313,\n",
      "    \"mean_squared_error\": -0.01784551910205627,\n",
      "    \"mean_absolute_error\": -0.05086702388505673,\n",
      "    \"r2\": 0.9975833055748694,\n",
      "    \"pearsonr\": 0.9987929303741115,\n",
      "    \"median_absolute_error\": -0.017739661368068226\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels282\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.09 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 907\n",
      "Label Column: 907\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.54932472, -13.5375078073, -1.58346, 2.83301)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60807.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.75 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 592 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 592 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t592 features in original data used to generate 592 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.44 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.465039\n",
      "[2000]\tvalid_set's rmse: 0.462215\n",
      "[3000]\tvalid_set's rmse: 0.461722\n",
      "[4000]\tvalid_set's rmse: 0.461742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4617\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.48s of the 162.48s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.415866\n",
      "[2000]\tvalid_set's rmse: 0.414821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4148\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 151.62s of the 151.62s of remaining time.\n",
      "\t-0.3782\t = Validation score   (-root_mean_squared_error)\n",
      "\t106.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 44.74s of the 44.74s of remaining time.\n",
      "\t-0.4457\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.7s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 15.58s of the 15.58s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 513. Best iteration is:\n",
      "\t[509]\tvalid_set's rmse: 0.456747\n",
      "\t-0.4567\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -0.7s of remaining time.\n",
      "\t-0.3765\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.5s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels282\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1247891777779876\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1247891777779876,\n",
      "    \"mean_squared_error\": -0.015572338890506245,\n",
      "    \"mean_absolute_error\": -0.054336633463853555,\n",
      "    \"r2\": 0.9980595581173264,\n",
      "    \"pearsonr\": 0.9990354853143453,\n",
      "    \"median_absolute_error\": -0.025636424509152267\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels283\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   46.01 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 908\n",
      "Label Column: 908\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (14.9952027714, -19.055177542, 1.74847, 3.29547)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60815.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.84 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 593 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 593 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t593 features in original data used to generate 593 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.53 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.563919\n",
      "[2000]\tvalid_set's rmse: 0.558121\n",
      "[3000]\tvalid_set's rmse: 0.557192\n",
      "[4000]\tvalid_set's rmse: 0.556981\n",
      "[5000]\tvalid_set's rmse: 0.556909\n",
      "[6000]\tvalid_set's rmse: 0.556898\n",
      "[7000]\tvalid_set's rmse: 0.556898\n",
      "[8000]\tvalid_set's rmse: 0.556896\n",
      "[9000]\tvalid_set's rmse: 0.556896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5569\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.89s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 146.15s of the 146.15s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.52353\n",
      "[2000]\tvalid_set's rmse: 0.521075\n",
      "[3000]\tvalid_set's rmse: 0.520923\n",
      "[4000]\tvalid_set's rmse: 0.520927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5209\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.96s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 124.78s of the 124.77s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5525.\n",
      "\t-0.4529\t = Validation score   (-root_mean_squared_error)\n",
      "\t124.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.66s of remaining time.\n",
      "\t-0.4528\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels283\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.14450155789719063\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.14450155789719063,\n",
      "    \"mean_squared_error\": -0.020880700234715253,\n",
      "    \"mean_absolute_error\": -0.047756295999928686,\n",
      "    \"r2\": 0.9980771233891832,\n",
      "    \"pearsonr\": 0.9990398986096984,\n",
      "    \"median_absolute_error\": -0.012291538958727921\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels284\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.92 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 909\n",
      "Label Column: 909\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (13.3625053756, -16.8051019351, -1.49755, 3.05756)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60813.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 75.92 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 594 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 594 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t594 features in original data used to generate 594 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.61 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.540058\n",
      "[2000]\tvalid_set's rmse: 0.535914\n",
      "[3000]\tvalid_set's rmse: 0.535225\n",
      "[4000]\tvalid_set's rmse: 0.535037\n",
      "[5000]\tvalid_set's rmse: 0.535018\n",
      "[6000]\tvalid_set's rmse: 0.535006\n",
      "[7000]\tvalid_set's rmse: 0.535007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.535\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.75s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 152.51s of the 152.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.469534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4689\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 143.7s of the 143.7s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6304.\n",
      "\t-0.422\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.65s of remaining time.\n",
      "\t-0.4209\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels284\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13384756094742284\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13384756094742284,\n",
      "    \"mean_squared_error\": -0.01791516957157408,\n",
      "    \"mean_absolute_error\": -0.039433264292328324,\n",
      "    \"r2\": 0.9980834790270618,\n",
      "    \"pearsonr\": 0.9990436884114942,\n",
      "    \"median_absolute_error\": -0.009510450692077677\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels285\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.84 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 910\n",
      "Label Column: 910\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.7134973877, -21.8170608029, 2.95938, 4.21973)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60812.4 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.0 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 595 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 595 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t595 features in original data used to generate 595 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.69 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.3s of the 179.3s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.677032\n",
      "[2000]\tvalid_set's rmse: 0.671532\n",
      "[3000]\tvalid_set's rmse: 0.671238\n",
      "[4000]\tvalid_set's rmse: 0.671096\n",
      "[5000]\tvalid_set's rmse: 0.671111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6711\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 159.65s of the 159.65s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.583034\n",
      "[2000]\tvalid_set's rmse: 0.581537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5815\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 148.04s of the 148.04s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6410.\n",
      "\t-0.5225\t = Validation score   (-root_mean_squared_error)\n",
      "\t148.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.3s of the -0.63s of remaining time.\n",
      "\t-0.5222\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.45s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels285\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1659659188664809\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1659659188664809,\n",
      "    \"mean_squared_error\": -0.027544686225195108,\n",
      "    \"mean_absolute_error\": -0.05131410242282756,\n",
      "    \"r2\": 0.9984529316208635,\n",
      "    \"pearsonr\": 0.9992276870020738,\n",
      "    \"median_absolute_error\": -0.010359872052240426\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels286\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.77 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 911\n",
      "Label Column: 911\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (21.2582095752, -17.3357217045, -1.18681, 3.54653)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60755.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.09 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 596 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 596 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t596 features in original data used to generate 596 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.78 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.720169\n",
      "[2000]\tvalid_set's rmse: 0.714562\n",
      "[3000]\tvalid_set's rmse: 0.713743\n",
      "[4000]\tvalid_set's rmse: 0.713758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7137\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 164.22s of the 164.21s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.600345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5999\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.42s of the 155.41s of remaining time.\n",
      "\t-0.5848\t = Validation score   (-root_mean_squared_error)\n",
      "\t94.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 60.3s of the 60.29s of remaining time.\n",
      "\t-0.654\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.96s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 18.91s of the 18.9s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 636. Best iteration is:\n",
      "\t[635]\tvalid_set's rmse: 0.651441\n",
      "\t-0.6514\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.76s of remaining time.\n",
      "\t-0.5737\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.59s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels286\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19144175997545665\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19144175997545665,\n",
      "    \"mean_squared_error\": -0.03664994746250033,\n",
      "    \"mean_absolute_error\": -0.08424196542152514,\n",
      "    \"r2\": 0.9970858835023386,\n",
      "    \"pearsonr\": 0.9985463114203793,\n",
      "    \"median_absolute_error\": -0.04342592477931517\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels287\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.69 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 912\n",
      "Label Column: 912\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (25.2154085523, -14.1486237676, 3.06647, 4.02197)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60689.47 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.17 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 597 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 597 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t597 features in original data used to generate 597 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.86 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.74s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.26s of the 179.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.500396\n",
      "[2000]\tvalid_set's rmse: 0.496907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4967\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 168.09s of the 168.09s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.47567\n",
      "[2000]\tvalid_set's rmse: 0.474418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4744\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 154.47s of the 154.47s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6693.\n",
      "\t-0.4204\t = Validation score   (-root_mean_squared_error)\n",
      "\t154.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.26s of the -0.87s of remaining time.\n",
      "\t-0.4179\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.73s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels287\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13250919149889012\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13250919149889012,\n",
      "    \"mean_squared_error\": -0.01755868583168953,\n",
      "    \"mean_absolute_error\": -0.037742105783522925,\n",
      "    \"r2\": 0.9989144357189721,\n",
      "    \"pearsonr\": 0.9994573767646242,\n",
      "    \"median_absolute_error\": -0.006020296693542537\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels288\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.62 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 913\n",
      "Label Column: 913\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (29.4295904926, -18.3510396815, -0.35251, 3.77673)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60757.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.25 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 598 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 598 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t598 features in original data used to generate 598 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.95 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.590887\n",
      "[2000]\tvalid_set's rmse: 0.58678\n",
      "[3000]\tvalid_set's rmse: 0.586619\n",
      "[4000]\tvalid_set's rmse: 0.586519\n",
      "[5000]\tvalid_set's rmse: 0.586485\n",
      "[6000]\tvalid_set's rmse: 0.586477\n",
      "[7000]\tvalid_set's rmse: 0.586476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5865\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 153.55s of the 153.55s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.555781\n",
      "[2000]\tvalid_set's rmse: 0.554183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5541\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 138.73s of the 138.73s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6064.\n",
      "\t-0.5223\t = Validation score   (-root_mean_squared_error)\n",
      "\t138.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.73s of remaining time.\n",
      "\t-0.5143\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.63s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels288\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16328334762783936\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16328334762783936,\n",
      "    \"mean_squared_error\": -0.02666145161255383,\n",
      "    \"mean_absolute_error\": -0.046755468256102564,\n",
      "    \"r2\": 0.9981306418845864,\n",
      "    \"pearsonr\": 0.9990655353805882,\n",
      "    \"median_absolute_error\": -0.009048905975976629\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels289\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.54 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 914\n",
      "Label Column: 914\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (32.0712337082, -14.9247527525, 2.63123, 4.2738)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60764.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.34 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 599 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 599 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t599 features in original data used to generate 599 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.03 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.746181\n",
      "[2000]\tvalid_set's rmse: 0.736661\n",
      "[3000]\tvalid_set's rmse: 0.735463\n",
      "[4000]\tvalid_set's rmse: 0.735245\n",
      "[5000]\tvalid_set's rmse: 0.735198\n",
      "[6000]\tvalid_set's rmse: 0.735207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7352\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.05s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 155.24s of the 155.24s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.649233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6488\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 147.08s of the 147.07s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6370.\n",
      "\t-0.595\t = Validation score   (-root_mean_squared_error)\n",
      "\t147.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.64s of remaining time.\n",
      "\t-0.5943\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.55s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels289\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18887310508142743\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18887310508142743,\n",
      "    \"mean_squared_error\": -0.035673049823099864,\n",
      "    \"mean_absolute_error\": -0.055889664011204224,\n",
      "    \"r2\": 0.9980467678657158,\n",
      "    \"pearsonr\": 0.9990251362700797,\n",
      "    \"median_absolute_error\": -0.012591819859808728\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels290\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.46 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 915\n",
      "Label Column: 915\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (29.6733267121, -21.5606237485, 0.16021, 4.17119)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60782.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.42 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 600 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 600 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t600 features in original data used to generate 600 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.11 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.74s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.26s of the 179.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.750008\n",
      "[2000]\tvalid_set's rmse: 0.744634\n",
      "[3000]\tvalid_set's rmse: 0.744019\n",
      "[4000]\tvalid_set's rmse: 0.743948\n",
      "[5000]\tvalid_set's rmse: 0.743887\n",
      "[6000]\tvalid_set's rmse: 0.743889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7439\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.05s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 155.19s of the 155.19s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.63269\n",
      "[2000]\tvalid_set's rmse: 0.631566\n",
      "[3000]\tvalid_set's rmse: 0.631308\n",
      "[4000]\tvalid_set's rmse: 0.631226\n",
      "[5000]\tvalid_set's rmse: 0.631201\n",
      "[6000]\tvalid_set's rmse: 0.631193\n",
      "[7000]\tvalid_set's rmse: 0.631189\n",
      "[8000]\tvalid_set's rmse: 0.631188\n",
      "[9000]\tvalid_set's rmse: 0.631188\n",
      "[10000]\tvalid_set's rmse: 0.631188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6312\t = Validation score   (-root_mean_squared_error)\n",
      "\t50.47s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 103.4s of the 103.4s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 4595.\n",
      "\t-0.6095\t = Validation score   (-root_mean_squared_error)\n",
      "\t103.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.26s of the -0.67s of remaining time.\n",
      "\t-0.6016\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.59s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels290\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.19185983171449286\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.19185983171449286,\n",
      "    \"mean_squared_error\": -0.03681019502551351,\n",
      "    \"mean_absolute_error\": -0.05939802643058946,\n",
      "    \"r2\": 0.9978841235016946,\n",
      "    \"pearsonr\": 0.9989431393560239,\n",
      "    \"median_absolute_error\": -0.01655561410653672\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels291\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.36 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 916\n",
      "Label Column: 916\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (21.1997393567, -16.0146102336, 2.46701, 3.96574)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60784.3 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.5 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 601 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 601 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t601 features in original data used to generate 601 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.2 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.25s of the 179.25s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.606282\n",
      "[2000]\tvalid_set's rmse: 0.600615\n",
      "[3000]\tvalid_set's rmse: 0.599749\n",
      "[4000]\tvalid_set's rmse: 0.599672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5996\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.13s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.65s of the 161.65s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.551363\n",
      "[2000]\tvalid_set's rmse: 0.550087\n",
      "[3000]\tvalid_set's rmse: 0.549847\n",
      "[4000]\tvalid_set's rmse: 0.549833\n",
      "[5000]\tvalid_set's rmse: 0.549825\n",
      "[6000]\tvalid_set's rmse: 0.549825\n",
      "[7000]\tvalid_set's rmse: 0.549825\n",
      "[8000]\tvalid_set's rmse: 0.549825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.5498\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.58s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 120.02s of the 120.02s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5279.\n",
      "\t-0.4825\t = Validation score   (-root_mean_squared_error)\n",
      "\t120.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.25s of the -0.84s of remaining time.\n",
      "\t-0.4804\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.6s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels291\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.1549057388657336\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.1549057388657336,\n",
      "    \"mean_squared_error\": -0.023995787933538803,\n",
      "    \"mean_absolute_error\": -0.05825072995575094,\n",
      "    \"r2\": 0.9984740941975844,\n",
      "    \"pearsonr\": 0.9992389920624796,\n",
      "    \"median_absolute_error\": -0.019877796405822434\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels292\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.28 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 917\n",
      "Label Column: 917\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (23.2485376495, -32.3822640621, 0.61916, 4.4695)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60791.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.59 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 602 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 602 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t602 features in original data used to generate 602 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.28 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.969204\n",
      "[2000]\tvalid_set's rmse: 0.962028\n",
      "[3000]\tvalid_set's rmse: 0.961416\n",
      "[4000]\tvalid_set's rmse: 0.961266\n",
      "[5000]\tvalid_set's rmse: 0.961263\n",
      "[6000]\tvalid_set's rmse: 0.961259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9613\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.19s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 156.44s of the 156.43s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.808238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8069\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 145.82s of the 145.82s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6311.\n",
      "\t-0.7684\t = Validation score   (-root_mean_squared_error)\n",
      "\t145.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.71s of remaining time.\n",
      "\t-0.763\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.56s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels292\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24253444592172688\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24253444592172688,\n",
      "    \"mean_squared_error\": -0.05882295745855895,\n",
      "    \"mean_absolute_error\": -0.06561761365216287,\n",
      "    \"r2\": 0.9970551013125659,\n",
      "    \"pearsonr\": 0.9985282428038805,\n",
      "    \"median_absolute_error\": -0.016015999604125586\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels293\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.20 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 918\n",
      "Label Column: 918\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (27.532893031, -18.0925228594, 2.55884, 4.24084)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60768.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.67 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 603 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 603 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t603 features in original data used to generate 603 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.36 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.74s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.26s of the 179.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.706799\n",
      "[2000]\tvalid_set's rmse: 0.699767\n",
      "[3000]\tvalid_set's rmse: 0.698752\n",
      "[4000]\tvalid_set's rmse: 0.698631\n",
      "[5000]\tvalid_set's rmse: 0.698569\n",
      "[6000]\tvalid_set's rmse: 0.698551\n",
      "[7000]\tvalid_set's rmse: 0.698555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6986\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 152.21s of the 152.2s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.593046\n",
      "[2000]\tvalid_set's rmse: 0.592027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.592\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 138.21s of the 138.21s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5991.\n",
      "\t-0.5314\t = Validation score   (-root_mean_squared_error)\n",
      "\t138.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.26s of the -0.66s of remaining time.\n",
      "\t-0.5304\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels293\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.16869896304755602\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.16869896304755602,\n",
      "    \"mean_squared_error\": -0.02845934013332048,\n",
      "    \"mean_absolute_error\": -0.05210193337205218,\n",
      "    \"r2\": 0.9984174289009266,\n",
      "    \"pearsonr\": 0.999209441635714,\n",
      "    \"median_absolute_error\": -0.011341842088949672\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels294\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.12 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 919\n",
      "Label Column: 919\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (27.3224186023, -28.1668719434, 1.36521, 4.78585)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60767.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.76 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 604 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 604 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t604 features in original data used to generate 604 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.45 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.25s of the 179.24s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.981523\n",
      "[2000]\tvalid_set's rmse: 0.973531\n",
      "[3000]\tvalid_set's rmse: 0.972823\n",
      "[4000]\tvalid_set's rmse: 0.972616\n",
      "[5000]\tvalid_set's rmse: 0.97261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9726\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.52s of the 160.52s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.864926\n",
      "[2000]\tvalid_set's rmse: 0.862258\n",
      "[3000]\tvalid_set's rmse: 0.862072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8621\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 142.94s of the 142.94s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6133.\n",
      "\t-0.7747\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.25s of the -0.87s of remaining time.\n",
      "\t-0.7747\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels294\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2457381284540896\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2457381284540896,\n",
      "    \"mean_squared_error\": -0.060387227776118726,\n",
      "    \"mean_absolute_error\": -0.06335865436700593,\n",
      "    \"r2\": 0.9973632526116201,\n",
      "    \"pearsonr\": 0.9986824617783041,\n",
      "    \"median_absolute_error\": -0.012103969749775706\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels295\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   45.04 GB / 2000.36 GB (2.3%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 920\n",
      "Label Column: 920\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (22.0389582533, -15.9023003596, 2.72582, 4.2921)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60750.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.84 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 605 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 605 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t605 features in original data used to generate 605 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.53 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.697818\n",
      "[2000]\tvalid_set's rmse: 0.693719\n",
      "[3000]\tvalid_set's rmse: 0.69277\n",
      "[4000]\tvalid_set's rmse: 0.692688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6926\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.08s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 161.75s of the 161.74s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.619663\n",
      "[2000]\tvalid_set's rmse: 0.618317\n",
      "[3000]\tvalid_set's rmse: 0.618239\n",
      "[4000]\tvalid_set's rmse: 0.618203\n",
      "[5000]\tvalid_set's rmse: 0.618205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6182\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.5s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 134.46s of the 134.46s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5835.\n",
      "\t-0.589\t = Validation score   (-root_mean_squared_error)\n",
      "\t134.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.64s of remaining time.\n",
      "\t-0.5827\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.57s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels295\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18485875741524452\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18485875741524452,\n",
      "    \"mean_squared_error\": -0.03417276019310822,\n",
      "    \"mean_absolute_error\": -0.053734228196536664,\n",
      "    \"r2\": 0.9981448355676303,\n",
      "    \"pearsonr\": 0.9990727522720616,\n",
      "    \"median_absolute_error\": -0.00925706137347018\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels296\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.96 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 921\n",
      "Label Column: 921\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (42.0308066664, -32.1580549535, 2.46407, 5.50042)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60737.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 76.92 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 606 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 606 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t606 features in original data used to generate 606 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.61 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.37012\n",
      "[2000]\tvalid_set's rmse: 1.3613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.3609\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.14s of the 169.14s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.16988\n",
      "[2000]\tvalid_set's rmse: 1.16636\n",
      "[3000]\tvalid_set's rmse: 1.16599\n",
      "[4000]\tvalid_set's rmse: 1.16596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.166\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.63s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 142.73s of the 142.73s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6159.\n",
      "\t-1.0821\t = Validation score   (-root_mean_squared_error)\n",
      "\t142.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.87s of remaining time.\n",
      "\t-1.0799\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.69s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels296\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.34274205102690447\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.34274205102690447,\n",
      "    \"mean_squared_error\": -0.11747211354212946,\n",
      "    \"mean_absolute_error\": -0.09953440470895827,\n",
      "    \"r2\": 0.9961168450683863,\n",
      "    \"pearsonr\": 0.9980612884036638,\n",
      "    \"median_absolute_error\": -0.019052448733898775\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels297\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.89 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 922\n",
      "Label Column: 922\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (36.7125651607, -30.0853212152, 2.85944, 5.70184)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60748.6 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.01 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 607 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 607 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t607 features in original data used to generate 607 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.7 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.26s of the 179.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.661752\n",
      "[2000]\tvalid_set's rmse: 0.660048\n",
      "[3000]\tvalid_set's rmse: 0.659634\n",
      "[4000]\tvalid_set's rmse: 0.659487\n",
      "[5000]\tvalid_set's rmse: 0.659497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6595\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.62s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.11s of the 160.11s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.444145\n",
      "[2000]\tvalid_set's rmse: 0.443489\n",
      "[3000]\tvalid_set's rmse: 0.443385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.4434\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 139.82s of the 139.82s of remaining time.\n",
      "\t-0.42\t = Validation score   (-root_mean_squared_error)\n",
      "\t126.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 13.17s of the 13.17s of remaining time.\n",
      "\t-0.4527\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.36s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.26s of remaining time.\n",
      "\t-0.4033\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels297\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.13051117030122386\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.13051117030122386,\n",
      "    \"mean_squared_error\": -0.01703316557339502,\n",
      "    \"mean_absolute_error\": -0.0471101092379924,\n",
      "    \"r2\": 0.9994760293321238,\n",
      "    \"pearsonr\": 0.9997380005831269,\n",
      "    \"median_absolute_error\": -0.018829592311804144\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels298\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.81 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 923\n",
      "Label Column: 923\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (26.5319498483, -18.8080136913, 2.13383, 4.23011)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60663.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.09 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 608 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 608 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t608 features in original data used to generate 608 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.78 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.756744\n",
      "[2000]\tvalid_set's rmse: 0.751675\n",
      "[3000]\tvalid_set's rmse: 0.75088\n",
      "[4000]\tvalid_set's rmse: 0.750864\n",
      "[5000]\tvalid_set's rmse: 0.750789\n",
      "[6000]\tvalid_set's rmse: 0.7508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7508\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.71s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 155.63s of the 155.62s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.635916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6354\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 147.18s of the 147.18s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6310.\n",
      "\t-0.573\t = Validation score   (-root_mean_squared_error)\n",
      "\t147.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -1.12s of remaining time.\n",
      "\t-0.5708\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.98s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels298\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.18172546154188188\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.18172546154188188,\n",
      "    \"mean_squared_error\": -0.033024143372610004,\n",
      "    \"mean_absolute_error\": -0.05363020302632405,\n",
      "    \"r2\": 0.9981542645834264,\n",
      "    \"pearsonr\": 0.9990776451260321,\n",
      "    \"median_absolute_error\": -0.014412327860327423\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels299\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.73 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 924\n",
      "Label Column: 924\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (24.1847229999, -20.6017469483, 2.59244, 4.57302)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60670.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.17 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 609 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 609 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t609 features in original data used to generate 609 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.86 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.71s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.29s of the 179.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.763571\n",
      "[2000]\tvalid_set's rmse: 0.76043\n",
      "[3000]\tvalid_set's rmse: 0.75986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7598\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.86s of the 166.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.66221\n",
      "[2000]\tvalid_set's rmse: 0.661647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.6614\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 155.35s of the 155.35s of remaining time.\n",
      "\t-0.6131\t = Validation score   (-root_mean_squared_error)\n",
      "\t84.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 69.96s of the 69.96s of remaining time.\n",
      "\t-0.7148\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.37s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 42.48s of the 42.47s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.702964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1467. Best iteration is:\n",
      "\t[1467]\tvalid_set's rmse: 0.70293\n",
      "\t-0.7029\t = Validation score   (-root_mean_squared_error)\n",
      "\t42.78s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.29s of the -1.05s of remaining time.\n",
      "\t-0.6109\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.94s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels299\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2100806022784783\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2100806022784783,\n",
      "    \"mean_squared_error\": -0.04413385945368784,\n",
      "    \"mean_absolute_error\": -0.10172593202546579,\n",
      "    \"r2\": 0.9978893943127969,\n",
      "    \"pearsonr\": 0.9989467429344082,\n",
      "    \"median_absolute_error\": -0.059277502880566324\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels300\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.64 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 925\n",
      "Label Column: 925\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (36.1532211658, -20.5371316321, 3.19999, 5.17746)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60653.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.26 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 610 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 610 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t610 features in original data used to generate 610 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 50.95 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.2286\n",
      "[2000]\tvalid_set's rmse: 1.21618\n",
      "[3000]\tvalid_set's rmse: 1.21441\n",
      "[4000]\tvalid_set's rmse: 1.21412\n",
      "[5000]\tvalid_set's rmse: 1.21406\n",
      "[6000]\tvalid_set's rmse: 1.21404\n",
      "[7000]\tvalid_set's rmse: 1.21403\n",
      "[8000]\tvalid_set's rmse: 1.21403\n",
      "[9000]\tvalid_set's rmse: 1.21403\n",
      "[10000]\tvalid_set's rmse: 1.21403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.214\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.96s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 143.85s of the 143.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.06397\n",
      "[2000]\tvalid_set's rmse: 1.06082\n",
      "[3000]\tvalid_set's rmse: 1.06054\n",
      "[4000]\tvalid_set's rmse: 1.06051\n",
      "[5000]\tvalid_set's rmse: 1.0605\n",
      "[6000]\tvalid_set's rmse: 1.0605\n",
      "[7000]\tvalid_set's rmse: 1.0605\n",
      "[8000]\tvalid_set's rmse: 1.0605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0605\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.27s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 101.55s of the 101.54s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 4454.\n",
      "\t-0.99\t = Validation score   (-root_mean_squared_error)\n",
      "\t101.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.83s of remaining time.\n",
      "\t-0.9888\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.78s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels300\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.3186690955618658\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.3186690955618658,\n",
      "    \"mean_squared_error\": -0.10154999246621824,\n",
      "    \"mean_absolute_error\": -0.11571563720702893,\n",
      "    \"r2\": 0.9962113195862969,\n",
      "    \"pearsonr\": 0.9981089733534587,\n",
      "    \"median_absolute_error\": -0.041353506065014756\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels301\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.54 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 926\n",
      "Label Column: 926\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (36.5843224782, -17.0597551237, 2.86142, 4.45199)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60659.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.34 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 611 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 611 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t611 features in original data used to generate 611 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.03 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.01056\n",
      "[2000]\tvalid_set's rmse: 1.00684\n",
      "[3000]\tvalid_set's rmse: 1.00604\n",
      "[4000]\tvalid_set's rmse: 1.00599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0059\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 162.51s of the 162.51s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.991641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9914\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 154.85s of the 154.85s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6559.\n",
      "\t-0.9313\t = Validation score   (-root_mean_squared_error)\n",
      "\t155.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.64s of remaining time.\n",
      "\t-0.9306\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels301\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.29762888732479537\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.29762888732479537,\n",
      "    \"mean_squared_error\": -0.08858295457019572,\n",
      "    \"mean_absolute_error\": -0.09680643622306037,\n",
      "    \"r2\": 0.9955302491444145,\n",
      "    \"pearsonr\": 0.9977664927521017,\n",
      "    \"median_absolute_error\": -0.02845186643222669\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels302\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.47 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 927\n",
      "Label Column: 927\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (34.3658383261, -12.6976536274, 2.48792, 3.97667)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60773.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.42 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 612 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 612 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t612 features in original data used to generate 612 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.11 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.8919\n",
      "[2000]\tvalid_set's rmse: 0.8868\n",
      "[3000]\tvalid_set's rmse: 0.887022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8866\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 167.02s of the 167.02s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.812717\n",
      "[2000]\tvalid_set's rmse: 0.811482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8114\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 152.52s of the 152.52s of remaining time.\n",
      "\t-0.7842\t = Validation score   (-root_mean_squared_error)\n",
      "\t100.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 51.67s of the 51.67s of remaining time.\n",
      "\t-0.8809\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.53s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 26.76s of the 26.76s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 903. Best iteration is:\n",
      "\t[860]\tvalid_set's rmse: 0.847795\n",
      "\t-0.8478\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.54s of remaining time.\n",
      "\t-0.7754\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.39s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels302\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2568953486062418\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2568953486062418,\n",
      "    \"mean_squared_error\": -0.06599522013552243,\n",
      "    \"mean_absolute_error\": -0.1071841927407469,\n",
      "    \"r2\": 0.9958263609600682,\n",
      "    \"pearsonr\": 0.9979251884245597,\n",
      "    \"median_absolute_error\": -0.05164587880986972\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels303\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.39 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 928\n",
      "Label Column: 928\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (32.8890727855, -16.528077354, 2.33816, 3.88509)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60673.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 613 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 613 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t613 features in original data used to generate 613 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.2 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.932197\n",
      "[2000]\tvalid_set's rmse: 0.927198\n",
      "[3000]\tvalid_set's rmse: 0.926447\n",
      "[4000]\tvalid_set's rmse: 0.926321\n",
      "[5000]\tvalid_set's rmse: 0.926315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.9263\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.36s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 160.4s of the 160.4s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.772928\n",
      "[2000]\tvalid_set's rmse: 0.772161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.772\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 149.07s of the 149.06s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6301.\n",
      "\t-0.7037\t = Validation score   (-root_mean_squared_error)\n",
      "\t149.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.65s of remaining time.\n",
      "\t-0.7031\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.53s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels303\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.2231520048898026\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.2231520048898026,\n",
      "    \"mean_squared_error\": -0.049796817286338474,\n",
      "    \"mean_absolute_error\": -0.061244586203359604,\n",
      "    \"r2\": 0.9967005565595315,\n",
      "    \"pearsonr\": 0.9983576340466009,\n",
      "    \"median_absolute_error\": -0.011887967844506742\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels304\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.31 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 929\n",
      "Label Column: 929\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (33.0441616222, -12.9514734596, 2.44767, 3.81827)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60779.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.59 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 614 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 614 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t614 features in original data used to generate 614 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.28 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.0962\n",
      "[2000]\tvalid_set's rmse: 1.08959\n",
      "[3000]\tvalid_set's rmse: 1.08951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.0895\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 166.31s of the 166.31s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.882518\n",
      "[2000]\tvalid_set's rmse: 0.880954\n",
      "[3000]\tvalid_set's rmse: 0.880671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8807\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 144.92s of the 144.92s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6157.\n",
      "\t-0.8597\t = Validation score   (-root_mean_squared_error)\n",
      "\t145.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.9s of remaining time.\n",
      "\t-0.8521\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.8s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels304\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.26990110134712847\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.26990110134712847,\n",
      "    \"mean_squared_error\": -0.07284660450839305,\n",
      "    \"mean_absolute_error\": -0.06529273006479966,\n",
      "    \"r2\": 0.9950029166991687,\n",
      "    \"pearsonr\": 0.9975074726636299,\n",
      "    \"median_absolute_error\": -0.00978301009206528\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels305\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.24 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 930\n",
      "Label Column: 930\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (32.9457168665, -12.4168088922, 2.18694, 3.76533)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60751.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.67 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 615 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 615 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t615 features in original data used to generate 615 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.36 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.28s of the 179.28s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.839812\n",
      "[2000]\tvalid_set's rmse: 0.836082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8356\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.65s of the 169.65s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.737964\n",
      "[2000]\tvalid_set's rmse: 0.7365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7363\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 157.35s of the 157.34s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 6621.\n",
      "\t-0.6831\t = Validation score   (-root_mean_squared_error)\n",
      "\t157.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.28s of the -0.65s of remaining time.\n",
      "\t-0.6831\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.58s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels305\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21634996175421528\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21634996175421528,\n",
      "    \"mean_squared_error\": -0.046807305951050544,\n",
      "    \"mean_absolute_error\": -0.05245078722939688,\n",
      "    \"r2\": 0.9966982055947622,\n",
      "    \"pearsonr\": 0.9983510389972134,\n",
      "    \"median_absolute_error\": -0.007371430083227715\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels306\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.17 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 931\n",
      "Label Column: 931\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (30.0067997167, -17.8569692336, 1.88305, 3.78212)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60725.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.76 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 616 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 616 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t616 features in original data used to generate 616 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.45 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.27s of the 179.26s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.84782\n",
      "[2000]\tvalid_set's rmse: 0.842691\n",
      "[3000]\tvalid_set's rmse: 0.842366\n",
      "[4000]\tvalid_set's rmse: 0.842225\n",
      "[5000]\tvalid_set's rmse: 0.842175\n",
      "[6000]\tvalid_set's rmse: 0.842163\n",
      "[7000]\tvalid_set's rmse: 0.842158\n",
      "[8000]\tvalid_set's rmse: 0.842158\n",
      "[9000]\tvalid_set's rmse: 0.842158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8422\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.7s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 146.43s of the 146.43s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.734799\n",
      "[2000]\tvalid_set's rmse: 0.733414\n",
      "[3000]\tvalid_set's rmse: 0.733217\n",
      "[4000]\tvalid_set's rmse: 0.733216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7332\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 122.02s of the 122.01s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 5118.\n",
      "\t-0.6976\t = Validation score   (-root_mean_squared_error)\n",
      "\t122.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.27s of the -0.33s of remaining time.\n",
      "\t-0.6922\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels306\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.21997627053749927\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.21997627053749927,\n",
      "    \"mean_squared_error\": -0.04838955959958709,\n",
      "    \"mean_absolute_error\": -0.05916046593092289,\n",
      "    \"r2\": 0.9966168355442115,\n",
      "    \"pearsonr\": 0.9983103751132669,\n",
      "    \"median_absolute_error\": -0.014379659311804405\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n",
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels307\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.08 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 932\n",
      "Label Column: 932\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (28.292740164, -19.2967808522, 1.78321, 3.95049)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60723.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 77.84 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 315): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 617 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 617 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t617 features in original data used to generate 617 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.53 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.25s of the 179.24s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.866357\n",
      "[2000]\tvalid_set's rmse: 0.864461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.8642\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 169.29s of the 169.29s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 0.756646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.7562\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 162.59s of the 162.58s of remaining time.\n",
      "\t-0.7391\t = Validation score   (-root_mean_squared_error)\n",
      "\t95.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 66.71s of the 66.71s of remaining time.\n",
      "\t-0.7995\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 22.78s of the 22.78s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 745. Best iteration is:\n",
      "\t[739]\tvalid_set's rmse: 0.806185\n",
      "\t-0.8062\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.25s of the -0.8s of remaining time.\n",
      "\t-0.7308\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 181.72s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels307\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.24643068631468634\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.24643068631468634,\n",
      "    \"mean_squared_error\": -0.060728083157527285,\n",
      "    \"mean_absolute_error\": -0.10541072858158759,\n",
      "    \"r2\": 0.9961083961348555,\n",
      "    \"pearsonr\": 0.9980620014680344,\n",
      "    \"median_absolute_error\": -0.062415860905792236\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels391\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   44.00 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 1016\n",
      "Label Column: 1016\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (33.1785590645, -27.8655930673, 1.0742, 4.21199)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60651.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 84.86 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 398): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '933', '934', '935', '936', '937', '938', '939', '940', '941', '942', '943', '944', '945', '946', '947', '948', '949', '950', '951', '952', '953', '954', '955', '956', '957', '958', '959', '960', '961', '962', '963', '964', '965', '966', '967', '968', '969', '970', '971', '972', '973', '974', '975', '976', '977', '978', '979', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '1010', '1011', '1012', '1013', '1014', '1015']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 618 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 618 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t618 features in original data used to generate 618 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.62 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.23s of the 179.22s of remaining time.\n",
      "\t-3.2399\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 177.33s of the 177.32s of remaining time.\n",
      "\t-3.1526\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 174.9s of the 174.89s of remaining time.\n",
      "\t-3.0931\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 153.61s of the 153.61s of remaining time.\n",
      "\t-3.2046\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 141.22s of the 141.22s of remaining time.\n",
      "\t-3.2158\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.23s of the 129.2s of remaining time.\n",
      "\t-3.0927\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 51.64s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels391\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -1.9791446958335348\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -1.9791446958335348,\n",
      "    \"mean_squared_error\": -3.9170137270459864,\n",
      "    \"mean_absolute_error\": -1.5297570084047727,\n",
      "    \"r2\": 0.779188135334901,\n",
      "    \"pearsonr\": 0.8940542164980023,\n",
      "    \"median_absolute_error\": -1.276290476322174\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "Beginning AutoGluon training ... Time limit = 180s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoGluon will save models to \"./a/y/result-high-1/1-1-6/tgModels541\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   43.94 GB / 2000.36 GB (2.2%)\n",
      "Train Data Rows:    10440\n",
      "Train Data Columns: 1166\n",
      "Label Column: 1166\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (11.9937730205, -8.2871147992, -0.64812, 1.6166)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    60617.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 97.38 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 547): ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '170', '171', '172', '173', '174', '175', '176', '177', '178', '196', '197', '198', '199', '200', '201', '202', '221', '222', '223', '224', '225', '226', '227', '247', '248', '249', '250', '251', '272', '273', '274', '275', '276', '297', '298', '299', '300', '301', '322', '323', '324', '325', '326', '347', '348', '349', '350', '351', '372', '373', '374', '375', '376', '377', '397', '398', '399', '400', '401', '402', '421', '422', '423', '424', '425', '426', '427', '428', '446', '447', '448', '449', '450', '451', '452', '453', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '933', '934', '935', '936', '937', '938', '939', '940', '941', '942', '943', '944', '945', '946', '947', '948', '949', '950', '951', '952', '953', '954', '955', '956', '957', '958', '959', '960', '961', '962', '963', '964', '965', '966', '967', '968', '969', '970', '971', '972', '973', '974', '975', '976', '977', '978', '979', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '1010', '1011', '1012', '1013', '1014', '1015', '1017', '1018', '1019', '1020', '1021', '1022', '1023', '1024', '1025', '1026', '1027', '1028', '1029', '1030', '1031', '1032', '1033', '1034', '1035', '1036', '1037', '1038', '1039', '1040', '1041', '1042', '1043', '1044', '1045', '1046', '1047', '1048', '1049', '1050', '1051', '1052', '1053', '1054', '1055', '1056', '1057', '1058', '1059', '1060', '1061', '1062', '1063', '1064', '1065', '1066', '1067', '1068', '1069', '1070', '1071', '1072', '1073', '1074', '1075', '1076', '1077', '1078', '1079', '1080', '1081', '1082', '1083', '1084', '1085', '1086', '1087', '1088', '1089', '1090', '1091', '1092', '1093', '1094', '1095', '1096', '1097', '1098', '1099', '1100', '1101', '1102', '1103', '1104', '1105', '1106', '1107', '1108', '1109', '1110', '1111', '1112', '1113', '1114', '1115', '1116', '1117', '1118', '1119', '1120', '1121', '1122', '1123', '1124', '1125', '1126', '1127', '1128', '1129', '1130', '1131', '1132', '1133', '1134', '1135', '1136', '1137', '1138', '1139', '1140', '1141', '1142', '1143', '1144', '1145', '1146', '1147', '1148', '1149', '1150', '1151', '1152', '1153', '1154', '1155', '1156', '1157', '1158', '1159', '1160', '1161', '1162', '1163', '1164', '1165']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 619 | ['83', '84', '85', '86', '87', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 619 | ['83', '84', '85', '86', '87', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t619 features in original data used to generate 619 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 51.7 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.83s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 9396, Val Rows: 1044\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "}\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 179.17s of the 179.16s of remaining time.\n",
      "\t-0.9855\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 176.02s of the 176.02s of remaining time.\n",
      "\t-1.0045\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 172.49s of the 172.49s of remaining time.\n",
      "\t-0.9908\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 158.34s of the 158.34s of remaining time.\n",
      "\t-1.0132\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.62s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 146.68s of the 146.67s of remaining time.\n",
      "\t-1.0064\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 179.17s of the 129.76s of remaining time.\n",
      "\t-0.9816\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 51.11s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./a/y/result-high-1/1-1-6/tgModels541\\\")\n",
      "Evaluation: root_mean_squared_error on test data: -0.5698339793965238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.5698339793965238,\n",
      "    \"mean_squared_error\": -0.3247107640748759,\n",
      "    \"mean_absolute_error\": -0.4034898376991716,\n",
      "    \"r2\": 0.8757391860132798,\n",
      "    \"pearsonr\": 0.9405444013631103,\n",
      "    \"median_absolute_error\": -0.28943365138623656\n",
      "}\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n",
      "C:\\Users\\ps\\AppData\\Local\\Temp\\ipykernel_4548\\1564152145.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pre_data.insert(len(pre_data.columns),i+625,nxt_data[stable_priority[i]])\n"
     ]
    }
   ],
   "source": [
    "crank_angle_trainer('high',-300,-300,-290)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
